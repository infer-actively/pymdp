{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import random as jr\n",
    "from pymdp.jax.agent import Agent as AIFAgent\n",
    "from pymdp.utils import random_A_matrix, random_B_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(f, init, xs, length=None, unroll=1):\n",
    "  if xs is None:\n",
    "    xs = [None] * length\n",
    "  carry = init\n",
    "  ys = []\n",
    "  for x in xs:\n",
    "    for _ in range(unroll):\n",
    "      carry, y = f(carry, x)\n",
    "    if y is not None:\n",
    "       ys.append(y)\n",
    "  \n",
    "  ys = None if len(ys) < 1 else jtu.tree_map(lambda *x: jnp.stack(x), *ys)\n",
    "\n",
    "  return carry, ys\n",
    "\n",
    "def evolve_trials(agent, env, block_idx, num_timesteps):\n",
    "\n",
    "    def step_fn(carry, xs):\n",
    "        actions = carry['actions']\n",
    "        outcomes = carry['outcomes']\n",
    "        beliefs = agent.infer_states(outcomes, actions, *carry['args'])\n",
    "        # q_pi, _ = agent.infer_policies(beliefs)\n",
    "        q_pi = jnp.ones((batch_size, len(agent.policies)))/len(agent.policies)\n",
    "        actions_t = agent.sample_action(q_pi)\n",
    "\n",
    "        outcome_t = env.step(actions_t)\n",
    "        outcomes = jtu.tree_map(\n",
    "           lambda prev_o, new_o: jnp.concatenate([prev_o, jnp.expand_dims(new_o, -1)], -1), outcomes, outcome_t\n",
    "          )\n",
    "\n",
    "        if actions is not None:\n",
    "          actions = jnp.concatenate([actions, jnp.expand_dims(actions_t, -2)], -2)\n",
    "        else:\n",
    "          actions = jnp.expand_dims(actions_t, -2)\n",
    "\n",
    "        # args = agent.update_empirical_prior(actions_t, beliefs)\n",
    "        args = (jtu.tree_map( lambda x: x[:, -1], beliefs), beliefs)  \n",
    "        \n",
    "        # args = (pred_{t+1}, [post_1, post_{2}, ..., post_{t}])\n",
    "        # beliefs =  [post_1, post_{2}, ..., post_{t}]\n",
    "        return {'args': args, 'outcomes': outcomes, 'beliefs': beliefs, 'actions': actions}, None\n",
    "\n",
    "    outcome_0  = jtu.tree_map(lambda x: jnp.expand_dims(x, -1), env.step())\n",
    "    init = {\n",
    "       'args': (agent.D, None,),\n",
    "       'outcomes': outcome_0, \n",
    "       'beliefs': [],\n",
    "       'actions': None\n",
    "    }\n",
    "    last, _ = scan(step_fn, init, range(num_timesteps))\n",
    "\n",
    "    return last, env\n",
    "\n",
    "def step_fn(carry, block_idx):\n",
    "    agent, env = carry\n",
    "    output, env = evolve_trials(agent, env, block_idx, num_timesteps)\n",
    "    output.pop('args')          \n",
    "\n",
    "    # How to deal with contiguous blocks of trials? Two options we can imagine: \n",
    "    # A) you use final posterior (over current and past timesteps) to compute the smoothing distribution over qs_{t=0} and update pD, and then pass pD as the initial state prior ($D = \\mathbb{E}_{pD}[qs_{t=0}]$);\n",
    "    # B) we don't assume that blocks 'reset time', and are really just adjacent chunks of one long sequence, so you set the initial state prior to be the final output (`output['beliefs']`) passed through\n",
    "    # the transition model entailed by the action taken at the last timestep of the previous block.\n",
    "    \n",
    "    # agent = agent.learning(**output)\n",
    "    \n",
    "    return (agent, env), output\n",
    "\n",
    "# define an agent and environment here\n",
    "batch_size = 10\n",
    "num_obs = [3, 3]\n",
    "num_states = [3, 3]\n",
    "num_controls = [2, 2]\n",
    "num_blocks = 2\n",
    "num_timesteps = 5\n",
    "\n",
    "A_np = random_A_matrix(num_obs=num_obs, num_states=num_states)\n",
    "B_np = random_B_matrix(num_states=num_states, num_controls=num_controls)\n",
    "A = jtu.tree_map(lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape), list(A_np))\n",
    "B = jtu.tree_map(lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape), list(B_np))\n",
    "C = [jnp.zeros((batch_size, no)) for no in num_obs]\n",
    "D = [jnp.ones((batch_size, ns)) / ns for ns in num_states]\n",
    "E = jnp.ones((batch_size, 4 )) / 4 \n",
    "\n",
    "class TestEnv:\n",
    "    def __init__(self, num_obs, prng_key=jr.PRNGKey(0)):\n",
    "      self.num_obs=num_obs\n",
    "      self.key = prng_key\n",
    "    def step(self, actions=None):\n",
    "      # return a list of random observations for each agent or parallel realization (each entry in batch_dim)\n",
    "      obs = [jr.randint(self.key, (batch_size,), 0, no) for no in self.num_obs]\n",
    "      self.key, _ = jr.split(self.key)\n",
    "      return obs\n",
    "\n",
    "agents = AIFAgent(A, B, C, D, E, inference_algo='mmp')\n",
    "env = TestEnv(num_obs)\n",
    "\n",
    "init = (agents, env)\n",
    "(agents, env), sequences = scan(step_fn, init, range(num_blocks) )\n",
    "\n",
    "sequences = jtu.tree_map(lambda x: x.swapaxes(1, 2), sequences)\n",
    "\n",
    "# NOTE: all elements of sequences will have dimensionality blocks, trials, batch_size, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scan(f, init, xs, length=None):\n",
    "#   if xs is None:\n",
    "#     xs = [None] * length\n",
    "#   carry = init\n",
    "#   ys = []\n",
    "#   for x in xs:\n",
    "#     carry, y = f(carry, x)\n",
    "#     ys.append(y)\n",
    "  \n",
    "#   return carry, jnp.stack(ys)\n",
    "\n",
    "# def evolve_trials(agent, env, block_idx, num_timesteps):\n",
    "\n",
    "#     def step_fn(carry, xs):\n",
    "#         actions = carry['actions']\n",
    "#         outcomes = carry['outcomes']\n",
    "#         beliefs = agent.infer_states(outcomes, actions, *carry['args'])\n",
    "#         q_pi, _ = agent.infer_policies(beliefs)\n",
    "#         actions_t = agent.sample_action(q_pi)\n",
    "\n",
    "#         outcome_t = env.step(actions_t)\n",
    "#         outcomes = jtu.tree_map(lambda prev_o, new_o: jnp.stack([prev_o, jnp.expand_dims(new_o, 0)], 0), outcomes, outcome_t)\n",
    "\n",
    "#         actions = jnp.stack([actions, jnp.expand_dims(actions_t, 0)], 0) if actions is not None else actions_t\n",
    "#         args = agent.update_empirical_prior(actions_t, beliefs)\n",
    "#         # (pred, [cond_1, ..., cond_{t-1}])\n",
    "\n",
    "#         # ovf beliefs = (post_T, [cond_1, cond_2, ..., cond_{T-1}])\n",
    "#         # else beliefs = (post_T, post_{T-1}, ..., post_1)\n",
    "#         return {'args': args, 'outcomes': outcomes, 'beliefs': beliefs, 'actions': actions}, None\n",
    "\n",
    "#     outcome_0  = env.step()\n",
    "#     init = ((agent.D, None), outcome_0, None, None)\n",
    "#     last, _ = scan(step_fn, init, range(num_timesteps))\n",
    "\n",
    "#     return last, env\n",
    "\n",
    "# def step_fn(carry, block_idx):\n",
    "#     agent, env = carry\n",
    "#     output, env = evolve_trials(agent, env, block_idx, num_timesteps)\n",
    "\n",
    "#     # How to deal with contiguous blocks of trials? Two options we can imagine: \n",
    "#     # A) you use final posterior (over current and past timesteps) to compute the smoothing distribution over qs_{t=0} and update pD, and then pass pD as the initial state prior ($D = \\mathbb{E}_{pD}[qs_{t=0}]$);\n",
    "#     # B) we don't assume that blocks 'reset time', and are really just adjacent chunks of one long sequence, so you set the initial state prior to be the final output (`output['beliefs']`) passed through\n",
    "#     # the transition model entailed by the action taken at the last timestep of the previous block.\n",
    "    \n",
    "#     agent = agent.learning(**output)\n",
    "    \n",
    "#     return (agent, env), output\n",
    "\n",
    "# init = (agent, env)\n",
    "# agent, squences = scan(step_fn, init, range(num_blocks) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_pymdp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
