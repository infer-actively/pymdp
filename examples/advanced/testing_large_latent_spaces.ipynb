{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set cuda device to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# do not prealocate memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import equinox as eqx\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from jax import vmap, lax, nn, jit, remat\n",
    "from jax import random as jr\n",
    "from pymdp.jax.agent import Agent as AIFAgent\n",
    "from pymdp.utils import random_A_matrix, random_B_matrix\n",
    "from opt_einsum import contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(jit, static_argnames=['dims', 'keep_dims'])\n",
    "def factor_dot(M, xs, dims, keep_dims = None):\n",
    "    \"\"\" Dot product of a multidimensional array with `x`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - `qs` [list of 1D numpy.ndarray] - list of jnp.ndarrays\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    - `Y` [1D numpy.ndarray] - the result of the dot product\n",
    "    \"\"\"\n",
    "    all_dims = list(range(M.ndim))\n",
    "    matrix = [[xs[f], dims[f]] for f in range(len(xs))]\n",
    "    args = [M, all_dims]\n",
    "    for row in matrix:\n",
    "        args.extend(row)\n",
    "\n",
    "    args += [keep_dims]\n",
    "    return contract(*args, backend='jax', optimize='auto')\n",
    "\n",
    "@vmap\n",
    "def get_marginals(posterior):\n",
    "  d = posterior.ndim - 1\n",
    "  marginals = []\n",
    "  for i in range(d):\n",
    "     marginals.append( jnp.sum(posterior, axis=(j + 1 for j in range(d) if j != i)) )\n",
    "\n",
    "  return marginals\n",
    "\n",
    "@vmap\n",
    "def merge_marginals(marginals):\n",
    "  q = marginals[0]\n",
    "  for m in marginals[1:]:\n",
    "    q = jnp.expand_dims(q, -1) * m\n",
    "  \n",
    "  return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tuple(i, d, ext):\n",
    "    l = [i,]\n",
    "    l.extend(d + i for i in ext)\n",
    "    return tuple(l)\n",
    "\n",
    "make_tuple(0, 1, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(vmap, in_axes=(0, 0, None, None))\n",
    "def delta_A(beliefs, outcomes, deps, num_obs):\n",
    "  def merge(beliefs, outcomes):\n",
    "    y = nn.one_hot(outcomes, num_obs)\n",
    "    d = beliefs.ndim\n",
    "    marg_beliefs = jnp.sum(beliefs, axis=(i for i in range(d) if i not in deps))\n",
    "    axis = ( - (i+1) for i in range(len(deps)))\n",
    "    return jnp.expand_dims(y, axis) * marg_beliefs\n",
    "  \n",
    "  return vmap(merge, in_axes=(0, None))(beliefs, outcomes)\n",
    "  \n",
    "@partial(vmap, in_axes=(0, 0, 0, None))\n",
    "def delta_B(post_b, cond_b, action, num_actions):\n",
    "   a = nn.one_hot(action, num_actions)\n",
    "   all_dims = tuple(range(cond_b.ndim - 1))\n",
    "   fd = lambda x, y: factor_dot(x, [y], ((0,),), keep_dims=all_dims)\n",
    "   b = vmap(fd)(cond_b, post_b)\n",
    "   return b * a\n",
    "\n",
    "@partial(vmap, in_axes=(None, 0))\n",
    "def get_reverse_conditionals(B, beliefs):\n",
    "  all_dims = tuple(range(B.ndim - 1))\n",
    "  dims = tuple((i,) for i in all_dims[1:-1])\n",
    "  fd = lambda x, y: factor_dot(x, y, dims, keep_dims=all_dims)\n",
    "  joint = vmap(fd)(B, beliefs)\n",
    "  pred = joint.sum(axis=all_dims[2:], keepdims=True)\n",
    "  return joint / pred\n",
    "\n",
    "@partial(vmap, in_axes=(0, 0, None))\n",
    "def get_reverse_predictive(post, cond, deps):\n",
    "  def pred(post, cond, deps):\n",
    "    d = post.ndim\n",
    "    dims = tuple(make_tuple(i, d, deps[i]) for i in range(len(deps)))\n",
    "    keep_dims = list(dims[0][1:])\n",
    "    for row in dims[1:]:\n",
    "      keep_dims.extend(list(row[1:]))\n",
    "    \n",
    "    unique_dims = tuple(set(keep_dims))\n",
    "\n",
    "    return factor_dot(post, cond, dims, keep_dims=unique_dims)\n",
    "  \n",
    "  out = vmap(pred, in_axes=(0, 0, None))(post, cond, deps)\n",
    "  return out\n",
    "\n",
    "def learning(agent, beliefs, actions, outcomes, lag=1):\n",
    "  A_deps = agent.A_dependencies\n",
    "  B_deps = agent.B_dependencies\n",
    "  num_obs = agent.num_obs\n",
    "  posterior_beliefs = merge_marginals( jtu.tree_map(lambda x: x[..., -1, :], beliefs) )\n",
    "  qA = agent.pA\n",
    "  qB = agent.pB\n",
    "\n",
    "  def step_fn(carry, xs):\n",
    "    posterior_beliefs, qA, qB = carry\n",
    "    obs, acts, filter_beliefs = xs\n",
    "    # learn A matrix\n",
    "    if agent.learn_A:\n",
    "      qA = jtu.tree_map(\n",
    "        lambda qa, o, m: qa + delta_A(posterior_beliefs, o, A_deps[m], num_obs[m]).sum(0), \n",
    "        qA, \n",
    "        obs, \n",
    "        list(range(len(num_obs)))\n",
    "        )\n",
    "\n",
    "    # learn B matrix\n",
    "    conditional_beliefs = jtu.tree_map(\n",
    "       lambda b, f: get_reverse_conditionals(b, [filter_beliefs[i] for i in B_deps[f]]),\n",
    "       agent.B, \n",
    "       list(range(len(agent.B))) \n",
    "    )\n",
    "    post_marg = get_marginals(posterior_beliefs)\n",
    "    acts =  [acts[..., i] for i in range(acts.shape[-1])]\n",
    "\n",
    "    qB = jtu.tree_map(\n",
    "       lambda qb, pb, cb, a, nc: qb + delta_B(pb, cb, a, nc).sum(0),\n",
    "       qB,\n",
    "       post_marg,\n",
    "       conditional_beliefs,\n",
    "       acts,\n",
    "       agent.num_controls  \n",
    "    )\n",
    "\n",
    "    # compute posterior beliefs for the next time step\n",
    "    get_transition = lambda cb, a: cb[..., a]\n",
    "    conditional_beliefs = jtu.tree_map(\n",
    "      lambda cb, a: vmap(get_transition)(cb, a), conditional_beliefs, acts\n",
    "    )\n",
    "    posterior_beliefs = get_reverse_predictive(posterior_beliefs, conditional_beliefs, B_deps)\n",
    "\n",
    "    return (posterior_beliefs, qA, qB), None\n",
    "\n",
    "  first_outcomes = jtu.tree_map(lambda x: x[..., 0], outcomes)\n",
    "  outcomes = jtu.tree_map(lambda x: jnp.flipud(x.swapaxes(0, 1))[1:lag+1], outcomes)\n",
    "  actions = jnp.flipud(actions.swapaxes(0, 1))[:lag]\n",
    "  beliefs = jtu.tree_map(lambda x: jnp.flipud(jnp.moveaxis(x, 2, 0))[1:lag+1], beliefs)\n",
    "  iters = (outcomes, actions, beliefs)\n",
    "  (last_beliefs, qA, qB), _ = lax.scan(step_fn, (posterior_beliefs, qA, qB), iters)\n",
    "\n",
    "  # update A with the first outcome \n",
    "  if agent.learn_A:\n",
    "    qA = jtu.tree_map(\n",
    "      lambda qa, o, m: qa + delta_A(last_beliefs, o, A_deps[m], num_obs[m]).sum(0), \n",
    "      qA, \n",
    "      first_outcomes, \n",
    "      list(range(len(num_obs)))\n",
    "    )\n",
    "\n",
    "  if qA is not None:\n",
    "    E_qA = jtu.tree_map(lambda qa: qa / qa.sum(0), qA)\n",
    "  else:\n",
    "    E_qA = agent.A\n",
    "  E_qB =jtu.tree_map(lambda qb: qb / qb.sum(0), qB)\n",
    "  agent = eqx.tree_at(\n",
    "    lambda x: (x.A, x.pA, x.B, x.pB), agent, (E_qA, qA, E_qB, qB), is_leaf=lambda x: x is None\n",
    "  )\n",
    "\n",
    "  return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEnv:\n",
    "    def __init__(self, num_agents, num_obs, prng_key=jr.PRNGKey(0)):\n",
    "      self.num_obs = num_obs\n",
    "      self.num_agents = num_agents\n",
    "      self.key = prng_key\n",
    "    \n",
    "    def step(self, actions=None):\n",
    "      # return a list of random observations for each agent or parallel realization (each entry in batch_dim)\n",
    "      obs = [jr.randint(self.key, (self.num_agents,), 0, no) for no in self.num_obs]\n",
    "      self.key, _ = jr.split(self.key)\n",
    "      return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_agent_state(agent, env, args, key, outcomes, actions):\n",
    "    beliefs = agent.infer_states(outcomes, actions, *args)\n",
    "    # q_pi, _ = agent.infer_policies(beliefs)\n",
    "    q_pi = jnp.ones((agent.batch_size, 6)) / 6\n",
    "    batch_keys = jr.split(key, agent.batch_size)\n",
    "    actions = agent.sample_action(q_pi, rng_key=batch_keys)\n",
    "\n",
    "    outcomes = env.step(actions)\n",
    "    outcomes = jtu.tree_map(lambda x: jnp.expand_dims(x, -1), outcomes)\n",
    "    args = agent.update_empirical_prior(actions, beliefs)\n",
    "    args = (args[0], None)  # remove belief history from args\n",
    "    latest_belief = jtu.tree_map(lambda x: x[:, 0], beliefs)\n",
    "\n",
    "    return args, latest_belief, outcomes, actions\n",
    "\n",
    "def evolve_trials(agent, env, batch_size, num_timesteps, prng_key=jr.PRNGKey(0)):\n",
    "\n",
    "    def step_fn(carry, xs):\n",
    "        actions = carry['actions']\n",
    "        outcomes = carry['outcomes']\n",
    "        key = carry['key']\n",
    "        key, _key = jr.split(key)\n",
    "        vect_uas = vmap(partial(update_agent_state, agent, env))\n",
    "        keys = jr.split(_key, batch_size)\n",
    "        args, beliefs, outcomes, actions = vect_uas(carry['args'], keys, outcomes, actions)\n",
    "        output = {\n",
    "           'args': args, \n",
    "           'outcomes': outcomes, \n",
    "           'actions': actions,\n",
    "           'key': key\n",
    "        }\n",
    "\n",
    "        return output, {'beliefs': beliefs, 'actions': actions[..., 0, :], 'outcomes': outcomes}\n",
    "\n",
    "   \n",
    "    outcome_0  = jtu.tree_map(lambda x: jnp.expand_dims(x, -1), env.step())\n",
    "    outcome_0 = jtu.tree_map(lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape), outcome_0)\n",
    "    prior = jtu.tree_map(lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape), agent.D)\n",
    "    init = {\n",
    "      'args': (prior, None),\n",
    "      'outcomes': outcome_0,\n",
    "      'actions': - jnp.ones((batch_size, 1, agent.policies.shape[-1]), dtype=jnp.int32),\n",
    "      'key': prng_key\n",
    "    }\n",
    "\n",
    "    last, sequences = lax.scan(step_fn, init, jnp.arange(num_timesteps))\n",
    "    sequences['outcomes'] = jtu.tree_map(\n",
    "        lambda x, y: jnp.concatenate([jnp.expand_dims(x.squeeze(), 0), y.squeeze()]), \n",
    "        outcome_0, \n",
    "        sequences['outcomes']\n",
    "      )\n",
    "\n",
    "    return last, sequences\n",
    "\n",
    "@partial(jit, static_argnums=(1, 2, 3, 4))\n",
    "def training_step(agent, env, batch_size, num_timesteps, lag=1):\n",
    "    output, sequences = evolve_trials(agent, env, batch_size, num_timesteps)\n",
    "    args = output.pop('args')\n",
    "    \n",
    "    outcomes = jtu.tree_map(lambda x: x.swapaxes(0, 1), sequences['outcomes'])\n",
    "    actions = sequences['actions'].swapaxes(0, 1)\n",
    "    beliefs = jtu.tree_map(lambda x: jnp.moveaxis(x, [0, 2], [1, 1]), sequences['beliefs'])\n",
    "\n",
    "    def update_beliefs(outcomes, actions, args):\n",
    "        return agent.infer_states(outcomes, actions, *args)\n",
    "\n",
    "    # update beliefs with the last action-outcome pair\n",
    "    last_belief = vmap(update_beliefs)(\n",
    "       output['outcomes'], \n",
    "       output['actions'],\n",
    "       args\n",
    "      )\n",
    "\n",
    "    beliefs = jtu.tree_map(lambda x, y: jnp.concatenate([x, y], -2), beliefs, last_belief)\n",
    "    # agent, beliefs, actions, outcomes = lax.stop_gradient((agent, beliefs, actions, outcomes))\n",
    "    agent = learning(agent, beliefs, actions, outcomes, lag=lag)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an agent and environment here\n",
    "batch_size = 16\n",
    "num_agents = 1\n",
    "\n",
    "num_pixels = 32\n",
    "# y_pos paddle 1, y_pos paddle 2, (x_pos, y_pos) ball\n",
    "num_obs = [num_pixels, num_pixels, num_pixels, num_pixels]\n",
    "num_states = [num_pixels, num_pixels, num_pixels, num_pixels, 96]\n",
    "num_controls = [1, 1, 1, 1, 6]\n",
    "num_blocks = 1\n",
    "num_timesteps = 25\n",
    "\n",
    "action_lists = [jnp.zeros(6, dtype=jnp.int32)] * 4\n",
    "action_lists += [jnp.arange(6, dtype=jnp.int32)]\n",
    "\n",
    "policies = jnp.expand_dims(jnp.stack(action_lists, -1), -2)\n",
    "num_policies = len(policies)\n",
    "\n",
    "A_dependencies = [[0], [1], [2], [3]]\n",
    "B_dependencies = [[0, 4], [1, 4], [2, 4], [3, 4], [4]]\n",
    "\n",
    "A_np = [np.eye(o) for o in num_obs]\n",
    "B_np = list(random_B_matrix(num_states=num_states, num_controls=num_controls, B_factor_list=B_dependencies))\n",
    "A = jtu.tree_map(lambda x: jnp.broadcast_to(x, (num_agents,) + x.shape), A_np)\n",
    "B = jtu.tree_map(lambda x: jnp.broadcast_to(x, (num_agents,) + x.shape), B_np)\n",
    "C = [jnp.zeros((num_agents, no)) for no in num_obs]\n",
    "D = [jnp.ones((num_agents, ns)) / ns for ns in num_states]\n",
    "E = jnp.ones((num_agents, num_policies )) / num_policies\n",
    "\n",
    "pA = None # jtu.tree_map(lambda x: jnp.broadcast_to(jnp.ones_like(x), (num_agents,) + x.shape), A_np)\n",
    "pB = jtu.tree_map(lambda x: jnp.broadcast_to(jnp.ones_like(x), (num_agents,) + x.shape), B_np)\n",
    "\n",
    "agents = AIFAgent(A, B, C, D, E, pA, pB, learn_A=False, policies=policies, A_dependencies=A_dependencies, B_dependencies=B_dependencies, use_param_info_gain=True, inference_algo='fpi', sampling_mode='marginal', action_selection='deterministic', num_iter=8)\n",
    "env = TestEnv(num_agents, num_obs)\n",
    "agents = training_step(agents, env, batch_size, num_timesteps, lag=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents = lax.stop_gradient(agents)\n",
    "%timeit training_step(agents, env, batch_size, num_timesteps, lag=25).A[0].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an agent and environment here\n",
    "batch_size = 16\n",
    "num_agents = 1\n",
    "\n",
    "num_pixels = 32\n",
    "# y_pos paddle 1, y_pos paddle 2, (x_pos, y_pos) ball\n",
    "num_obs = [num_pixels, num_pixels, num_pixels, num_pixels]\n",
    "num_states = [num_pixels, 2, num_pixels, 2, num_pixels, num_pixels, 24]\n",
    "num_controls = [1, 6, 1, 6, 1, 1, 6]\n",
    "num_blocks = 1\n",
    "num_timesteps = 25\n",
    "\n",
    "action_lists = [jnp.zeros(6, dtype=jnp.int32), jnp.arange(6, dtype=jnp.int32)] * 2\n",
    "action_lists += [jnp.zeros(6, dtype=jnp.int32), jnp.zeros(6, dtype=jnp.int32), jnp.arange(6, dtype=jnp.int32)]\n",
    "\n",
    "policies = jnp.expand_dims(jnp.stack(action_lists, -1), -2)\n",
    "num_policies = len(policies)\n",
    "\n",
    "A_dependencies = [[0], [2], [4], [5]]\n",
    "B_dependencies = [[0, 1], [1], [2, 3], [3], [4, 6], [5, 6], [6]]\n",
    "\n",
    "A_np = [np.eye(o) for o in num_obs]\n",
    "B_np = list(random_B_matrix(num_states=num_states, num_controls=num_controls, B_factor_list=B_dependencies))\n",
    "A = jtu.tree_map(lambda x: jnp.broadcast_to(x, (num_agents,) + x.shape), A_np)\n",
    "B = jtu.tree_map(lambda x: jnp.broadcast_to(x, (num_agents,) + x.shape), B_np)\n",
    "C = [jnp.zeros((num_agents, no)) for no in num_obs]\n",
    "D = [jnp.ones((num_agents, ns)) / ns for ns in num_states]\n",
    "E = jnp.ones((num_agents, num_policies )) / num_policies\n",
    "\n",
    "pA = None # jtu.tree_map(lambda x: jnp.broadcast_to(jnp.ones_like(x), (num_agents,) + x.shape), A_np)\n",
    "pB = jtu.tree_map(lambda x: jnp.broadcast_to(jnp.ones_like(x), (num_agents,) + x.shape), B_np)\n",
    "\n",
    "agents = AIFAgent(A, B, C, D, E, pA, pB, learn_A=False, policies=policies, A_dependencies=A_dependencies, B_dependencies=B_dependencies, use_param_info_gain=True, inference_algo='fpi', sampling_mode='marginal', action_selection='deterministic', num_iter=8)\n",
    "env = TestEnv(num_agents, num_obs)\n",
    "agents = training_step(agents, env, batch_size, num_timesteps, lag=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.4 s ± 15.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit training_step(agents, env, batch_size, num_timesteps, lag=25).A[0].block_until_ready()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_pymdp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
