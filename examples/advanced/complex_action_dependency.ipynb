{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex action dependencies\n",
    "\n",
    "In this notebook, we will show some examples of how to specify and run agents with complex action dependencies. Complex action dependencies refer to situations where a state variables depends on multiple actions or no action. These state transitions tensors have shapes of the form: `[state_dim, *prev_state_dims, *prev_action_dims]`. \n",
    "\n",
    "The general strategy for dealing with this is to flatten the `prev_action_dims` while initializing the agent so that the new B tensor shapes are `[state_dim, *prev_state_dims, math.prod(prev_action_dims)]`. If a state has no action dependency, the new B tensor will have shape `[state_dim, *prev_state_dims, 1]` where 1 stands for a dummy action. All computations will be done in the flattened B tensors and actions will be sampled in the flattened action dimensions. After a flattened action is sampled, one can convert it back to the original action dimensions by calling `agent.decode_multi_actions`. To flatten multi actions, for example from collected data, one can call `agent.encode_multi_actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import itertools\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "from jax import tree_util as jtu\n",
    "\n",
    "from pymdp.jax.agent import Agent\n",
    "from pymdp.jax import distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple action dependencies\n",
    "In this example, some states depend on multiple actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_dependencies [[0]]\n",
      "B_dependencies [[0], [1]]\n",
      "B_action_dependencies [[0, 1], [0]]\n",
      "original control dims [2, 3]\n",
      "flattened control dims [6, 2]\n",
      "original B shapes [(4, 4, 2, 3), (4, 4, 2)]\n",
      "flattened B shapes [(1, 4, 4, 6), (1, 4, 4, 2)]\n",
      "B normalized [Array(True, dtype=bool), Array(True, dtype=bool)]\n",
      "B flat normalized [Array(True, dtype=bool), Array(True, dtype=bool)]\n",
      "\n",
      "\n",
      "prior\n",
      "[Array([[0.  , 0.25, 0.25, 0.5 ]], dtype=float32),\n",
      " Array([[0.  , 0.25, 0.25, 0.5 ]], dtype=float32)]\n",
      "post\n",
      "[Array([[[0.5 , 0.12, 0.12, 0.25]]], dtype=float32),\n",
      " Array([[[0.  , 0.25, 0.25, 0.5 ]]], dtype=float32)]\n",
      "action\n",
      "Array([[0, 0]], dtype=int32)\n",
      "action_multi\n",
      "Array([[0, 0]], dtype=int32)\n",
      "action_reconstruct\n",
      "Array([[0, 0]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model = {\n",
    "    \"observations\": {\n",
    "        \"o1\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s1\"]},\n",
    "    },\n",
    "    \"controls\": {\"c1\": {\"elements\": [\"up\", \"down\"]}, \"c2\": {\"elements\": [\"left\", \"right\", \"stay\"]}},\n",
    "    \"states\": {\n",
    "        \"s1\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s1\"], \"controlled_by\": [\"c1\", \"c2\"]},\n",
    "        \"s2\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s2\"], \"controlled_by\": [\"c1\"]},\n",
    "    },\n",
    "}\n",
    "\n",
    "B_action_dependencies = [\n",
    "    [list(model[\"controls\"].keys()).index(i) for i in s[\"controlled_by\"]] \n",
    "    for s in model[\"states\"].values()\n",
    "]\n",
    "num_controls = [len(c[\"elements\"]) for c in model[\"controls\"].values()]\n",
    "\n",
    "As, Bs = distribution.compile_model(model)\n",
    "\n",
    "# initialize tensor values\n",
    "As[0][\"A\", \"A\"] = 1.0\n",
    "As[0][\"B\", \"B\"] = 1.0\n",
    "As[0][\"C\", \"C\"] = 1.0\n",
    "As[0][\"D\", \"D\"] = 1.0\n",
    "\n",
    "for i, state in enumerate(model[\"states\"].keys()):\n",
    "    controls = list(itertools.product(*[\n",
    "        model[\"controls\"][c][\"elements\"] for c in model[\"states\"][state][\"controlled_by\"]\n",
    "    ]))\n",
    "    for control in controls:\n",
    "        Bs[i][*[\"B\", \"A\"], *control] = 1.0\n",
    "        Bs[i][*[\"C\", \"B\"], *control] = 1.0\n",
    "        Bs[i][*[\"D\", \"C\"], *control] = 1.0\n",
    "        Bs[i][*[\"D\", \"D\"], *control] = 1.0\n",
    "\n",
    "agent = Agent(\n",
    "    As, Bs,\n",
    "    B_action_dependencies=B_action_dependencies,\n",
    "    num_controls=num_controls,\n",
    ")\n",
    "\n",
    "# dummy history\n",
    "action = agent.policies[np.random.randint(0, len(agent.policies))]\n",
    "observation = [np.random.randint(0, d, size=(1, 1)) for d in agent.num_obs]\n",
    "qs_hist = jtu.tree_map(lambda x: jnp.expand_dims(x, 0), agent.D)\n",
    "\n",
    "prior, _ = agent.infer_empirical_prior(action, qs_hist)\n",
    "qs = agent.infer_states(observation, None, prior, None)\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "action_multi = agent.decode_multi_actions(action)\n",
    "action_reconstruct = agent.encode_multi_actions(action_multi)\n",
    "\n",
    "print(\"A_dependencies\", agent.A_dependencies)\n",
    "print(\"B_dependencies\", agent.B_dependencies)\n",
    "print(\"B_action_dependencies\", agent.B_action_dependencies)\n",
    "print(\"original control dims\", agent.num_controls_multi)\n",
    "print(\"flattened control dims\", agent.num_controls)\n",
    "print(\"original B shapes\", [a.data.shape for a in Bs])\n",
    "print(\"flattened B shapes\", [a.shape for a in agent.B])\n",
    "print(\"B normalized\", [jnp.isclose(a.data.sum(0), 1.).all() for a in Bs])\n",
    "print(\"B flat normalized\", [jnp.isclose(a.sum(1), 1.).all() for a in agent.B])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"prior\")\n",
    "pprint([p.round(2) for p in prior])\n",
    "print(\"post\")\n",
    "pprint([p.round(2) for p in qs])\n",
    "print(\"action\")\n",
    "pprint(action)\n",
    "print(\"action_multi\")\n",
    "pprint(action_multi)\n",
    "print(\"action_reconstruct\")\n",
    "pprint(action_reconstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No action dependency\n",
    "\n",
    "In this example, some states do not depend on any action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_dependencies [[0]]\n",
      "B_dependencies [[0], [1]]\n",
      "B_action_dependencies [[0, 1], []]\n",
      "original control dims [2, 3]\n",
      "flattened control dims [6, 1]\n",
      "original B shapes [(4, 4, 2, 3), (4, 4)]\n",
      "flattened B shapes [(1, 4, 4, 6), (1, 4, 4, 1)]\n",
      "B normalized [Array(True, dtype=bool), Array(True, dtype=bool)]\n",
      "B flat normalized [Array(True, dtype=bool), Array(True, dtype=bool)]\n",
      "\n",
      "\n",
      "prior\n",
      "[Array([[0.  , 0.25, 0.25, 0.5 ]], dtype=float32),\n",
      " Array([[0.  , 0.25, 0.25, 0.5 ]], dtype=float32)]\n",
      "post\n",
      "[Array([[[0., 0., 1., 0.]]], dtype=float32),\n",
      " Array([[[0.  , 0.25, 0.25, 0.5 ]]], dtype=float32)]\n",
      "action\n",
      "Array([[0, 0]], dtype=int32)\n",
      "action_multi\n",
      "Array([[0, 0]], dtype=int32)\n",
      "action_reconstruct\n",
      "Array([[0, 0]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model = {\n",
    "    \"observations\": {\n",
    "        \"o1\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s1\"]},\n",
    "    },\n",
    "    \"controls\": {\"c1\": {\"elements\": [\"up\", \"down\"]}, \"c2\": {\"elements\": [\"left\", \"right\", \"stay\"]}},\n",
    "    \"states\": {\n",
    "        \"s1\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s1\"], \"controlled_by\": [\"c1\", \"c2\"]},\n",
    "        \"s2\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s2\"], \"controlled_by\": []},\n",
    "    },\n",
    "}\n",
    "\n",
    "B_action_dependencies = [\n",
    "    [list(model[\"controls\"].keys()).index(i) for i in s[\"controlled_by\"]] \n",
    "    for s in model[\"states\"].values()\n",
    "]\n",
    "num_controls = [len(c[\"elements\"]) for c in model[\"controls\"].values()]\n",
    "\n",
    "As, Bs = distribution.compile_model(model)\n",
    "\n",
    "# initialize tensor values\n",
    "As[0][\"A\", \"A\"] = 1.0\n",
    "As[0][\"B\", \"B\"] = 1.0\n",
    "As[0][\"C\", \"C\"] = 1.0\n",
    "As[0][\"D\", \"D\"] = 1.0\n",
    "\n",
    "for i, state in enumerate(model[\"states\"].keys()):\n",
    "    controls = list(itertools.product(*[\n",
    "        model[\"controls\"][c][\"elements\"] for c in model[\"states\"][state][\"controlled_by\"]\n",
    "    ]))\n",
    "    for control in controls:\n",
    "        Bs[i][*[\"B\", \"A\"], *control] = 1.0\n",
    "        Bs[i][*[\"C\", \"B\"], *control] = 1.0\n",
    "        Bs[i][*[\"D\", \"C\"], *control] = 1.0\n",
    "        Bs[i][*[\"D\", \"D\"], *control] = 1.0\n",
    "\n",
    "agent = Agent(\n",
    "    As, Bs,\n",
    "    B_action_dependencies=B_action_dependencies,\n",
    "    num_controls=num_controls,\n",
    ")\n",
    "\n",
    "# dummy history\n",
    "action = agent.policies[np.random.randint(0, len(agent.policies))]\n",
    "observation = [np.random.randint(0, d, size=(1, 1)) for d in agent.num_obs]\n",
    "qs_hist = jtu.tree_map(lambda x: jnp.expand_dims(x, 0), agent.D)\n",
    "\n",
    "prior, _ = agent.infer_empirical_prior(action, qs_hist)\n",
    "qs = agent.infer_states(observation, None, prior, None)\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "action_multi = agent.decode_multi_actions(action)\n",
    "action_reconstruct = agent.encode_multi_actions(action_multi)\n",
    "\n",
    "print(\"A_dependencies\", agent.A_dependencies)\n",
    "print(\"B_dependencies\", agent.B_dependencies)\n",
    "print(\"B_action_dependencies\", agent.B_action_dependencies)\n",
    "print(\"original control dims\", agent.num_controls_multi)\n",
    "print(\"flattened control dims\", agent.num_controls)\n",
    "print(\"original B shapes\", [a.data.shape for a in Bs])\n",
    "print(\"flattened B shapes\", [a.shape for a in agent.B])\n",
    "print(\"B normalized\", [jnp.isclose(a.data.sum(0), 1.).all() for a in Bs])\n",
    "print(\"B flat normalized\", [jnp.isclose(a.sum(1), 1.).all() for a in agent.B])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"prior\")\n",
    "pprint([p.round(2) for p in prior])\n",
    "print(\"post\")\n",
    "pprint([p.round(2) for p in qs])\n",
    "print(\"action\")\n",
    "pprint(action)\n",
    "print(\"action_multi\")\n",
    "pprint(action_multi)\n",
    "print(\"action_reconstruct\")\n",
    "pprint(action_reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
