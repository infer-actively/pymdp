{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Knapsack Problem\n",
    "\n",
    "In this notebook, we demonstrate how to solve the knapsack problem, a classic Operations Research problem. In this problem, we have a knapsack with fixed weight capacity and a set of items each associated with a weight and a value. We want fit items into the knapsack in a way such as the total value of all fitted items is as high as possible, however, the sum of item weights cannot exceed the knapsack weight capacity. \n",
    "\n",
    "While is problem is traditionally solved with linear programming, we can convert it into a contextual bandit problem (a simplified 1-stage Markov decision problem) and solve it using pymdp.\n",
    "\n",
    "Let us define our actions `a_i` as whether to include an item or not for each item i. The state `s_i` of the system is defined as whether an item is included or not, i.e., copying the action variables over to the corresponding state variables. We also need another state variable `z` which represents whether the knapsack capacity is exceeded. If an item is included, i.e., `s_i = 1`, we get a reward `r_i`, otherwise, we get a reward of 0 when `s_i = 0`. We can thus define our preference of including valuable items to be proportional to the exponential of reward: `C[s_i] = softmax([0, r_i])`. Our preference on the capacity constraint variable `z` is to never violate it, i.e., `C[z] = [1, 0]`. Since the system is fully observable, we will set all the Categorical/Dirichlet parameters of the emission model (the `A` tensors) to be diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jax import numpy as jnp, random as jr\n",
    "from jax import tree_util as jtu\n",
    "import jax.nn as nn\n",
    "\n",
    "from pymdp.agent import Agent\n",
    "from pymdp import distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item rewards [0.947667   0.9785799  0.33229148 0.46866846 0.5698887 ]\n",
      "item weights [8.3257477  5.90736203 6.15180577 8.64391967 4.4631106 ]\n",
      "item weight sum 33.491945771719365\n"
     ]
    }
   ],
   "source": [
    "# knapsack problem setup\n",
    "num_items = 5\n",
    "max_capacity = 20\n",
    "item_weights = max_capacity / num_items + np.random.uniform(0, 5, size=(num_items,))\n",
    "rewards = jr.uniform(jr.PRNGKey(0), shape=(num_items,), minval=0.0, maxval=1.0)\n",
    "\n",
    "print(\"item rewards\", rewards)\n",
    "print(\"item weights\", item_weights)\n",
    "print(\"item weight sum\", item_weights.sum())\n",
    "\n",
    "# mdp config\n",
    "state_config = {\n",
    "    f\"s_{i}\": {\"elements\": [\"not enclude\", \"include\"], \"depends_on\": [f\"s_{i}\"], \"controlled_by\": [f\"a_{i}\"]} \n",
    "    for i in range(num_items)\n",
    "}\n",
    "state_config[\"z\"] = {\n",
    "    \"elements\": [\"not violated\", \"violated\"], \"depends_on\": [\"z\"], \"controlled_by\": [f\"a_{i}\" for i in range(num_items)]\n",
    "} \n",
    "\n",
    "obs_config = {\n",
    "    k: {\"elements\": v[\"elements\"], \"depends_on\": [k]} for k, v in state_config.items()\n",
    "}\n",
    "\n",
    "act_config = {\n",
    "    f\"a_{i}\": {\"elements\": [\"not enclude\", \"include\"]} \n",
    "    for i in range(num_items)\n",
    "}\n",
    "\n",
    "model_description = {\n",
    "    \"observations\": obs_config,\n",
    "    \"controls\": act_config,\n",
    "    \"states\": state_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shapes [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "B shapes [(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2, 2, 2, 2, 2)]\n"
     ]
    }
   ],
   "source": [
    "model = distribution.compile_model(model_description)\n",
    "\n",
    "print(\"A shapes\", [a.data.shape for a in model.A])\n",
    "print(\"B shapes\", [a.data.shape for a in model.B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shapes [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "B shapes [(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2, 2, 2, 2, 2)]\n",
      "A normalized [np.True_, np.True_, np.True_, np.True_, np.True_, np.True_]\n",
      "B normalized [np.True_, np.True_, np.True_, np.True_, np.True_, np.True_]\n",
      "C normalized [np.True_, np.True_, np.True_, np.True_, np.True_, np.True_]\n"
     ]
    }
   ],
   "source": [
    "def create_identity_transition_factor(mat):\n",
    "    for i in range(mat.shape[1]):\n",
    "        mat[:, i] = np.eye(len(mat))\n",
    "    return mat\n",
    "\n",
    "def create_constraint_factor_z_greater_than(act_dim, maximum, num_items, weights):\n",
    "    # Create an array of shape (2, act_dim, act_dim, ..., act_dim)\n",
    "    tensor_shape = (2,) + (act_dim,) * num_items\n",
    "    \n",
    "    # Create an array with indices from 0 to act_dim - 1 along each dimension\n",
    "    indices = np.indices(tensor_shape[1:])\n",
    "\n",
    "    # Reshape weights to fit indices shape\n",
    "    weights_reshaped = np.array(weights).reshape((-1,) + (1,) * (indices.ndim - 1))\n",
    "    # Multiply weights with matrix that conforms to constraint\n",
    "    result = np.array(indices == (act_dim - 1)) * weights_reshaped\n",
    "\n",
    "    # Calculate the total for each combination of actions\n",
    "    total = np.sum(result, axis=0)\n",
    "    \n",
    "    # Create the tensor based on the total hours condition\n",
    "    tensor = np.where(total > maximum, 1, 0)\n",
    "\n",
    "    # Stack the tensor along the first axis to create the final tensor\n",
    "    tensor = np.stack((1 - tensor, tensor), axis=0)\n",
    "\n",
    "    # make a copy for self state \n",
    "    tensor = np.stack([tensor, tensor], axis=1)\n",
    "    return tensor\n",
    "\n",
    "# update A tensor\n",
    "for i in range(len(model.A)):\n",
    "    model.A[i].data = np.eye(len(model.A[i].data))\n",
    "\n",
    "# update B tensors\n",
    "for i in range(num_items):\n",
    "    model.B[i].data = create_identity_transition_factor(model.B[i].data)\n",
    "\n",
    "model.B[-1].data = create_constraint_factor_z_greater_than(2, max_capacity, num_items, item_weights)\n",
    "\n",
    "# create C tensors\n",
    "preferences = nn.softmax(np.stack([np.zeros_like(rewards), rewards], axis=-1), axis=-1)\n",
    "Cs = [None for _ in range(len(model.A))]\n",
    "for i in range(len(model.A)):\n",
    "    Cs[i] = preferences[i]\n",
    "    \n",
    "Cs[-1] = np.array([1., 0]) # capacity constraint cannot be violated\n",
    "\n",
    "print(\"A shapes\", [a.data.shape for a in model.A])\n",
    "print(\"B shapes\", [a.data.shape for a in model.B])\n",
    "\n",
    "print(\"A normalized\", [np.isclose(a.data.sum(0), 1.).all() for a in model.A])\n",
    "print(\"B normalized\", [np.isclose(a.data.sum(0), 1.).all() for a in model.A])\n",
    "print(\"C normalized\", [np.isclose(a.sum(0), 1.).all() for a in Cs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_action_dependencies = [\n",
    "    [list(model_description[\"controls\"].keys()).index(i) for i in s[\"controlled_by\"]] \n",
    "    for s in model_description[\"states\"].values()\n",
    "]\n",
    "num_controls = [len(c[\"elements\"]) for c in model_description[\"controls\"].values()]\n",
    "\n",
    "agent = Agent(\n",
    "    model.A, model.B, Cs,\n",
    "    B_action_dependencies=B_action_dependencies,\n",
    "    num_controls=num_controls,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = jtu.tree_map(lambda x: jnp.expand_dims(x, axis=0), agent.D)\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "action_multi = agent.decode_multi_actions(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best action [[ 1  1  0  0  1 25]]\n",
      "best action multi [[1 1 0 0 1]]\n",
      "item weights\n",
      "[8.3257477  5.90736203 6.15180577 8.64391967 4.4631106 ]\n",
      "item rewards\n",
      "[0.947667   0.9785799  0.33229148 0.46866846 0.5698887 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"best action\", action)\n",
    "print(\"best action multi\", action_multi)\n",
    "print(\"item weights\")\n",
    "print(item_weights)\n",
    "print(\"item rewards\")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "action [ 1  1  0  0  1 25]\n",
      "action multi [1 1 0 0 1]\n",
      "efe: 3.89, reward: 2.50\n",
      "\n",
      "action [ 0  1  0  1  1 11]\n",
      "action multi [0 1 0 1 1]\n",
      "efe: 3.68, reward: 2.02\n",
      "\n",
      "action [ 0  1  1  0  1 13]\n",
      "action multi [0 1 1 0 1]\n",
      "efe: 3.61, reward: 1.88\n",
      "\n",
      "action [ 1  1  0  0  0 24]\n",
      "action multi [1 1 0 0 0]\n",
      "efe: 3.61, reward: 1.93\n",
      "\n",
      "action [ 1  0  1  0  1 21]\n",
      "action multi [1 0 1 0 1]\n",
      "efe: 3.60, reward: 1.85\n",
      "\n",
      "action [0 1 0 0 1 9]\n",
      "action multi [0 1 0 0 1]\n",
      "efe: 3.45, reward: 1.55\n"
     ]
    }
   ],
   "source": [
    "# compare actions\n",
    "from pymdp import utils\n",
    "for i, idx in enumerate(np.argsort(q_pi[0])[::-1]):\n",
    "    action_multi_f = utils.index_to_combination(agent.policies[idx, 0][-1].tolist(), agent.num_controls_multi)\n",
    "    print(\"\\naction\", agent.policies[idx, 0])\n",
    "    print(\"action multi\", action_multi_f)\n",
    "    print(\"efe: {:.2f}, reward: {:.2f}\".format(G[0, idx], np.sum(rewards * action_multi_f)))\n",
    "    if i == 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymdp_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
