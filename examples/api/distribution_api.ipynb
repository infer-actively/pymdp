{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions API Tutorial\n",
    "\n",
    "In this tutorial, we introduce a feature which makes it easier to build and work with complex active inference models by allowing you to use meaningful names to set up a generative model instead of using numerical indices.\n",
    "\n",
    "The API allows you to:\n",
    "- Give semantic names to tensor dimensions and values (e.g., \"left\", \"right\" instead of 0, 1)\n",
    "- Avoid indexing errors by working with named elements\n",
    "- Build complex models more intuitively using descriptive labels and structures\n",
    "\n",
    "## Tutorial Structure\n",
    "\n",
    "1. Basic Example: A simple grid navigation task built from a structured description using labels, in which the agent has to travel to a goal location.\n",
    "2. A More Advanced Example: A simple foraging task built from a structured description using labels, in which the agent has to search for apples and eat them while they spawn at a set rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 1: Grid World Navigation\n",
    "\n",
    "Let's start with the simple example: an agent moving in a 1D grid world. Our agent can be in one of four positions: \"left\", \"centre_left\", \"centre_right\", or \"right\", and can take actions \"move_right\" or \"move_left\". We give an example of setting up one agent as well as three batched agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.tree_util as jtu\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jr\n",
    "from pymdp.agent import Agent\n",
    "from pymdp.distribution import compile_model\n",
    "from pymdp.envs.env import Env\n",
    "from pymdp.envs import rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [\"left\", \"centre_left\", \"centre_right\", \"right\"]\n",
    "actions = [\"move_left\", \"move_right\"]\n",
    "\n",
    "model_description = {\n",
    "    \"observations\": {\n",
    "        \"position_obs\": {\n",
    "            \"elements\": positions, \n",
    "            \"depends_on\": [\"position\"] # we specify that the observation depends on the \"position\" state factor\n",
    "        },\n",
    "    },\n",
    "    \"controls\": {\n",
    "        \"movement\": {\"elements\": actions} # we specify the available actions\n",
    "    },\n",
    "    \"states\": {\n",
    "        \"position\": {\n",
    "            \"elements\": positions, \n",
    "            \"depends_on\": [\"position\"],  # our current position depends on previous position...\n",
    "            \"controlled_by\": [\"movement\"]  # ...and the movement action taken\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# compile the model structure from the description\n",
    "model = compile_model(model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a generative model structure using the model description, however the model is currently empty. So now, we fill it in by indexing using the labels we provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the likelihood (A) tensor\n",
    "# the observations have an identical mapping to the states (i.e., the agent will perfectly observe its position)\n",
    "model.A[\"position_obs\"][\"left\", \"left\"] = 1.0\n",
    "model.A[\"position_obs\"][\"centre_left\", \"centre_left\"] = 1.0\n",
    "model.A[\"position_obs\"][\"centre_right\", \"centre_right\"] = 1.0\n",
    "model.A[\"position_obs\"][\"right\", \"right\"] = 1.0\n",
    "# model.A[\"position_obs\"].data = jnp.eye(len(positions)) # you could also use the .data attribute to set the identity mapping directly\n",
    "\n",
    "# fill in the transition model (B) tensor\n",
    "# note that it's specified as [\"to\", \"from\", \"action\"]\n",
    "# moving right\n",
    "model.B[\"position\"][\"centre_left\", \"left\", \"move_right\"] = 1.0     \n",
    "model.B[\"position\"][\"centre_right\", \"centre_left\", \"move_right\"] = 1.0  \n",
    "model.B[\"position\"][\"right\", \"centre_right\", \"move_right\"] = 1.0    \n",
    "model.B[\"position\"][\"right\", \"right\", \"move_right\"] = 1.0           \n",
    "\n",
    "# moving left  \n",
    "model.B[\"position\"][\"left\", \"left\", \"move_left\"] = 1.0              \n",
    "model.B[\"position\"][\"left\", \"centre_left\", \"move_left\"] = 1.0       \n",
    "model.B[\"position\"][\"centre_left\", \"centre_right\", \"move_left\"] = 1.0  \n",
    "model.B[\"position\"][\"centre_right\", \"right\", \"move_left\"] = 1.0    \n",
    "\n",
    "# set preferences (C) tensor - prefer to be at \"centre_right\"\n",
    "model.C[\"position_obs\"][\"centre_left\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the agent (via the Agent object) and have it infer which state it is in via an observation and select an action according to it's goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current belief about position: left\n",
      "Goal position: centre_left\n",
      "Action chosen: move_right\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 \n",
    "gamma = 10 # deterministic behaviour; make it smaller for stochastic behaviour\n",
    "\n",
    "# create agent\n",
    "agent = Agent(**model, batch_size = batch_size, gamma = gamma)\n",
    "\n",
    "# set up initial observation to be \"left\"\n",
    "observation = jnp.broadcast_to(jnp.zeros(1), (batch_size, 1)) # broadcast to batch size and add a time dimension\n",
    "\n",
    "# get the prior\n",
    "qs_init = jtu.tree_map(lambda x: jnp.expand_dims(x, 1), agent.D) # qs needs a time dimension too\n",
    "\n",
    "# print initial beliefs, goal, and action chosen\n",
    "qs = agent.infer_states([observation], qs_init)\n",
    "print(f\"Current belief about position: {positions[jnp.argmax(qs[0][0])]}\")\n",
    "qs = [jnp.squeeze(q, 1) for q in qs]\n",
    "\n",
    "print(f\"Goal position: {positions[jnp.argmax(agent.C[0])]}\")\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "print(f\"Action chosen: {actions[action[0][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run multiple trials in parallel, each with a different initial observation (i.e., different initial position), via the batching feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0's current belief about position: left\n",
      "Agent 1's current belief about position: centre_right\n",
      "Agent 2's current belief about position: right\n",
      "\n",
      "Goal position for all agents: centre_left\n",
      "\n",
      "Agent 0's action chosen: move_right\n",
      "Agent 1's action chosen: move_left\n",
      "Agent 2's action chosen: move_left\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3 # running 3 trials in parallel\n",
    "gamma = 10 # deterministic behaviour; make it smaller for stochastic behaviour\n",
    "\n",
    "# create agent\n",
    "agent = Agent(**model, batch_size = batch_size, gamma = gamma)\n",
    "\n",
    "# set up initial observations to be \"left\", \"centre_right\", and \"right\"\n",
    "observation = jnp.array([[0], [2], [3]])\n",
    "\n",
    "# get the prior\n",
    "qs_init = jtu.tree_map(lambda x: jnp.expand_dims(x, 1), agent.D) # qs needs a time dimension too\n",
    "\n",
    "# print goal and initial beliefs\n",
    "qs = agent.infer_states([observation], qs_init)\n",
    "for a in range(batch_size): \n",
    "    print(f\"Agent {a}'s current belief about position: {positions[jnp.argmax(qs[0][a])]}\")\n",
    "qs = [jnp.squeeze(q, 1) for q in qs]\n",
    "\n",
    "print(f\"\\nGoal position for all agents: {positions[jnp.argmax(agent.C[0])]}\\n\")\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "for a in range(batch_size): \n",
    "    print(f\"Agent {a}'s action chosen: {actions[action[a][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: Apple Foraging Task\n",
    "\n",
    "Now let's look at a slightly more complex example: an apple foraging task. Here, we have a 1x3 grid, with a \"left\", \"centre\", and \"right\" cell. These are orchard cells where an apple can grow at a set rate (1/3). The agent's objective is to find apples and eat them as they get a reward to eat apples. The agent can stay, move_left, move_right, or eat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_locations = 3 # these correspond to \"left\", \"centre\", and \"right\" and you can just specify [\"left\", \"centre\", \"right\"] but we use numbers to show how to use the 'size' key instead of 'elements'\n",
    "item_list = [\"orchard\", \"apple\"]\n",
    "\n",
    "model_description = {\n",
    "    \"observations\": {\n",
    "        \"location_obs\": {\"size\": num_locations, # if you want to use numbers instead of strings, you can use the 'size' key instead of 'elements'\n",
    "                         \"depends_on\": [\"location_state\"],\n",
    "        },\n",
    "        \"item_obs\": {\"elements\": item_list, # \"elements\" key for strings\n",
    "                     \"depends_on\": [\"location_state\", \"left_state\", \"centre_state\", \"right_state\"],\n",
    "        },\n",
    "        \"reward_obs\": {\"elements\": [\"no_reward\", \"reward\"],\n",
    "                       \"depends_on\": [\"reward_state\"],\n",
    "        },\n",
    "    },\n",
    "    \"controls\": {\n",
    "        \"move\": {\"elements\": [\"stay\", \"move_left\", \"move_right\"],\n",
    "        },\n",
    "        \"eat\": {\"elements\": [\"noop\", \"eat\"], # noop = no-operation\n",
    "        # note that if you cannot control a state, you still need to add \n",
    "        # an action for it (e.g., with elements: [\"null\"]) for the model to be initialised \n",
    "        # with the correct dimensions\n",
    "        },\n",
    "    },\n",
    "    \"states\": {\n",
    "        \"location_state\": {\"size\": num_locations,\n",
    "                           \"depends_on\": [\"location_state\"],\n",
    "                           \"controlled_by\": [\"move\"],\n",
    "        },\n",
    "        \"reward_state\": {\"elements\": [\"no_reward\", \"reward\"],\n",
    "                            # if you have more than one dependency,the first dependency is its own state factor (at the previous timestep), \n",
    "                            # then add the other dependencies in the order they are specified (you can skip over some state factors)\n",
    "                            \"depends_on\": [\"reward_state\", \"location_state\", \n",
    "                                           \"left_state\", \"centre_state\", \"right_state\"],\n",
    "                            \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "        \"left_state\": {\"elements\": item_list,\n",
    "                        \"depends_on\": [\"left_state\", \"location_state\"], \n",
    "                        \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "        \"centre_state\": {\"elements\": item_list,\n",
    "                        \"depends_on\": [\"centre_state\", \"location_state\"],\n",
    "                        \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "        \"right_state\": {\"elements\": item_list,\n",
    "                        \"depends_on\": [\"right_state\", \"location_state\"],\n",
    "                        \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "model = compile_model(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a generative model structure using the model description, however the model is currently empty. So now, we fill it in by indexing using the labels we provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SPECIFY THE A TENSOR\n",
    "'''\n",
    "# identity mapping for the observations regarding location and reward\n",
    "model.A[\"location_obs\"].data = jnp.eye(len(model.A[\"location_obs\"].data))\n",
    "model.A[\"reward_obs\"].data = jnp.eye(len(model.A[\"reward_obs\"].data))\n",
    "\n",
    "# in any of the locations, the agent may observe apple or orchard\n",
    "model.A[\"item_obs\"][\"apple\", 0, \"apple\", :, :] = 1.0\n",
    "model.A[\"item_obs\"][\"apple\", 1, :, \"apple\", :] = 1.0\n",
    "model.A[\"item_obs\"][\"apple\", 2, :, :, \"apple\"] = 1.0\n",
    "model.A[\"item_obs\"][\"orchard\", 0, \"orchard\", :, :] = 1.0\n",
    "model.A[\"item_obs\"][\"orchard\", 1, :, \"orchard\", :] = 1.0\n",
    "model.A[\"item_obs\"][\"orchard\", 2, :, :, \"orchard\"] = 1.0\n",
    "model.A[\"item_obs\"].data = model.A[\"item_obs\"].data + 1e-3 # add a small amount of noise to the observations\n",
    "\n",
    "'''\n",
    "SPECIFY THE B TENSOR\n",
    "'''\n",
    "\n",
    "# for moving between locations\n",
    "# (to, from, action)\n",
    "valid_transitions = [\n",
    "    # from 0 (left)\n",
    "    (0, 0, \"stay\"), # from left to left, stay\n",
    "    (1, 0, \"move_right\"), # from left to centre, move right\n",
    "    (2, 0, \"move_left\"), # from left to right, move left\n",
    "\n",
    "    # from 1 (centre)\n",
    "    (0, 1, \"move_left\"), # from centre to left, move left\n",
    "    (1, 1, \"stay\"), # from centre to centre, stay\n",
    "    (2, 1, \"move_right\"), # from centre to right, move right\n",
    "\n",
    "    # from 2 (right)\n",
    "    (0, 2, \"move_right\"), # from right to left, move right\n",
    "    (1, 2, \"move_left\"), # from right to centre, move left\n",
    "    (2, 2, \"stay\"), # from right to right, stay\n",
    "]\n",
    "\n",
    "for to_state, from_state, action in valid_transitions:\n",
    "    model.B[\"location_state\"][to_state, from_state, action] = 1.0\n",
    "\n",
    "# again, remember the reward states will be set as [\"to\", \"from\", ...dependencies..., \"action\"]\n",
    "# if the agent sees an apple and does not eat the apple (i.e., noop), it does not get a reward\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 0, \"apple\", :, :, \"noop\"] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 1, :, \"apple\", :, \"noop\"] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 2, :, :, \"apple\", \"noop\"] = 1.0\n",
    "\n",
    "# if the agent sees an orchard, it does not get a reward regardless of its actions\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 0, \"orchard\", :, :, :] = 1.0 \n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 1, :, \"orchard\", :, :] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 2, :, :, \"orchard\", :] = 1.0\n",
    "\n",
    "# from a reward state, there will always be no reward in the next timestep regardless of the action\n",
    "model.B[\"reward_state\"][\"no_reward\", \"reward\", 0, :, :, :, :] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"reward\", 1, :, :, :, :] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"reward\", 2, :, :, :, :] = 1.0\n",
    "\n",
    "# if the agent sees an orchard and eats, it will not get a reward\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 0, \"orchard\", :, :, \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 1, :, \"orchard\", :, \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 2, :, :, \"orchard\", \"eat\"] = 0.0\n",
    "\n",
    "# if the agent sees an apple and eats the apple, it gets a reward and never not get a reward\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 0, \"apple\", :, :, \"eat\"] = 0.0 \n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 1, :, \"apple\", :, \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 2, :, :, \"apple\", \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 0, \"apple\", :, :, \"eat\"] = 1.0 \n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 1, :, \"apple\", :, \"eat\"] = 1.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 2, :, :, \"apple\", \"eat\"] = 1.0\n",
    "\n",
    "apple_spawn_locations = [\"left_state\", \"centre_state\", \"right_state\"]\n",
    "apple_spawn_rate = 1/3\n",
    "for i, state in enumerate(apple_spawn_locations):\n",
    "    model.B[state][\"orchard\", \"orchard\", :, :] = 1.0 - apple_spawn_rate # no spawn\n",
    "    model.B[state][\"apple\", \"orchard\", :, :] = apple_spawn_rate # spawn\n",
    "    for agent_location in range(num_locations):\n",
    "        if i == agent_location:\n",
    "            # if the agent does not eat the apple (noop), the apple will stay in the cell\n",
    "            model.B[state][\"apple\", \"apple\", agent_location, \"noop\"] = 1.0\n",
    "            # if the agent eats the apple, it will become an orchard cell\n",
    "            model.B[state][\"orchard\", \"apple\", agent_location, \"eat\"] = 1.0\n",
    "    model.B[state].data = model.B[state].data + 1e-3 # add a small amount of noise to the observations\n",
    "\n",
    "'''\n",
    "SPECIFY THE C TENSOR. \n",
    "'''\n",
    "model.C[\"reward_obs\"][\"reward\"] = 1.0\n",
    "\n",
    "'''\n",
    "NORMALISE THE TENSORS\n",
    "'''\n",
    "\n",
    "model.A[\"location_obs\"].normalize()\n",
    "model.A[\"item_obs\"].normalize()\n",
    "model.A[\"reward_obs\"].normalize()\n",
    "\n",
    "model.B[\"location_state\"].normalize()\n",
    "model.B[\"reward_state\"].normalize()\n",
    "model.B[\"left_state\"].normalize()\n",
    "model.B[\"centre_state\"].normalize()\n",
    "model.B[\"right_state\"].normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "gamma = 1.0\n",
    "\n",
    "agent = Agent(**model, batch_size=batch_size, learn_A=False, learn_B=False, gamma=gamma, sampling_mode=\"full\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
