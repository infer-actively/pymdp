{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Model` and `Distribution` Tutorial\n",
    "\n",
    "This tutorial walks through usage of the `Model` and `Distribution` classes. The `Model` class wraps the `A`, `B`, `C`, and `D` arrays into a unified model object, whose structure can be generated automatically from a JSON-like configuration dict. The configuration dict, often labelled `model_description`, allows you to assign string-valued names to the random variables of the model (observations, hidden states, and control states), and to specify dependencies between these random variables (via the `\"depends_on\"` key within each variable-specific sub-dict). \n",
    "\n",
    "Within the `Model` object, each component distribution is an instance of the `Distribution` class. The `Distribution` class can use string- or integer-valued labels for both the axes and specific indices along those axes. The intention of these classes and their named dimensions/values, is to provide users a more user-friendly, interpretable entrypoint for building discrete generative models in `pymdp`.\n",
    "\n",
    "In the summary, the advantages of the `Model` and `Distribution` classes are:\n",
    "- Reduce memory burden on the user (and the indexing errors that often ensue) by working with named axes and elements (e.g., \"left\", \"right\" instead of 0, 1)\n",
    "- Increase interpretability (if applicable) and intuition in the generation and inspection of the POMDP model\n",
    "- Flexible specification of dependencies between random variables via string labels\n",
    "\n",
    "## Tutorial Structure\n",
    "\n",
    "1. Basic Example: A simple grid navigation task built from a structured description using labels, in which the agent has to travel to a goal location.\n",
    "2. A More Advanced Example: A simple foraging task built from a structured description using labels, in which the agent has to search for apples and eat them while they spawn at a set rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 1: Grid World Navigation\n",
    "\n",
    "Let's start with the simple example: an agent moving horizontally in a 1D grid world. Our agent can be in one of four positions: \"left\", \"center_left\", \"center_right\", or \"right\", and can take actions \"move_right\" or \"move_left\". We give an example of setting up a single agent and then an example of running three agents in parallel by using the`batch_size` argument to the `Agent` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.tree_util as jtu\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jr\n",
    "from pymdp.agent import Agent\n",
    "from pymdp.distribution import compile_model\n",
    "from pymdp.envs.env import Env\n",
    "from pymdp.envs import rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [\"left\", \"center_left\", \"center_right\", \"right\"]\n",
    "actions = [\"move_left\", \"move_right\"]\n",
    "\n",
    "model_description = {\n",
    "    \"observations\": {\n",
    "        \"position_obs\": {\n",
    "            \"elements\": positions, \n",
    "            \"depends_on\": [\"position\"] # we specify that the observation depends on the \"position\" state factor\n",
    "        },\n",
    "    },\n",
    "    \"controls\": {\n",
    "        \"movement\": {\"elements\": actions} # we specify the available actions\n",
    "    },\n",
    "    \"states\": {\n",
    "        \"position\": {\n",
    "            \"elements\": positions, \n",
    "            \"depends_on\": [\"position\"],  # our current position depends on previous position...\n",
    "            \"controlled_by\": [\"movement\"]  # ...and the movement action taken\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# compile the model structure from the description\n",
    "model = compile_model(model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a generative model structure using the model description, however the model's parameters are currently uninitialized arrays of zeros. So now, we can fill in the parameter tensors by using the axis and element labels we defined in the model description dict, to set values in particular indices of these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the likelihood (A) tensor\n",
    "# the observations have an identical mapping to the states (i.e., the agent will perfectly observe its position)\n",
    "model.A[\"position_obs\"][\"left\", \"left\"] = 1.0\n",
    "model.A[\"position_obs\"][\"center_left\", \"center_left\"] = 1.0\n",
    "model.A[\"position_obs\"][\"center_right\", \"center_right\"] = 1.0\n",
    "model.A[\"position_obs\"][\"right\", \"right\"] = 1.0\n",
    "# model.A[\"position_obs\"].data = jnp.eye(len(positions)) # you could also use the .data attribute to set the identity mapping directly\n",
    "\n",
    "# fill in the transition model (B) tensor\n",
    "# note that it's specified as [\"to\", \"from\", \"action\"]\n",
    "# moving right\n",
    "model.B[\"position\"][\"center_left\", \"left\", \"move_right\"] = 1.0     \n",
    "model.B[\"position\"][\"center_right\", \"center_left\", \"move_right\"] = 1.0  \n",
    "model.B[\"position\"][\"right\", \"center_right\", \"move_right\"] = 1.0    \n",
    "model.B[\"position\"][\"right\", \"right\", \"move_right\"] = 1.0           \n",
    "\n",
    "# moving left  \n",
    "model.B[\"position\"][\"left\", \"left\", \"move_left\"] = 1.0              \n",
    "model.B[\"position\"][\"left\", \"center_left\", \"move_left\"] = 1.0       \n",
    "model.B[\"position\"][\"center_left\", \"center_right\", \"move_left\"] = 1.0  \n",
    "model.B[\"position\"][\"center_right\", \"right\", \"move_left\"] = 1.0    \n",
    "\n",
    "# set preferences (C) tensor - prefer to be at \"center_right\"\n",
    "model.C[\"position_obs\"][\"center_left\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the agent (via the Agent object) and have it infer which state it is in via an observation and select an action according to it's goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current belief about position: left\n",
      "Goal position: center_left\n",
      "Action chosen: move_right\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 \n",
    "gamma = 10 # deterministic behavior; make it gamma smaller for stochastic behavior\n",
    "\n",
    "# create agent\n",
    "agent = Agent(**model, batch_size=batch_size, gamma=gamma)\n",
    "\n",
    "# set up initial observation to be \"left\"\n",
    "observation = jnp.zeros((batch_size, 1)) # broadcast to batch size and add a time dimension\n",
    "\n",
    "# get the prior\n",
    "qs_init = jtu.tree_map(lambda x: jnp.expand_dims(x, 1), agent.D) # qs needs a time dimension too\n",
    "\n",
    "# print initial beliefs, goal, and action chosen\n",
    "qs = agent.infer_states([observation], qs_init)\n",
    "print(f\"Current belief about position: {positions[jnp.argmax(qs[0][0])]}\")\n",
    "qs = [jnp.squeeze(q, 1) for q in qs]\n",
    "\n",
    "print(f\"Goal position: {positions[jnp.argmax(agent.C[0])]}\")\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action_idx = agent.sample_action(q_pi)\n",
    "print(f\"Action chosen: {actions[action_idx[0][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run multiple agents or independent trials in parallel, each with a different initial observation (i.e., different initial position), via the batching feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0's current belief about position: left\n",
      "Agent 1's current belief about position: center_right\n",
      "Agent 2's current belief about position: right\n",
      "\n",
      "Goal position for all agents: center_left\n",
      "\n",
      "Agent 0's action chosen: move_right\n",
      "Agent 1's action chosen: move_left\n",
      "Agent 2's action chosen: move_left\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3 # running 3 trials or agents in parallel\n",
    "gamma = 10     # deterministic behavior; make gamma smaller for stochastic behavior\n",
    "\n",
    "# create agent\n",
    "agent = Agent(**model, batch_size=batch_size, gamma=gamma)\n",
    "\n",
    "# set up different initial observations for each agent: \"left\", \"center_right\", and \"right\"\n",
    "observation = [jnp.array([[0], [2], [3]])] # wrap in a list to indicate the single modality; observation[0].shape = (batch_size, 1)\n",
    "\n",
    "# get the prior\n",
    "qs_init = jtu.tree_map(lambda x: jnp.expand_dims(x, 1), agent.D) # qs needs a time dimension too\n",
    "\n",
    "# print goal and initial beliefs\n",
    "qs = agent.infer_states(observation, qs_init)\n",
    "for a in range(batch_size): \n",
    "    print(f\"Agent {a}'s current belief about position: {positions[jnp.argmax(qs[0][a])]}\")\n",
    "qs = [jnp.squeeze(q, 1) for q in qs]\n",
    "\n",
    "print(f\"\\nGoal position for all agents: {positions[jnp.argmax(agent.C[0])]}\\n\")\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "for a in range(batch_size): \n",
    "    print(f\"Agent {a}'s action chosen: {actions[action[a][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: Apple Foraging Task\n",
    "\n",
    "Now let's look at a slightly more complex example: an apple foraging task. Here, we have a 1x3 grid, with a \"left\", \"center\", and \"right\" cell. These are orchard cells where an apple can grow at a set rate (1/3). The agent's objective is to find apples and eat them as they get a reward to eat apples. The agent can stay, move_left, move_right, or eat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_locations = 3 # these correspond to \"left\", \"center\", and \"right\" and you can just specify [\"left\", \"center\", \"right\"] but we use numbers to show how to use the 'size' key instead of 'elements'\n",
    "item_list = [\"orchard\", \"apple\"]\n",
    "\n",
    "model_description = {\n",
    "    \"observations\": {\n",
    "        \"location_obs\": {\"size\": num_locations, # if you want to use numbers instead of strings, you can use the 'size' key instead of 'elements'\n",
    "                         \"depends_on\": [\"location_state\"],\n",
    "        },\n",
    "        \"item_obs\": {\"elements\": item_list, # \"elements\" key for strings\n",
    "                     \"depends_on\": [\"location_state\", \"left_state\", \"center_state\", \"right_state\"],\n",
    "        },\n",
    "        \"reward_obs\": {\"elements\": [\"no_reward\", \"reward\"],\n",
    "                       \"depends_on\": [\"reward_state\"],\n",
    "        },\n",
    "    },\n",
    "    \"controls\": {\n",
    "        \"move\": {\"elements\": [\"stay\", \"move_left\", \"move_right\"],\n",
    "        },\n",
    "        \"eat\": {\"elements\": [\"noop\", \"eat\"], # noop = no-operation\n",
    "        # note that if you cannot control a state, you still need to add \n",
    "        # an action for it (e.g., with elements: [\"null\"]) for the model to be initialized \n",
    "        # with the correct dimensions\n",
    "        },\n",
    "    },\n",
    "    \"states\": {\n",
    "        \"location_state\": {\"size\": num_locations,\n",
    "                           \"depends_on\": [\"location_state\"],\n",
    "                           \"controlled_by\": [\"move\"],\n",
    "        },\n",
    "        \"reward_state\": {\"elements\": [\"no_reward\", \"reward\"],\n",
    "                            # if you have more than one dependency, the first dependency is its own state factor (at the previous timestep), \n",
    "                            # then add the other dependencies in the order they are specified (you can skip over some state factors)\n",
    "                            \"depends_on\": [\"reward_state\", \"location_state\", \n",
    "                                           \"left_state\", \"center_state\", \"right_state\"],\n",
    "                            \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "        \"left_state\": {\"elements\": item_list,\n",
    "                        \"depends_on\": [\"left_state\", \"location_state\"], \n",
    "                        \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "        \"center_state\": {\"elements\": item_list,\n",
    "                        \"depends_on\": [\"center_state\", \"location_state\"],\n",
    "                        \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "        \"right_state\": {\"elements\": item_list,\n",
    "                        \"depends_on\": [\"right_state\", \"location_state\"],\n",
    "                        \"controlled_by\": [\"eat\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "model = compile_model(model_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a generative model structure using the model description, however the model's parameters are currently uninitialized arrays of zeros. So now, we can fill in the parameter tensors by using the axis and element labels we defined in the model description dict, to set values in particular indices of these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SPECIFY THE A TENSOR\n",
    "'''\n",
    "# identity mapping for the observations regarding location and reward\n",
    "model.A[\"location_obs\"].data = jnp.eye(len(model.A[\"location_obs\"].data))\n",
    "model.A[\"reward_obs\"].data = jnp.eye(len(model.A[\"reward_obs\"].data))\n",
    "\n",
    "# in any of the locations, the agent may observe apple or orchard\n",
    "model.A[\"item_obs\"][\"apple\", 0, \"apple\", :, :] = 1.0\n",
    "model.A[\"item_obs\"][\"apple\", 1, :, \"apple\", :] = 1.0\n",
    "model.A[\"item_obs\"][\"apple\", 2, :, :, \"apple\"] = 1.0\n",
    "model.A[\"item_obs\"][\"orchard\", 0, \"orchard\", :, :] = 1.0\n",
    "model.A[\"item_obs\"][\"orchard\", 1, :, \"orchard\", :] = 1.0\n",
    "model.A[\"item_obs\"][\"orchard\", 2, :, :, \"orchard\"] = 1.0\n",
    "model.A[\"item_obs\"].data = model.A[\"item_obs\"].data + 1e-3 # add a small amount of noise to the observations\n",
    "\n",
    "'''\n",
    "SPECIFY THE B TENSOR\n",
    "'''\n",
    "\n",
    "# for moving between locations\n",
    "# (to, from, action)\n",
    "valid_transitions = [\n",
    "    # from 0 (left)\n",
    "    (0, 0, \"stay\"), # from left to left, stay\n",
    "    (1, 0, \"move_right\"), # from left to center, move right\n",
    "    (2, 0, \"move_left\"), # from left to right, move left\n",
    "\n",
    "    # from 1 (center)\n",
    "    (0, 1, \"move_left\"), # from center to left, move left\n",
    "    (1, 1, \"stay\"), # from center to center, stay\n",
    "    (2, 1, \"move_right\"), # from center to right, move right\n",
    "\n",
    "    # from 2 (right)\n",
    "    (0, 2, \"move_right\"), # from right to left, move right\n",
    "    (1, 2, \"move_left\"), # from right to cecenterntre, move left\n",
    "    (2, 2, \"stay\"), # from right to right, stay\n",
    "]\n",
    "\n",
    "for to_state, from_state, action in valid_transitions:\n",
    "    model.B[\"location_state\"][to_state, from_state, action] = 1.0\n",
    "\n",
    "# again, remember the reward states will be set as [\"to\", \"from\", ...dependencies..., \"action\"]\n",
    "# if the agent sees an apple and does not eat the apple (i.e., noop), it does not get a reward\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 0, \"apple\", :, :, \"noop\"] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 1, :, \"apple\", :, \"noop\"] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 2, :, :, \"apple\", \"noop\"] = 1.0\n",
    "\n",
    "# if the agent sees an orchard, it does not get a reward regardless of its actions\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 0, \"orchard\", :, :, :] = 1.0 \n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 1, :, \"orchard\", :, :] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 2, :, :, \"orchard\", :] = 1.0\n",
    "\n",
    "# from a reward state, there will always be no reward in the next timestep regardless of the action\n",
    "model.B[\"reward_state\"][\"no_reward\", \"reward\", 0, :, :, :, :] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"reward\", 1, :, :, :, :] = 1.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"reward\", 2, :, :, :, :] = 1.0\n",
    "\n",
    "# if the agent sees an orchard and eats, it will not get a reward\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 0, \"orchard\", :, :, \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 1, :, \"orchard\", :, \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 2, :, :, \"orchard\", \"eat\"] = 0.0\n",
    "\n",
    "# if the agent sees an apple and eats the apple, it gets a reward and never not get a reward\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 0, \"apple\", :, :, \"eat\"] = 0.0 \n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 1, :, \"apple\", :, \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"no_reward\", \"no_reward\", 2, :, :, \"apple\", \"eat\"] = 0.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 0, \"apple\", :, :, \"eat\"] = 1.0 \n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 1, :, \"apple\", :, \"eat\"] = 1.0\n",
    "model.B[\"reward_state\"][\"reward\", \"no_reward\", 2, :, :, \"apple\", \"eat\"] = 1.0\n",
    "\n",
    "apple_spawn_locations = [\"left_state\", \"center_state\", \"right_state\"]\n",
    "apple_spawn_rate = 1/3\n",
    "for i, state in enumerate(apple_spawn_locations):\n",
    "    model.B[state][\"orchard\", \"orchard\", :, :] = 1.0 - apple_spawn_rate # no spawn\n",
    "    model.B[state][\"apple\", \"orchard\", :, :] = apple_spawn_rate # spawn\n",
    "    for agent_location in range(num_locations):\n",
    "        if i == agent_location:\n",
    "            # if the agent does not eat the apple (noop), the apple will stay in the cell\n",
    "            model.B[state][\"apple\", \"apple\", agent_location, \"noop\"] = 1.0\n",
    "            # if the agent eats the apple, it will become an orchard cell\n",
    "            model.B[state][\"orchard\", \"apple\", agent_location, \"eat\"] = 1.0\n",
    "    model.B[state].data = model.B[state].data + 1e-3 # add a small amount of noise to the observations\n",
    "\n",
    "'''\n",
    "SPECIFY THE C TENSOR. \n",
    "'''\n",
    "model.C[\"reward_obs\"][\"reward\"] = 1.0\n",
    "\n",
    "'''\n",
    "NORMALISE THE TENSORS\n",
    "'''\n",
    "\n",
    "model.A[\"location_obs\"].normalize()\n",
    "model.A[\"item_obs\"].normalize()\n",
    "model.A[\"reward_obs\"].normalize()\n",
    "\n",
    "model.B[\"location_state\"].normalize()\n",
    "model.B[\"reward_state\"].normalize()\n",
    "model.B[\"left_state\"].normalize()\n",
    "model.B[\"center_state\"].normalize()\n",
    "model.B[\"right_state\"].normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "gamma = 1.0\n",
    "\n",
    "agent = Agent(**model, batch_size=batch_size, learn_A=False, learn_B=False, gamma=gamma, sampling_mode=\"full\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
