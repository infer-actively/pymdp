{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE This is a version of the original simglucose experiment (done using Genius SDK) and modified to use JAX pyMDP. It may contain errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Insulin Pump with the Genius SDK\n",
    "\n",
    "One of the most powerful features of Genius is agents that learn to make decisions under uncertainty. In this example, we will use the Genius SDK to build an agent that acts as an insulin pump for users with Type 1 Diabetes.\n",
    "\n",
    "According to [The Cleveland Clinic](https://my.clevelandclinic.org/health/articles/insulin-pumps) an insulin pump is:\n",
    "> A wearable medical device that supplies a continuous flow of rapid-acting insulin underneath your skin. Most pumps are small, computerized devices that are roughly the size of a juice box or a deck of cards. Insulin pumps are an alternative to multiple daily injection (MDI) insulin therapy (syringe or pen injections) for people with diabetes who require insulin to manage the condition.\n",
    "\n",
    "We can simulate diabetic users with a gym environment called [SimGlucose](https://github.com/jxx123/simglucose), which is based on the FDA-approved [UVa/Padova Simulator](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4454102/) (2008 version). It models 30 virtual patients (10 adolescents, 10 adults, 10 children) who consume meals at random times, causing their blood glucose levels to rise. Besides the patient's age, our agent will only be able to observe the user's CGM reading, which is a noisy signal of their blood glucose levels. Minute by minute, the agent must administer the right level of basal insulin to keep the user's blood glucose levels in a healthy range.\n",
    "\n",
    "For diabetes type-1 in general, the healthy range for for blood glucose levels is around 70-180 mg/dL. \"Time in Range\" (TIR) is the percentage of time that the blood glucose level is in the healthy range, and the general target for insulin pumps is 70% TIR. For children, who have a higher risk of hypoglycemia, 70% TIR is quite challenging, and the main goal is to avoid hypoglycemia. In our example, we will only use the \"adult\" and \"adolescent\" patients. \n",
    "\n",
    "If you want to challenge yourself, try adding in the \"child\" patients in `generate_glucose_gyms()` and see if you can get the agent to learn a policy that works for all ages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "We will need the following:\n",
    "\n",
    "1. A running Genius server.\n",
    "2. A Python environment with version 3.13 or higher...\n",
    "3. ...with the following packages installed (using `pip` or `uv`):\n",
    "\n",
    "```python\n",
    "[\n",
    "  \"genius-client-sdk>=4.0.3\",\n",
    "  \"gymnasium>=0.29.1\",\n",
    "  \"gym>=0.9.4\",\n",
    "  \"numpy>=1.26.4\",\n",
    "  \"pyvfg~=6.0.0\",\n",
    "  \"simglucose>=0.2.9\",\n",
    "  \"setuptools>=78.1.0\",\n",
    "  \"matplotlib>=3.8.0\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "\n",
    "\n",
    "from pymdp.agent import Agent\n",
    "from pymdp.distribution import compile_model\n",
    "\n",
    "# import genius_client_sdk as sdk\n",
    "# from genius_client_sdk.utils import send_http_request\n",
    "# from genius_client_sdk.pomdp import POMDPModel\n",
    "# from genius_client_sdk.agent import GeniusAgent\n",
    "# from genius_client_sdk.utils import (\n",
    "#     control_map,\n",
    "# )\n",
    "from gymnasium.envs.registration import EnvSpec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from simglucose.simulation.env import T1DSimEnv\n",
    "from simglucose.controller.base import Action\n",
    "\n",
    "from simglucose.patient.t1dpatient import T1DPatient\n",
    "from simglucose.sensor.cgm import CGMSensor\n",
    "from simglucose.actuator.pump import InsulinPump\n",
    "from simglucose.simulation.scenario_gen import RandomScenario\n",
    "from datetime import datetime\n",
    "from matplotlib.animation import FuncAnimation # Add this import\n",
    "\n",
    "from IPython import display\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a directory for output files\n",
    "os.makedirs('out', exist_ok=True)\n",
    "\n",
    "# np.random.seed(42) # JAX has no global seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Environment\n",
    "\n",
    "Next we'll set up some classes to simulate the users. SimGlucose can model 10 patients of each age group (child, adolescent, adult). The simulation provides many outputs, including a reward signal and a risk score. For the sake of realism, our insulin pump won't be able to observe these. But we can use the risk score to end the episode when the user's blood glucose level goes critically high or low (risk level = 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions for simglucose Environment\"\"\"\n",
    "\n",
    "def init_glucose_env(env_class, patient_age, patient_number):\n",
    "    \"\"\"Creates a SimGlucose environment for a given patient age and number.\"\"\"\n",
    "\n",
    "    assert patient_age in [\"adolescent\", \"child\", \"adult\"]\n",
    "    assert patient_number in range(0, 10)\n",
    "    # set number to a string with two digits\n",
    "    patient_number = str(patient_number).zfill(2)\n",
    "\n",
    "    try:\n",
    "        env = env_class(patient_name=f'{patient_age}#0{patient_number}')\n",
    "    except:\n",
    "        raise \"Use an Environment with gym 0.9.4 to use SimGlucose.\"\n",
    "    return env\n",
    "\n",
    "def get_env_name(patient_age, patient_number):\n",
    "    return f\"simglucose-{patient_age}-v{patient_number}\"\n",
    "\n",
    "def generate_glucose_gyms(env_class):\n",
    "    \"\"\"Generates a dictionary of SimGlucose environments for 10 patients of each age group.\"\"\"\n",
    "\n",
    "    GLUCOSE_GYMS = dict()\n",
    "    # for patient_age in [\"adolescent\", \"adult\", \"child\"]: # Removing children for now\n",
    "    for patient_age in [\"adolescent\", \"adult\"]:\n",
    "        for patient_number in range(1, 10):\n",
    "            env_name = get_env_name(patient_age, patient_number)\n",
    "            GLUCOSE_GYMS[env_name] = init_glucose_env(env_class, patient_age, patient_number)\n",
    "    return GLUCOSE_GYMS\n",
    "\n",
    "\n",
    "def get_response_from_glucose_gym(action, GYM):\n",
    "    \"\"\"Executes an action in the SimGlucose environment and returns the response.\"\"\"\n",
    "\n",
    "    obs, reward, done, info = GYM.step(float(action))\n",
    "    info[\"CGM\"] = float(obs[0])\n",
    "    \n",
    "    if \"adolescent\" in info[\"patient_name\"]:\n",
    "        info[\"patient_age\"] = \"adolescent\"\n",
    "    elif \"child\" in info[\"patient_name\"]:\n",
    "        info[\"patient_age\"] = \"child\"\n",
    "    else:\n",
    "        info[\"patient_age\"] = \"adult\"\n",
    "    \n",
    "    info[\"time\"] = int(info[\"time\"].strftime(\"%Y%m%d%H%M%S\"))\n",
    "    response = {\n",
    "        \"state\": info,\n",
    "        \"reward\": reward,\n",
    "        \"done\": done,\n",
    "        \"action\": 0,\n",
    "    }\n",
    "    return response\n",
    "\n",
    "\n",
    "class ModernGlucoseEnv(gym.Wrapper):\n",
    "    \"\"\"A wrapper around the SimGlucose environment.\"\"\"\n",
    "\n",
    "    def __init__(self, patient_name):\n",
    "        MAX_BG = 1000\n",
    "        # Create base environment\n",
    "        patient = T1DPatient.withName(patient_name)\n",
    "        sensor = CGMSensor.withName('Dexcom')\n",
    "        pump = InsulinPump.withName('Insulet')\n",
    "        scenario = RandomScenario(start_time=datetime(2025, 1, 1, 8, 0, 0))\n",
    "        env = T1DSimEnv(patient, sensor, pump, scenario)\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=MAX_BG, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=0, high=self.env.pump._params[\"max_basal\"], shape=(1,), dtype=jnp.float32\n",
    "        )\n",
    "        # Create a proper spec\n",
    "        self._spec = EnvSpec(\n",
    "            id=\"glucose\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def spec(self):\n",
    "        return self._spec\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take one action in the environment and return the result.\n",
    "\n",
    "        Though the reward is returned, we will not use it and instead\n",
    "        use the CGM value as our reward.\n",
    "\n",
    "        Returns:\n",
    "            observation: The CGM observation from the environment.\n",
    "            reward: The reward from the environment.\n",
    "            done: Whether the episode is done.\n",
    "            info: Additional information from the environment.\n",
    "        \"\"\"\n",
    "        act = Action(basal=float(action), bolus=0)\n",
    "        state = self.env.step(act)\n",
    "        observation = np.array([state.observation.CGM], dtype=np.float32)\n",
    "        return observation, state.reward, state.done, state.info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        state = self.env.reset()\n",
    "        if \"adolescent\" in state.info[\"patient_name\"]:\n",
    "            patient_age = \"adolescent\"\n",
    "        elif \"child\" in state.info[\"patient_name\"]:\n",
    "            patient_age = \"child\"\n",
    "        else:\n",
    "            # \"adult\" in info[\"patient_name\"]:\n",
    "            patient_age = \"adult\"\n",
    "        # observation = np.array([state.observation.CGM], dtype=np.float32)\n",
    "        observation = {\"CGM\": state.observation.CGM, \"patient_age\": patient_age}\n",
    "        return observation, state.info\n",
    "\n",
    "if \"gyms\" not in globals():\n",
    "    # This line should only be run once per session, even if the cell is run multiple times.\n",
    "    gyms = generate_glucose_gyms(ModernGlucoseEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a couple functions to initialize the factors for the POMDP, and a function that chunks continuous observations into discrete elements.\n",
    "\n",
    "In a future version, Genius will provide tools to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## UTILITY FUNCTIONS ########\n",
    "\n",
    "def create_normalized_random_tensor(shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a tensor of the given shape with random uniform values,\n",
    "    normalized along the first axis to create a probability distribution.\n",
    "\n",
    "    The parameters will be learned continuously by interacting with the environment.\n",
    "    \"\"\"\n",
    "    tensor = np.random.uniform(size=shape)\n",
    "    sum_ax0 = np.sum(tensor, axis=0, keepdims=True)\n",
    "    # Use np.where to handle potential zero sums gracefully\n",
    "    normalized_tensor = np.where(sum_ax0 == 0, 1.0 / shape[0], tensor / sum_ax0)\n",
    "    normalized_tensor /= np.sum(normalized_tensor, axis=0, keepdims=True)\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "def create_normalized_identity_transition_tensor(shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a tensor of the given shape with identity values, normalized\n",
    "    along the first axis to create a probability distribution.\n",
    "    Assumes a 3d tensor.\n",
    "    \"\"\"\n",
    "    tensor = np.eye(shape[0])\n",
    "    L = shape[0]\n",
    "    A = shape[2]\n",
    "    tensor = np.broadcast_to(\n",
    "        np.eye(L)[:, :, np.newaxis],\n",
    "        (L, L, A)\n",
    "    )\n",
    "    return tensor\n",
    "\n",
    "def create_initialized_likelihood_tensor(\n",
    "    cgm_values: list[str],\n",
    "    state1_values: list[str], # e.g., glucose_utilization (Insulin Effect)\n",
    "    state2_values: list[str], # e.g., uncontrolled_glucose_dynamics (Baseline State)\n",
    "    center_mode: float = 0.4, # Target mode for 'ideal' CGM (adjust based on cgm_values bins)\n",
    "    baseline_influence: float = 0.3, # How much state2 shifts the mode (+/-)\n",
    "    insulin_influence: float = -0.5, # How much state1 shifts the mode (should be negative)\n",
    "    default_std: float = 1.5,\n",
    "    default_skew: float = 0.0 # Skew: positive = tail to right, negative = tail to left\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an initialized likelihood tensor P(CGM | State1, State2).\n",
    "\n",
    "    Assumes State1 relates to insulin effect (higher index = more effect -> lower CGM)\n",
    "    and State2 relates to baseline glucose factors (higher index = higher baseline -> higher CGM).\n",
    "\n",
    "    Args:\n",
    "        cgm_values: List of discretized CGM observation values (strings).\n",
    "        state1_values: List of possible values for latent state 1 (strings).\n",
    "        state2_values: List of possible values for latent state 2 (strings).\n",
    "        center_mode: The target scaled_mode for CGM when states are 'neutral'.\n",
    "        baseline_influence: Factor controlling how much state 2 shifts the mode.\n",
    "        insulin_influence: Factor controlling how much state 1 shifts the mode (negative).\n",
    "        default_std: Default standard deviation for the gaussian distributions.\n",
    "        default_skew: Default skewness for the gaussian distributions.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (len(cgm_values), len(state1_values), len(state2_values)).\n",
    "    \"\"\"\n",
    "    n_cgm = len(cgm_values)\n",
    "    n_s1 = len(state1_values)\n",
    "    n_s2 = len(state2_values)\n",
    "\n",
    "    likelihood_tensor = np.zeros((n_cgm, n_s1, n_s2))\n",
    "\n",
    "    # Avoid division by zero if a state has only one value\n",
    "    norm_factor_s1 = n_s1 - 1 if n_s1 > 1 else 1\n",
    "    norm_factor_s2 = n_s2 - 1 if n_s2 > 1 else 1\n",
    "\n",
    "    for s1_idx in range(n_s1):\n",
    "        for s2_idx in range(n_s2):\n",
    "            # Normalize indices to range [0, 1]\n",
    "            # If only one state value exists, norm is 0.5 to represent 'medium'\n",
    "            norm_s1 = s1_idx / norm_factor_s1 if n_s1 > 1 else 0.5\n",
    "            norm_s2 = s2_idx / norm_factor_s2 if n_s2 > 1 else 0.5\n",
    "\n",
    "            # Calculate the target mode for the CGM distribution based on state levels\n",
    "            # State 2 shifts mode around the center: high s2 -> higher mode\n",
    "            base_mode = center_mode + baseline_influence * (norm_s2 * 2 - 1)\n",
    "            # State 1 shifts mode based on its level: high s1 -> lower mode\n",
    "            mode_shift = insulin_influence * norm_s1\n",
    "            \n",
    "            target_mode = base_mode + mode_shift\n",
    "\n",
    "            # Clip mode to avoid extreme values near 0 or 1\n",
    "            target_mode = np.clip(target_mode, 0.05, 0.95)\n",
    "\n",
    "            # Generate the probability distribution P(CGM | s1, s2)\n",
    "            # You could potentially adjust std or skew here based on states too\n",
    "            cgm_dist = dist_gaussian(\n",
    "                N=n_cgm,\n",
    "                scaled_mode=target_mode,\n",
    "                std=default_std,\n",
    "                # Use skew to potentially model faster drops vs rises, etc.\n",
    "                # e.g., left_skew = default_std * (1 - default_skew),\n",
    "                #       right_skew = default_std * (1 + default_skew)\n",
    "            )\n",
    "\n",
    "            likelihood_tensor[:, s1_idx, s2_idx] = cgm_dist\n",
    "\n",
    "    return likelihood_tensor\n",
    "\n",
    "\n",
    "def discretize_observation(obs: dict, observation_elements: dict[str, list]) -> dict:\n",
    "    \"\"\"\n",
    "    Convert continuous observations to discrete elements.\n",
    "\n",
    "    obs: A dictionary of continuous observations.\n",
    "    observation_elements: A dictionary of variables and the bin edges to use for discretization.\n",
    "\n",
    "    Returns: A dictionary of discrete observations.\n",
    "    \"\"\"\n",
    "    for map_key, map_elements in observation_elements.items():\n",
    "        if map_key in obs:\n",
    "            if isinstance(obs[map_key], str):\n",
    "                obs[map_key] = map_elements.index(obs[map_key])\n",
    "            else:\n",
    "                obs[map_key] = np.digitize(obs[map_key], [float(x) for x in map_elements[:-1]])\n",
    "    return obs\n",
    "\n",
    "def dist_gaussian(N, scaled_mode=0.5, left_skew=None, right_skew=None, std=1.0):\n",
    "    \"\"\"\n",
    "    Returns a length-N array shaped like a skewed discrete Gaussian.\n",
    "    scaled_mode: float between 0 and 1 indicating relative position of peak in array (default: 0.5)\n",
    "    left_skew: spread parameter for values less than mode (default: std)\n",
    "    right_skew: spread parameter for values greater than mode (default: std)\n",
    "    std: overall scale factor for the spread (default: 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle edge cases for scaled_mode\n",
    "    scaled_mode = np.clip(scaled_mode, 0.0, 1.0)  # Constrain to valid range\n",
    "\n",
    "    if left_skew is None:\n",
    "        left_skew = 1.0  # will be scaled by std\n",
    "    if right_skew is None:\n",
    "        right_skew = 1.0  # will be scaled by std\n",
    "\n",
    "    # Ensure positive spread parameters\n",
    "    left_skew = abs(left_skew)\n",
    "    right_skew = abs(right_skew)\n",
    "    std = abs(std)\n",
    "\n",
    "    # Scale the skew values by std\n",
    "    left_skew *= std\n",
    "    right_skew *= std\n",
    "\n",
    "    mode = scaled_mode * (N - 1)\n",
    "    i_vals = np.arange(N)\n",
    "\n",
    "    # Use different spreads for left and right sides of the mode\n",
    "    raw = np.zeros(N)\n",
    "    left_mask = i_vals <= mode\n",
    "    right_mask = i_vals > mode\n",
    "\n",
    "    # Calculate exponential terms separately for left and right sides\n",
    "    raw[left_mask] = np.exp(-0.5 * ((i_vals[left_mask] - mode) / left_skew) ** 2)\n",
    "    raw[right_mask] = np.exp(-0.5 * ((i_vals[right_mask] - mode) / right_skew) ** 2)\n",
    "\n",
    "    total = raw.sum()\n",
    "    if total > 0:\n",
    "        return raw / total\n",
    "    else:\n",
    "        return np.ones(N) / N\n",
    "    \n",
    "def create_high_temperature_identity_tensor(shape: tuple, noise_level: float = 0.5, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a tensor of the given shape with identity values, normalized\n",
    "    along the first axis to create a probability distribution.\n",
    "\n",
    "    noise_level: float between 0 and 1, the amount of noise to add to the tensor. Experimental attempt to break symmetry.\n",
    "    \"\"\"\n",
    "    assert shape[0] == shape[1], \"shape[0] must equal shape[1]\"\n",
    "    cpds = []\n",
    "    if \"std\" not in kwargs:\n",
    "        kwargs[\"std\"] = 1.0\n",
    "    for i in range(shape[0]):\n",
    "        cpds.append(dist_gaussian(\n",
    "            N=shape[1],\n",
    "            scaled_mode=i / (shape[0] - 1),\n",
    "            **kwargs,\n",
    "        ))\n",
    "    tensor = np.stack(cpds, axis=0)\n",
    "    tensor /= np.sum(tensor, axis=0, keepdims=True)\n",
    "    \n",
    "    if len(shape) > 2:\n",
    "        # Expand and tile for each dimension after the first two\n",
    "        for i in range(2, len(shape)):\n",
    "            tensor = np.expand_dims(tensor, axis=i)\n",
    "            # Repeat the tensor along axis i to match the desired shape\n",
    "            reps = [1] * len(tensor.shape)\n",
    "            reps[i] = shape[i]\n",
    "            tensor = np.tile(tensor, reps)\n",
    "            \n",
    "            # Add 20% random noise to each tile\n",
    "            noise = np.random.normal(0, noise_level, tensor.shape)\n",
    "            tensor += noise\n",
    "            # Re-normalize after adding noise\n",
    "            tensor = np.maximum(tensor, 0)  # Ensure non-negative\n",
    "            tensor /= np.sum(tensor, axis=0, keepdims=True)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def create_modulated_tensor(shape: tuple, std: float = 1.0, max_shift_effect: float = 0.2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a 3D tensor P(Dim0 | Dim1, Dim2).\n",
    "\n",
    "    The value P(Dim0=i | Dim1=j, Dim2=k) is derived from a Gaussian function\n",
    "    whose mode depends on the target index 'i' and is modulated by the index 'k'.\n",
    "    The final tensor is normalized such that Sum_i T[i, j, k] = 1 for all j, k.\n",
    "\n",
    "    Args:\n",
    "        shape: A tuple (N0, N1, N2).\n",
    "        std: Standard deviation for the Gaussian distributions used internally.\n",
    "        max_shift_effect: Controls how much the index 'k' (Dim2) can shift the mode.\n",
    "                          The shift ranges proportionally from -max_shift_effect (k=0)\n",
    "                          to +max_shift_effect (k=N2-1), being zero at the midpoint.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the specified shape, where sum over axis 0 is 1.0 for each [:, j, k].\n",
    "    \"\"\"\n",
    "    if len(shape) != 3:\n",
    "        raise ValueError(f\"Shape must be a 3-tuple (N0, N1, N2), got {shape}\")\n",
    "\n",
    "    N0, N1, N2 = shape\n",
    "    tensor = np.zeros(shape)\n",
    "\n",
    "    # Avoid division by zero if only one state or action\n",
    "    norm_factor_0 = N0 - 1 if N0 > 1 else 1\n",
    "    norm_factor_2 = N2 - 1 if N2 > 1 else 1\n",
    "\n",
    "    # Generate the tensor slice by slice for the third dimension (k)\n",
    "    for k in range(N2):\n",
    "        # Calculate shift factor for this k: ranges from -1 to 1\n",
    "        mod_factor = ((k / norm_factor_2) * 2 - 1) if N2 > 1 else 0.0\n",
    "        shift = - (mod_factor * max_shift_effect)\n",
    "\n",
    "        # Create the intermediate N0 x N1 matrix for this k\n",
    "        # This matrix holds the unnormalized values derived from Gaussian distributions\n",
    "        intermediate_matrix_k = np.zeros((N0, N1))\n",
    "        for i in range(N0): # Loop over the first dimension index (target)\n",
    "            # Base mode depends on i (normalized index of the first dimension)\n",
    "            base_mode = i / norm_factor_0 if N0 > 1 else 0.5\n",
    "            # Apply the shift based on k\n",
    "            final_mode = np.clip(base_mode + shift, 0.0, 1.0)\n",
    "\n",
    "            # Generate the i-th row: a Gaussian distribution of length N1,\n",
    "            # with the calculated mode.\n",
    "            row_dist = dist_gaussian(\n",
    "                N=N1,\n",
    "                scaled_mode=final_mode,\n",
    "                std=std,\n",
    "            )\n",
    "            intermediate_matrix_k[i, :] = row_dist\n",
    "\n",
    "        # Normalize the columns (axis 0) of the intermediate matrix\n",
    "        # This ensures Sum_i M[i, j] = 1 for each j\n",
    "        matrix_sum_axis0 = np.sum(intermediate_matrix_k, axis=0, keepdims=True)\n",
    "\n",
    "        # Avoid division by zero if a column sums to 0\n",
    "        normalized_matrix_k = np.divide(intermediate_matrix_k, matrix_sum_axis0,\n",
    "                                        out=np.zeros_like(intermediate_matrix_k),\n",
    "                                        where=matrix_sum_axis0 != 0)\n",
    "\n",
    "        # Handle columns that summed to zero (assign uniform probability if N0 > 0)\n",
    "        # This might happen if std is extremely small and mode is outside [0,1] range\n",
    "        # before clipping, though dist_gaussian should handle most cases.\n",
    "        zero_sum_mask = (matrix_sum_axis0 == 0)[0, :] # Get mask for columns j\n",
    "        if N0 > 0 and np.any(zero_sum_mask):\n",
    "            uniform_val = 1.0 / N0\n",
    "            normalized_matrix_k[:, zero_sum_mask] = uniform_val\n",
    "\n",
    "        # Assign the normalized matrix to the k-th slice of the final tensor\n",
    "        tensor[:, :, k] = normalized_matrix_k\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def add_and_normalize_tensors(tensors: list[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds a list of tensors together and normalizes the result.\n",
    "    \"\"\"\n",
    "    # Sum all tensors elementwise\n",
    "    summed_tensor = np.sum(tensors, axis=0)\n",
    "    \n",
    "    # Normalize along axis 0 (first dimension)\n",
    "    # First compute sum along axis 0 while keeping dims for broadcasting\n",
    "    sum_axis0 = np.sum(summed_tensor, axis=0, keepdims=True)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    normalized_tensor = np.divide(summed_tensor, sum_axis0,\n",
    "                                out=np.zeros_like(summed_tensor),\n",
    "                                where=sum_axis0 != 0)\n",
    "    \n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "\n",
    "def create_generalized_modulated_tensor(\n",
    "    shape: Tuple[int, ...],\n",
    "    modulator_dims: List[int] = [2],\n",
    "    max_mode_shift_effects: List[float] = [.2],\n",
    "    base_std: float = 1.0,\n",
    "    std_mod_effects: Optional[List[float]] = None,\n",
    "    noise_level: float = 0.2,\n",
    "    state_dim_idx: int = 0,\n",
    "    prev_state_dim_idx: int = 1\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an N-dimensional tensor P(State | Prev_State, Mod1, Mod2, ..., Other1, Other2, ...).\n",
    "\n",
    "    Dim0 (state_dim_idx) and Dim1 (prev_state_dim_idx) represent the core state transition.\n",
    "    The value P(State=i | Prev_State=j, ...) is derived from a Gaussian function\n",
    "    whose mode (for State=i) is modulated by specified modulator dimensions.\n",
    "    The standard deviation of the Gaussian can also be modulated.\n",
    "    Dimensions not involved in state, prev_state, or modulation are tiled,\n",
    "    with optional noise.\n",
    "    The final tensor is normalized such that Sum_i T[i, ..., j_prev, ...] = 1 for all other indices.\n",
    "\n",
    "    Args:\n",
    "        shape: Tuple of N integers defining the tensor dimensions.\n",
    "        modulator_dims: List of dimension indices that modulate the Gaussian mode/std.\n",
    "        max_mode_shift_effects: List of floats, same length as modulator_dims.\n",
    "                                Controls max mode shift by each modulator.\n",
    "                                Positive value means higher index in modulator -> higher mode.\n",
    "        base_std: Base standard deviation for the Gaussian distributions.\n",
    "        std_mod_effects: Optional list of floats, same length as modulator_dims.\n",
    "                         Controls how much each modulator shifts the std.\n",
    "        noise_level: Percentage of random noise (0 to 1) to apply to non-modulating,\n",
    "                     non-state dimensions.\n",
    "        state_dim_idx: Index of the 'current state' dimension (summed over for normalization).\n",
    "        prev_state_dim_idx: Index of the 'previous state' dimension (forms rows of Gaussian).\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the specified shape.\n",
    "    \"\"\"\n",
    "    # --- Input Validations ---\n",
    "    if not isinstance(shape, tuple) or not all(isinstance(s, int) and s > 0 for s in shape):\n",
    "        raise ValueError(\"Shape must be a tuple of positive integers.\")\n",
    "    if len(shape) < 2:\n",
    "        raise ValueError(\"Shape must have at least 2 dimensions.\")\n",
    "\n",
    "    if not (0 <= state_dim_idx < len(shape) and 0 <= prev_state_dim_idx < len(shape)):\n",
    "        raise ValueError(\"state_dim_idx or prev_state_dim_idx are out of bounds.\")\n",
    "    if state_dim_idx == prev_state_dim_idx:\n",
    "        raise ValueError(\"state_dim_idx and prev_state_dim_idx must be different.\")\n",
    "\n",
    "    if not all(0 <= md < len(shape) for md in modulator_dims):\n",
    "        raise ValueError(\"All modulator_dims must be valid dimension indices.\")\n",
    "    if any(md == state_dim_idx or md == prev_state_dim_idx for md in modulator_dims):\n",
    "        raise ValueError(\"Modulator dimensions cannot be state_dim_idx or prev_state_dim_idx.\")\n",
    "    if len(set(modulator_dims)) != len(modulator_dims):\n",
    "        raise ValueError(\"Modulator dimensions must be unique.\")\n",
    "\n",
    "    if len(max_mode_shift_effects) != len(modulator_dims):\n",
    "        raise ValueError(\"max_mode_shift_effects must have the same length as modulator_dims.\")\n",
    "    if std_mod_effects is not None and len(std_mod_effects) != len(modulator_dims):\n",
    "        raise ValueError(\"std_mod_effects must have the same length as modulator_dims if provided.\")\n",
    "\n",
    "    if base_std <= 0:\n",
    "        raise ValueError(\"base_std must be positive.\")\n",
    "    if not (0 <= noise_level <= 1):\n",
    "        raise ValueError(\"noise_level must be between 0 and 1.\")\n",
    "\n",
    "    # --- Initialization ---\n",
    "    tensor = np.zeros(shape)\n",
    "    N0_val = shape[state_dim_idx]\n",
    "    N1_val = shape[prev_state_dim_idx]\n",
    "\n",
    "    norm_factor_N0 = N0_val - 1 if N0_val > 1 else 1.0 # Avoid division by zero if N0_val is 1\n",
    "\n",
    "    # Precompute modulator normalization factors\n",
    "    modulator_norm_factors = []\n",
    "    modulator_dim_sizes = []\n",
    "    for mod_dim in modulator_dims:\n",
    "        size = shape[mod_dim]\n",
    "        modulator_dim_sizes.append(size)\n",
    "        modulator_norm_factors.append(size - 1 if size > 1 else 1.0) # Avoid div by zero\n",
    "\n",
    "    # Identify axes to iterate over (all dims except state_dim_idx and prev_state_dim_idx)\n",
    "    axes_to_iterate = [d for d in range(len(shape)) if d not in (state_dim_idx, prev_state_dim_idx)]\n",
    "    shape_to_iterate = tuple(shape[d] for d in axes_to_iterate)\n",
    "\n",
    "    # --- Main Loop: Iterate over slices defined by non-state/prev_state dimensions ---\n",
    "    for current_iter_indices_tuple in np.ndindex(shape_to_iterate):\n",
    "        # Map iterated indices back to their original dimension positions\n",
    "        current_full_indices_for_iter_dims = {}\n",
    "        for i, dim_idx_original in enumerate(axes_to_iterate):\n",
    "            current_full_indices_for_iter_dims[dim_idx_original] = current_iter_indices_tuple[i]\n",
    "\n",
    "        # Calculate combined mode shift and effective std from modulators\n",
    "        total_mode_shift = 0.0\n",
    "        current_std_mod_effect_sum = 0.0\n",
    "\n",
    "        for k, mod_dim_original_idx in enumerate(modulator_dims):\n",
    "            mod_val_of_current_dim = current_full_indices_for_iter_dims[mod_dim_original_idx]\n",
    "            dim_size = modulator_dim_sizes[k]\n",
    "            norm_f = modulator_norm_factors[k]\n",
    "            \n",
    "            mod_factor = 0.0\n",
    "            if dim_size > 1:\n",
    "                mod_factor = (mod_val_of_current_dim / norm_f) * 2.0 - 1.0\n",
    "            \n",
    "            total_mode_shift -= (mod_factor * max_mode_shift_effects[k]) # Original logic: shift = -(mod_factor * effect)\n",
    "            if std_mod_effects:\n",
    "                current_std_mod_effect_sum += (mod_factor * std_mod_effects[k])\n",
    "        \n",
    "        effective_std = base_std + current_std_mod_effect_sum\n",
    "        effective_std = max(effective_std, 1e-6) # Ensure std is positive\n",
    "\n",
    "        # Create the intermediate (N0, N1) matrix for this slice\n",
    "        intermediate_matrix_N0_N1 = np.zeros((N0_val, N1_val))\n",
    "        for i_N0 in range(N0_val):  # Loop over the state_dim_idx (rows)\n",
    "            base_mode = (i_N0 / norm_factor_N0) if N0_val > 1 else 0.5\n",
    "            final_mode = np.clip(base_mode + total_mode_shift, 0.0, 1.0)\n",
    "            \n",
    "            row_dist = dist_gaussian(\n",
    "                N=N1_val,\n",
    "                scaled_mode=final_mode,\n",
    "                std=effective_std,\n",
    "            )\n",
    "            intermediate_matrix_N0_N1[i_N0, :] = row_dist\n",
    "\n",
    "        # Apply noise if specified\n",
    "        if noise_level > 0.0:\n",
    "            # Noise is applied to the (N0, N1) matrix before normalization\n",
    "            noise_matrix = noise_level * np.random.uniform(-1, 1, size=(N0_val, N1_val))\n",
    "            intermediate_matrix_N0_N1 *= (1 + noise_matrix)\n",
    "            intermediate_matrix_N0_N1 = np.maximum(intermediate_matrix_N0_N1, 0) # Ensure non-negativity\n",
    "\n",
    "        # Normalize columns (axis 0 of intermediate_matrix, which corresponds to state_dim_idx)\n",
    "        matrix_sum_axis0 = np.sum(intermediate_matrix_N0_N1, axis=0, keepdims=True)\n",
    "        \n",
    "        normalized_matrix_N0_N1 = np.divide(\n",
    "            intermediate_matrix_N0_N1, matrix_sum_axis0,\n",
    "            out=np.zeros_like(intermediate_matrix_N0_N1),\n",
    "            where=matrix_sum_axis0 != 0\n",
    "        )\n",
    "\n",
    "        # Handle columns that summed to zero\n",
    "        if N0_val > 0:\n",
    "            zero_sum_cols_mask = (matrix_sum_axis0 == 0)[0, :] # Get mask for columns j\n",
    "            if np.any(zero_sum_cols_mask):\n",
    "                uniform_val = 1.0 / N0_val\n",
    "                normalized_matrix_N0_N1[:, zero_sum_cols_mask] = uniform_val\n",
    "        \n",
    "        # --- Assign the normalized (N0, N1) matrix to the correct slice of the N-D tensor ---\n",
    "        # Construct the slice for the output tensor\n",
    "        output_slice_constructor = [slice(None)] * len(shape)\n",
    "        iter_coord_idx = 0\n",
    "        for d_original in range(len(shape)):\n",
    "            if d_original == state_dim_idx or d_original == prev_state_dim_idx:\n",
    "                continue # These are the slice(None) parts handled by the matrix itself\n",
    "            else: # This dimension was part of axes_to_iterate\n",
    "                output_slice_constructor[d_original] = current_iter_indices_tuple[iter_coord_idx]\n",
    "                iter_coord_idx += 1\n",
    "        \n",
    "        # The normalized_matrix_N0_N1 has its 0-axis as state_dim and 1-axis as prev_state_dim\n",
    "        # We need to place it into tensor[output_slice_constructor]\n",
    "        # If state_dim_idx appears before prev_state_dim_idx in the overall tensor shape,\n",
    "        # the assignment is direct. Otherwise, the 2D slice needs to be transposed.\n",
    "        \n",
    "        target_slice_in_tensor = tensor[tuple(output_slice_constructor)]\n",
    "\n",
    "        if state_dim_idx < prev_state_dim_idx:\n",
    "            # The slice target_slice_in_tensor has shape (N0_val, N1_val)\n",
    "            # if state_dim_idx is the first of the two 'active' dimensions.\n",
    "            target_slice_in_tensor[:,:] = normalized_matrix_N0_N1\n",
    "        else: # prev_state_dim_idx < state_dim_idx\n",
    "            # The slice target_slice_in_tensor has shape (N1_val, N0_val)\n",
    "            target_slice_in_tensor[:,:] = normalized_matrix_N0_N1.T\n",
    "            \n",
    "    return tensor\n",
    "\n",
    "def plot_tensor_slice(tensor, N, dim=2):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot a heatmap of a 2D slice from a tensor at index [:,:,N] along the specified dimension,\n",
    "    summing out any additional dimensions\n",
    "    \n",
    "    Args:\n",
    "        tensor: Tensor/array with at least 2 dimensions\n",
    "        N: Integer index for the slice dimension\n",
    "        dim: Dimension along which to take the slice (default=2)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    \n",
    "    # Move slice dimension to position 2\n",
    "    tensor = np.moveaxis(tensor, dim, 2)\n",
    "    \n",
    "    # Extract the slice and sum out additional dimensions if they exist\n",
    "    slice_2d = tensor[:,:,N]\n",
    "    if slice_2d.ndim > 2:\n",
    "        slice_2d = np.sum(slice_2d, axis=tuple(range(2, slice_2d.ndim)))\n",
    "    # normalize the slice\n",
    "    slice_2d = np.divide(slice_2d, np.sum(slice_2d))\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(slice_2d, cmap='viridis', annot=False)\n",
    "    plt.title(f'2D Slice at index [:,:,{N}] along dimension {dim}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_reward_function(value_reward_tuples):\n",
    "    \"\"\"\n",
    "    Plot a reward function from tuples of (value, reward) pairs.\n",
    "    \n",
    "    Args:\n",
    "        value_reward_tuples: List of tuples, each containing (value, reward)\n",
    "                           where value can be string or numeric\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert values to float and separate into x,y lists\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    for val, reward in value_reward_tuples:\n",
    "        try:\n",
    "            x_values.append(float(val))\n",
    "            y_values.append(float(reward))\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "            \n",
    "    # Create the line plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_values, y_values, '-o')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward Function')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_n_back(history, n, quantization, min_max_values=(-5.0, 5.0), calculate_acceleration=False):\n",
    "    \"\"\"\n",
    "    Calculate a quantized slope or acceleration from the n most recent values in history.\n",
    "    \n",
    "    Args:\n",
    "        history: List of values (possibly strings) with most recent at the end.\n",
    "        n: Number of recent points to use for slope/acceleration calculation.\n",
    "        quantization: Number of bins to quantize the slope/acceleration into.\n",
    "        min_max_values: tuple[float, float], optional\n",
    "                        Defines the (min_slope, max_slope) or (min_accel, max_accel) for quantization.\n",
    "                        Values outside this range will be clipped.\n",
    "                        Defaults to (-5.0, 5.0).\n",
    "        calculate_acceleration: bool, optional\n",
    "                        If True, calculates acceleration (rate of change of slope)\n",
    "                        If False, calculates slope (default)\n",
    "    \n",
    "    Returns:\n",
    "        Integer from 0 to quantization-1 representing the quantized slope or acceleration.\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be a positive integer.\")\n",
    "    if quantization <= 0:\n",
    "        raise ValueError(\"quantization must be a positive integer.\")\n",
    "\n",
    "    recent_history = history[-n:]\n",
    "    \n",
    "    # Convert to float, handling potential errors if conversion is not possible\n",
    "    numeric_history = []\n",
    "    for val in recent_history:\n",
    "        try:\n",
    "            numeric_history.append(float(val))\n",
    "        except (ValueError, TypeError):\n",
    "            # Skip non-numeric values or handle as per specific requirements\n",
    "            # For this implementation, we'll skip them.\n",
    "            # If all values are non-numeric or too few points, slope will be 0.\n",
    "            pass\n",
    "\n",
    "    if len(numeric_history) < 2:\n",
    "        # Not enough data points to calculate a slope, return a neutral value\n",
    "        return quantization // 2 if quantization > 1 else 0\n",
    "    \n",
    "    if calculate_acceleration:\n",
    "        if len(numeric_history) < 3:\n",
    "            # Need at least 3 points for acceleration\n",
    "            return quantization // 2 if quantization > 1 else 0\n",
    "            \n",
    "        # Calculate two slopes and find their difference\n",
    "        time_intervals = len(numeric_history) - 1\n",
    "        mid_point = len(numeric_history) // 2\n",
    "        \n",
    "        # First half slope\n",
    "        delta_glucose1 = numeric_history[mid_point] - numeric_history[0]\n",
    "        slope1 = delta_glucose1 / mid_point if mid_point > 0 else 0.0\n",
    "        \n",
    "        # Second half slope\n",
    "        delta_glucose2 = numeric_history[-1] - numeric_history[mid_point]\n",
    "        remaining_intervals = time_intervals - mid_point\n",
    "        slope2 = delta_glucose2 / remaining_intervals if remaining_intervals > 0 else 0.0\n",
    "        \n",
    "        # Acceleration is change in slope\n",
    "        value = slope2 - slope1\n",
    "    else:\n",
    "        # Calculate regular slope\n",
    "        delta_glucose = numeric_history[-1] - numeric_history[0]\n",
    "        time_intervals = len(numeric_history) - 1\n",
    "        value = delta_glucose / time_intervals if time_intervals > 0 else 0.0\n",
    "\n",
    "    min_val, max_val = min_max_values\n",
    "\n",
    "    # Clip the value\n",
    "    value = max(min_val, min(value, max_val))\n",
    "\n",
    "    # Quantize the value\n",
    "    if max_val == min_val: # Avoid division by zero if range is a single point\n",
    "        return 0 if quantization == 1 else quantization // 2 # or some default\n",
    "\n",
    "    # Normalize value to [0, 1] within the defined range\n",
    "    normalized_value = (value - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Scale to [0, quantization] and find the bin\n",
    "    quantized_value = int(normalized_value * quantization)\n",
    "    \n",
    "    # Ensure the quantized value is within [0, quantization-1]\n",
    "    quantized_value = max(0, min(quantized_value, quantization - 1))\n",
    "    \n",
    "    return quantized_value\n",
    "\n",
    "\n",
    "\n",
    "def calculate_and_bin_iob(past_dosages_per_minute,\n",
    "                          num_bins,\n",
    "                          max_expected_iob=None,\n",
    "                          max_possible_dose=None,\n",
    "                          aggregation_window_minutes=240,\n",
    "                          effect_half_life_minutes=75):\n",
    "    \"\"\"\n",
    "    Calculates Insulin on Board (IOB) from a list of past insulin dosages\n",
    "    and bins the result into a specified number of discrete bins.\n",
    "\n",
    "    Args:\n",
    "        past_dosages_per_minute (list of float): A list where each element\n",
    "            is the insulin dosage delivered in a minute. The list should be\n",
    "            ordered from oldest (index 0) to most recent (index -1).\n",
    "            It's assumed the length of this list can be up to\n",
    "            aggregation_window_minutes. If shorter, it's assumed no insulin\n",
    "            was delivered in the unlisted preceding minutes within the window.\n",
    "        num_bins (int): The number of discrete bins to categorize IOB into.\n",
    "        max_expected_iob (float): The maximum IOB value expected. This defines\n",
    "            the upper limit for binning. Values above this will fall into the last bin.\n",
    "        aggregation_window_minutes (int): How far back in minutes to consider\n",
    "            insulin dosages.\n",
    "        effect_half_life_minutes (float): The half-life of insulin's effect in minutes.\n",
    "\n",
    "    Returns:\n",
    "        int: The bin number (0 to num_bins-1) that the current IOB falls into.\n",
    "             Returns -1 if inputs are invalid.\n",
    "    \"\"\"\n",
    "    if not isinstance(past_dosages_per_minute, list) or \\\n",
    "       not all(isinstance(d, (int, float, str)) for d in past_dosages_per_minute) or \\\n",
    "       num_bins <= 0 or \\\n",
    "       aggregation_window_minutes <= 0 or effect_half_life_minutes <= 0:\n",
    "        print(\"Error: Invalid input parameters.\")\n",
    "        return -1 # Or raise an error\n",
    "    \n",
    "    if len(past_dosages_per_minute) == 0:\n",
    "        return 0\n",
    "    \n",
    "    past_dosages_per_minute = [float(x) for x in past_dosages_per_minute]\n",
    "    \n",
    "    \n",
    "    def calculate_iob(dosages, lambda_decay, window_minutes):\n",
    "        \"\"\"\n",
    "        Calculate total insulin on board from a list of dosages.\n",
    "        \n",
    "        Args:\n",
    "            dosages (list): List of insulin doses from oldest to most recent\n",
    "            lambda_decay (float): Decay rate calculated from half-life\n",
    "            window_minutes (int): How far back to consider doses\n",
    "            \n",
    "        Returns:\n",
    "            float: Total insulin on board\n",
    "        \"\"\"\n",
    "        iob = 0.0\n",
    "        num_dosages = min(len(dosages), window_minutes)\n",
    "        dosages = list(reversed(dosages))\n",
    "        \n",
    "        for i in range(num_dosages):\n",
    "            dose = dosages[i]\n",
    "            time_elapsed_minutes = i\n",
    "            decayed_dose = dose * math.exp(-lambda_decay * time_elapsed_minutes)\n",
    "            iob += decayed_dose\n",
    "            \n",
    "        return iob\n",
    "\n",
    "    # calculate max expected iob if not provided\n",
    "    if not max_expected_iob:\n",
    "        if not max_possible_dose:\n",
    "            print(\"Error: max_expected_iob or max_possible_dose must be provided.\")\n",
    "            return -1\n",
    "        else:\n",
    "            # Calculate theoretical max IOB if max dose was given every minute\n",
    "            lambda_decay = math.log(2) / effect_half_life_minutes\n",
    "            theoretical_max_doses = [float(max_possible_dose)] * aggregation_window_minutes\n",
    "            max_expected_iob = calculate_iob(theoretical_max_doses, lambda_decay, aggregation_window_minutes)\n",
    "    else:\n",
    "        if max_expected_iob <= 0:\n",
    "            print(\"Error: max_expected_iob must be greater than 0.\")\n",
    "            return -1\n",
    "    lambda_decay = math.log(2) / effect_half_life_minutes\n",
    "    current_iob = calculate_iob(past_dosages_per_minute, lambda_decay, aggregation_window_minutes)\n",
    "\n",
    "    if current_iob < 0: # Should not happen with positive dosages\n",
    "        current_iob = 0\n",
    "\n",
    "    # Binning the IOB using log transform\n",
    "    if current_iob >= max_expected_iob:\n",
    "        return num_bins - 1 # Last bin for values at or above max_expected_iob\n",
    "\n",
    "    if max_expected_iob == 0: # Avoid division by zero if max_iob is 0\n",
    "        return 0 if current_iob == 0 else num_bins - 1\n",
    "        \n",
    "    # # Apply log transform to compress higher values\n",
    "    # if current_iob == 0:\n",
    "    #     log_iob = 0\n",
    "    # else:\n",
    "    #     log_iob = math.log(current_iob + 1) # Add 1 to handle values between 0 and 1\n",
    "        \n",
    "\n",
    "    # skip log transform\n",
    "    log_iob = current_iob\n",
    "\n",
    "    # log_max_iob = math.log(max_expected_iob + 1)\n",
    "    log_max_iob = max_expected_iob\n",
    "    bin_width = log_max_iob / num_bins\n",
    "    \n",
    "    if bin_width == 0: # Avoid division by zero\n",
    "        return 0 if current_iob == 0 else num_bins - 1\n",
    "\n",
    "    binned_value = math.floor(log_iob / bin_width)\n",
    "\n",
    "    # Ensure it doesn't exceed num_bins-1 due to floating point issues\n",
    "    binned_value = min(binned_value, num_bins - 1)\n",
    "\n",
    "    return int(binned_value)\n",
    "\n",
    "# examples = []\n",
    "# for i in range(len(cgm_history)):\n",
    "\n",
    "#     slope = get_slope_n_back(cgm_history[:i], 20, len(OBSERVATION_VALUES_CGM_T_MINUS_1), (-5, 5), calculate_acceleration=True)\n",
    "#     examples.append(slope)\n",
    "\n",
    "\n",
    "# examples = []\n",
    "# actions_1 = [0.08,0.08,0.08,0.02,0.02,0.02,0.01,0.01,0.01,0.08,0.08,0.08,0.08,0.08,0.08,0.0,0.0,0.0,0.08,0.08,0.08,0.01,0.01,0.01,0.12,0.12,0.12,0.08,0.08,0.08,0.01,0.01,0.01,0.02,0.02,0.02,0.01,0.01,0.01,0.08,0.08,0.08,0.0,0.0,0.0,0.04,0.04,0.04,0.02,0.02,0.02,0.02,0.02,0.02,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.12,0.12,0.12,0.08,0.08,0.08,0.12,0.12,0.12,0.08,0.08,0.08,0.12,0.12,0.12,0.04,0.04,0.04,0.12,0.12,0.12,0.02,0.02,0.02,0.04,0.04,0.04,0.08,0.08,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.02,0.02,0.08,0.08,0.08,0.08,0.08,0.08,0.01,0.01,0.01,0.08,0.08,0.08,0.0,0.0,0.0,0.01,0.01,0.01,0.0,0.0,0.0,0.08,0.08,0.08,0.0,0.0,0.0,0.12,0.12,0.12,0.02,0.02,0.02,0.08,0.08,0.08,0.04,0.04,0.04,0.08,0.08,0.08,0.02,0.02,0.02,0.04,0.04,0.04,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.08,0.08,0.08,0.04,0.04,0.04,0.0,0.0,0.0,0.12,0.12,0.12,0.02,0.02,0.02,0.08,0.08,0.08,0.12,0.12,0.12,0.01,0.01,0.01,0.0,0.0,0.0,0.01,0.01,0.01,0.01,0.01,0.01,0.08,0.08,0.08,0.04,0.04,0.04,0.02,0.02,0.02,0.0,0.0,0.0,0.02,0.02,0.02,0.0,0.0,0.0,0.01,0.01,0.01,0.01,0.01,0.01,0.0,0.0,0.0,0.12,0.12,0.12,0.0,0.0,0.0,0.08,0.08,0.08,0.08,0.08,0.08,0.12,0.12,0.12,0.04,0.04,0.04,0.04,0.04,0.04,0.12,0.12,0.12,0.12,0.12,0.12,0.01,0.01,0.01,0.04,0.04,0.04,0.01,0.01,0.01,0.0,0.0,0.0,0.08,0.08,0.08,0.01,0.01,0.01,0.01,0.01,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.08,0.08,0.0,0.0,0.0,0.01,0.01,0.01,0.08,0.08,0.08,0.02,0.02,0.02,0.04,0.04,0.04,0.0,0.0,0.0,0.02,0.02,0.02,0.04,0.04,0.04,0.04,0.04,0.04,0.08,0.08,0.08,0.01,0.01,0.01,0.12,0.12,0.12,0.04,0.04,0.04,0.02,0.02,0.02,0.02,0.02,0.02,0.08,0.08,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.12,0.12,0.04,0.04,0.04,0.0,0.0,0.0,0.01,0.01,0.01,0.02,0.02,0.02,0.08,0.08,0.08,0.08,0.08,0.08,0.02,0.02,0.02,0.0,0.0,0.0,0.04,0.04,0.04,0.0,0.0,0.0,0.08,0.08,0.08,0.0,0.0,0.0,0.01,0.01,0.01,0.08,0.08,0.08,0.08,0.08,0.08,0.01,0.01,0.01,0.12,0.12,0.12,0.08,0.08,0.08,0.12,0.12,0.12,0.01,0.01,0.01,0.02,0.02,0.02,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.0,0.0,0.0,0.08,0.08,0.08,0.04,0.04,0.04,0.02,0.02,0.02,0.08,0.08,0.08,0.02,0.02,0.02,0.08,0.08,0.08,0.08,0.08,0.08,0.0,0.0,0.0,0.04,0.04,0.04,0.08,0.08,0.08,0.02,0.02,0.02,0.08,0.08,0.08,0.0,0.0,0.0,0.12,0.12,0.12,0.01,0.01,0.01,0.04,0.04,0.04,0.02,0.02,0.02,0.08,0.08,0.08,0.01,0.01,0.01,0.08,0.08,0.08,0.04,0.04,0.04,0.12,0.12,0.12,0.01,0.01,0.01,0.02,0.02,0.02,0.04,0.04,0.04,0.04,0.04,0.04,0.08,0.08,0.08,0.08,0.08,0.08,0.0,0.0,0.0,0.01,0.01,0.01,0.02,0.02]\n",
    "# print(actions_1)\n",
    "# # dosages = [RAW_ACTION_VALUES_BASAL_INSULIN[x] for x in actions_1]\n",
    "# # print(dosages)\n",
    "# for i in range(len(actions_1)):\n",
    "#     iob = calculate_and_bin_iob(\n",
    "#         past_dosages_per_minute=dosages[:i],\n",
    "#         num_bins=50,\n",
    "#         max_expected_iob=5.5,\n",
    "#         # max_possible_dose=RAW_ACTION_VALUES_BASAL_INSULIN[-3],\n",
    "#         aggregation_window_minutes=240,\n",
    "#         effect_half_life_minutes=75\n",
    "#     )\n",
    "#     examples.append(iob)\n",
    "# print(max(actions_1))\n",
    "# print(examples)\n",
    "# print(max(examples))\n",
    "# print(min(examples))\n",
    "# print(len(set(examples)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_observation(obs: dict, modalities_lengths: list[int]) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts an observation dictionary to a list of one-hot encoded numpy arrays.\n",
    "\n",
    "    Args:\n",
    "        obs (dict): A dictionary of observations where values are indices. The order of keys\n",
    "                    is assumed to match the order of `modalities_lengths`.\n",
    "        modalities_lengths (list of int): A list of the lengths for each one-hot vector,\n",
    "                                          in the same order as the observation dictionary.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: A list of one-hot encoded numpy arrays.\n",
    "    \"\"\"\n",
    "    one_hot_obs = []\n",
    "    # In Python 3.7+ dicts preserve insertion order.\n",
    "    obs_values = list(obs.values()) \n",
    "    for i, modality_length in enumerate(modalities_lengths):\n",
    "        obs_index = int(obs_values[i])\n",
    "        one_hot_vector = np.zeros(modality_length)\n",
    "        one_hot_vector[obs_index] = 1.0\n",
    "        one_hot_obs.append(one_hot_vector)\n",
    "    return one_hot_obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part: designing the POMDP.\n",
    "\n",
    "## POMDP Overview\n",
    "POMDP stands for Partially Observable Markov Decision Process. This is a type of bayesian model that can take action to reduce its uncertainty. It has three types of variables:\n",
    "1. Action variables: The actions the agent can take.\n",
    "2. Observation variables: The data the agent can observe. Also called \"observation modalities\".\n",
    "3. State variables: The unobserved variables that determine the behavior of the environment. Also called \"latent\" or \"hidden\" variables.\n",
    "\n",
    "It also has factors of various types, which define the dynamics of the model with respect to each variable.\n",
    "1. **Transition factors** define how the hidden state variables change over time, given some other variable's input.\n",
    "2. **Likelihood factors** define how the observation variables are generated from the hidden state variables. When the generative model is inverted, the model uses likelihoods to ask \"given I observe this, what is the most likely set of hidden states?\"\n",
    "3. **Preference factors** define the agent's reward for observing certain states.\n",
    "4. **Prior factors** define the agent's prior belief about the state variables.\n",
    "\n",
    "The POMDP uses observations to infer what the state variables are most likely to be. It then uses that inference to choose the action that minimizes Free Energy. Minimizing Free Energy means the agent tries to maximize its long-term reward by learning how the variables interact. Sometimes that means taking exploratory action, and sometimes it means exploiting its knowledge to directly maximize its goal.\n",
    "\n",
    "The POMDP's generative model can be thought of as a DAG (directed acyclic graph) where each node is a variable and each edge is a conditional probability distribution. The DAG has action nodes that causally affect the state variables, and state variables that causally affect the observation variables. When the model observes observations, it inverts the generative model to reason backwards to the most likely hidden states.\n",
    "\n",
    "The latest version of Genius is capable of learning the parameters of the hidden states through continual learning. This is enabled in the SDK by setting the `learn_transitions` parameter in `GeniusAgent.act()`.\n",
    "\n",
    "\n",
    "## Model Design\n",
    "\n",
    "Our POMDP will have only one action variable: the basal insulin dose. Below, we've chosen a small range of values that are common in insulin pumps. For observations, our insulin pump receives a CGM reading and a patient age at every 1-minute timestep. Of course, the patient's age is constant throughout one episode.\n",
    "\n",
    "Variables are connected by factors, which in Genius are tensors representing conditional probability distributions. Generally we can initialize these as random, and Genius will learn the parameters from data. If we are domain experts, we could also initialize them to have more meaningful values.\n",
    "\n",
    "The POMDP will have a preference that the CGM stays within the ideal range of 70-180 mg/dL. The preference is expressed by a reward for states within that range, and a penalty for states outside that range.\n",
    "\n",
    "\n",
    "## Model Structure\n",
    "\n",
    "Here is a diagram of the model structure:\n",
    "\n",
    "<img src=\"./pomdp-structure.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "In a future version of Genius, we'll be able to user higher-dimensional transition factors, allowing us to represent more complex dynamics in the hidden state variables. We'll also be able to use sparse likelihood factors, which more faithfully represent the causal relationships between states and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## HYPERPARAMETERS AFFECTING STRUCTURE ########\n",
    "SOJOURN_TIME = 1\n",
    "\n",
    "######## ACTION VARIABLES ########\n",
    "# Define the variables in the model, with a string for each value the variable can take.\n",
    "# We use strings because Genius requires the values of variables to be strings.\n",
    "\n",
    "# The agent's action is to administer a basal insulin dose.\n",
    "# A normal basal insulin dose is 0.008 - 0.033 units/min.\n",
    "BASAL_INSULIN_VALUES = [0, .008, .016, .032, .06, .1, .12]\n",
    "BASAL_INSULIN_VALUES_SUSTAINED = []\n",
    "# if SOJOURN_TIME > 1:\n",
    "#     for i in range(SOJOURN_TIME)[2:]:\n",
    "#         BASAL_INSULIN_VALUES_SUSTAINED += [x * i for x in BASAL_INSULIN_VALUES]\n",
    "\n",
    "BASAL_INSULIN_ARPEGGIOS_3 = [\n",
    "    # [0, 0, 0],\n",
    "    # [0, .01, .02],\n",
    "    # [.02, .04, .02],\n",
    "    # [.02, .04, .08],\n",
    "    # [.04, .04, .08],\n",
    "    # [.08, .08, .08],\n",
    "    # [.12, .12, .08],\n",
    "    [0, 0, 0, 0, 0 ,0],\n",
    "    [.01, .01, .01, .01, .01, .01],\n",
    "    [.02, .02, .02, .02, .02, .02],\n",
    "    [.04, .04, .04, .04, .04, .04],\n",
    "    [.08, .08, .08, .04, .04, .04],\n",
    "    [.12, .12, .12, .12, .12, .08],\n",
    "]\n",
    "if SOJOURN_TIME > 1:\n",
    "    BASAL_INSULIN_VALUES = BASAL_INSULIN_ARPEGGIOS_3\n",
    "    RAW_ACTION_VALUES_BASAL_INSULIN = [0, .01, .02, .04, .08, .12]\n",
    "    ACTION_VALUES_BASAL_INSULIN = [str(x) for x in BASAL_INSULIN_ARPEGGIOS_3]\n",
    "else:\n",
    "    ACTION_VALUES_BASAL_INSULIN = [str(x) for x in BASAL_INSULIN_VALUES]\n",
    "    RAW_ACTION_VALUES_BASAL_INSULIN = ACTION_VALUES_BASAL_INSULIN\n",
    "\n",
    "ACTION_VALUES_CONSTANT = [\"constant\"]\n",
    "\n",
    "######## OBSERVATION VARIABLES ########\n",
    "\n",
    "# The agent observes the patient's CGM reading and age group.\n",
    "# ideal range is 70-180 mg/dL, but it can go as high as 600 mg/dL.\n",
    "# Here we set the bins to be more granular for hypoglycemia so that the model is\n",
    "# more sensitive to small changes in CGM there.\n",
    "OBSERVATION_VALUES_CGM_LEVELS = 35\n",
    "HYPOGLYCEMIC_VALUES_CGM = [\"50\", \"55\", \"60\", \"65\", \"69\"]\n",
    "GOOD_VALUES_CGM = [str(round(x)) for x in jnp.linspace(80, 180, 10)]\n",
    "START_LEN_CGM = len(HYPOGLYCEMIC_VALUES_CGM) + len(GOOD_VALUES_CGM)\n",
    "HYPERGLYCEMIC_VALUES_CGM = [str(round(x)) for x in jnp.linspace(int(GOOD_VALUES_CGM[-1]) + 20, 600, OBSERVATION_VALUES_CGM_LEVELS-START_LEN_CGM)]\n",
    "OBSERVATION_VALUES_CGM = HYPOGLYCEMIC_VALUES_CGM + GOOD_VALUES_CGM + HYPERGLYCEMIC_VALUES_CGM\n",
    "\n",
    "print(\"Observation values:\")\n",
    "print(OBSERVATION_VALUES_CGM)\n",
    "\n",
    "# The patient's age group is a categorical variable with 3 levels.\n",
    "OBSERVATION_VALUES_PATIENT_AGE = [\"adolescent\", \"adult\"]\n",
    "\n",
    "OBSERVATION_VALUES_CGM_T_MINUS_1 = [str(x) for x in range(5)]\n",
    "\n",
    "OBSERVATION_VALUES_CGM_T_MINUS_2 = [str(x) for x in range(len(OBSERVATION_VALUES_CGM_T_MINUS_1))]\n",
    "\n",
    "OBSERVATION_VALUES_IOB_LEVELS = 16\n",
    "IOB_AGGREGATION_WINDOW_MINUTES = 180\n",
    "OBSERVATION_VALUES_IOB = [str(x) for x in range(OBSERVATION_VALUES_IOB_LEVELS)]\n",
    "\n",
    "# hour of day\n",
    "# assuming the episode starts at 8am and ends at 4pm\n",
    "OBSERVATION_VALUES_HOUR = [str(int(x)) for x in jnp.linspace(60, 60*8, 8)]\n",
    "\n",
    "# OBSERVATION_VALUES_BASAL_INSULIN_T_MINUS_1 = [str(x) for x in RAW_ACTION_VALUES_BASAL_INSULIN]\n",
    "# pump.add_observation_variable(name=\"basal_insulin_T-1\", values=OBSERVATION_VALUES_BASAL_INSULIN_T_MINUS_1)\n",
    "\n",
    "\n",
    "OBSERVATION_VALUES_TOP_POLICY_COUNT_RECENT_STEPS_LEVELS = 5\n",
    "OBSERVATION_VALUES_TOP_POLICY_COUNT_RECENT_STEPS = [str(x) for x in range(OBSERVATION_VALUES_TOP_POLICY_COUNT_RECENT_STEPS_LEVELS)]\n",
    "\n",
    "# OBSERVATION_VALUES_STEPS_SINCE_PENULTIMATE_POLICY_LEVELS = 4\n",
    "# OBSERVATION_VALUES_STEPS_SINCE_PENULTIMATE_POLICY = [str(x) for x in range(OBSERVATION_VALUES_STEPS_SINCE_PENULTIMATE_POLICY_LEVELS)]\n",
    "# pump.add_observation_variable(name=\"steps_since_penultimate_policy\", values=OBSERVATION_VALUES_STEPS_SINCE_PENULTIMATE_POLICY)\n",
    "\n",
    "\n",
    "\n",
    "######## LATENT STATE VARIABLES ########\n",
    "\n",
    "# # A state for the patient's glucose utilization.\n",
    "# STATE_VALUES_GLUCOSE_UTILIZATION = [str(x) for x in range(5)]\n",
    "# pump.add_state_variable(name=\"glucose_utilization\", values=STATE_VALUES_GLUCOSE_UTILIZATION)\n",
    "\n",
    "# A state for the patient's glucose dynamics.\n",
    "STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS = [str(x) for x in OBSERVATION_VALUES_CGM]\n",
    "\n",
    "# A state for the patient's age (doesn't change in an episode)\n",
    "# Even though we observe this directly, we still add a latent state for it,\n",
    "# because that state has an effect on the glucose dynamics and in turn the CGM.\n",
    "STATE_VALUES_PATIENT_AGE = [\"adolescent\", \"adult\"]\n",
    "\n",
    "# state for the time since the last meal\n",
    "STATE_VALUES_MEAL_LEVELS = 2\n",
    "STATE_VALUES_MEAL = [str(x) for x in range(STATE_VALUES_MEAL_LEVELS)]\n",
    "\n",
    "\n",
    "STATE_VALUES_IOB = [str(x) for x in range(OBSERVATION_VALUES_IOB_LEVELS)]\n",
    "\n",
    "STATE_VALUES_MOMENTUM_LEVELS = 5\n",
    "STATE_VALUES_MOMENTUM = [str(x) for x in range(STATE_VALUES_MOMENTUM_LEVELS)]\n",
    "\n",
    "\n",
    "\n",
    "model_description = {\n",
    "    \"observations\": {\n",
    "        \"CGM\": {\n",
    "            \"elements\": OBSERVATION_VALUES_CGM, \n",
    "            \"depends_on\": [\"uncontrolled_glucose_dynamics\"]\n",
    "        },\n",
    "        \"patient_age\": {\n",
    "            \"elements\": OBSERVATION_VALUES_PATIENT_AGE, \n",
    "            \"depends_on\": [\"latent_patient_age\"]\n",
    "        },\n",
    "        \"CGM_T-1\": {\n",
    "            \"elements\": OBSERVATION_VALUES_CGM_T_MINUS_1, \n",
    "            \"depends_on\": [\"meal\"]\n",
    "        },\n",
    "        \"CGM_T-2\": {\n",
    "            \"elements\": OBSERVATION_VALUES_CGM_T_MINUS_2, \n",
    "            \"depends_on\": [\"meal\"]\n",
    "        },\n",
    "        \"iob\": {\n",
    "            \"elements\": OBSERVATION_VALUES_IOB, \n",
    "            \"depends_on\": [\"latent_iob\"]\n",
    "        },\n",
    "    },\n",
    "    \"controls\": {\n",
    "        \"basal_insulin\": {\"elements\": BASAL_INSULIN_VALUES},\n",
    "        \"constant\": {\"elements\": [\"constant\"]}\n",
    "    },\n",
    "    \"states\": {\n",
    "        \"uncontrolled_glucose_dynamics\": {\n",
    "            \"elements\": STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS, \n",
    "            \"depends_on\": [\"uncontrolled_glucose_dynamics\", \"latent_patient_age\", \"momentum\"],\n",
    "            \"controlled_by\": [\"constant\"]\n",
    "        },\n",
    "        \"latent_patient_age\": {\n",
    "            \"elements\": STATE_VALUES_PATIENT_AGE, \n",
    "            \"depends_on\": [\"latent_patient_age\"], \n",
    "            \"controlled_by\": [\"constant\"]  \n",
    "        },\n",
    "        \"meal\": {\n",
    "            \"elements\": STATE_VALUES_MEAL, \n",
    "            \"depends_on\": [\"meal\"], \n",
    "            \"controlled_by\": [\"constant\"]  \n",
    "        },\n",
    "        \"latent_iob\": {\n",
    "            \"elements\": STATE_VALUES_IOB, \n",
    "            \"depends_on\": [\"latent_iob\"], \n",
    "            \"controlled_by\": [\"basal_insulin\"]  \n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"elements\": STATE_VALUES_MOMENTUM, \n",
    "            \"depends_on\": [\"momentum\", \"meal\", \"latent_iob\"], \n",
    "            \"controlled_by\": [\"basal_insulin\"]  \n",
    "        },\n",
    "    },\n",
    "}\n",
    "pump = compile_model(model_description)\n",
    "print(f\"num actions: {len(ACTION_VALUES_BASAL_INSULIN)}\")\n",
    "print(ACTION_VALUES_BASAL_INSULIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "######## TRANSITION FACTORS ########\n",
    "# A transition factor represents how a variable changes over time,\n",
    "# given all the things that influence it (its parents). Since the\n",
    "# next state of a variable is a function of its current state and its\n",
    "# parents' states, the variable has itself as its first parent.\n",
    "\n",
    "\n",
    "# # TODO: consider conditioning on latent patient age as well\n",
    "# TENSOR_GLUCOSE_UTILIZATION = create_normalized_random_tensor(\n",
    "#     shape=(\n",
    "#         len(STATE_VALUES_GLUCOSE_UTILIZATION),\n",
    "#         len(STATE_VALUES_GLUCOSE_UTILIZATION),\n",
    "#         len(ACTION_VALUES_BASAL_INSULIN),\n",
    "#     )\n",
    "# )       \n",
    "# pump.add_transition_factor(\n",
    "#     values=TENSOR_GLUCOSE_UTILIZATION,\n",
    "#     target=\"glucose_utilization\",\n",
    "#     parents=[\n",
    "#         \"glucose_utilization\",\n",
    "#         \"basal_insulin\",\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS = create_high_temperature_identity_tensor(\n",
    "#     shape=(\n",
    "#         len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "#         len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "#         len(STATE_VALUES_PATIENT_AGE),\n",
    "#     ),\n",
    "#     std=2,\n",
    "#     left_skew = .35,\n",
    "# )\n",
    "# NOISE_TENSOR = create_normalized_random_tensor(\n",
    "#     shape=(\n",
    "#         len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "#         len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "#         len(STATE_VALUES_PATIENT_AGE),\n",
    "#     ),\n",
    "# )\n",
    "# print(TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS.shape)\n",
    "# print(NOISE_TENSOR.shape)\n",
    "\n",
    "# TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS = add_and_normalize_tensors([\n",
    "#     TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS,\n",
    "#     NOISE_TENSOR,\n",
    "# ])\n",
    "\n",
    "\n",
    "TENSOR_LATENT_IOB = create_generalized_modulated_tensor(\n",
    "    shape=(\n",
    "        len(STATE_VALUES_IOB),\n",
    "        len(STATE_VALUES_IOB),\n",
    "        len(ACTION_VALUES_BASAL_INSULIN),\n",
    "    ),\n",
    "    base_std=3,\n",
    ")\n",
    "# pump.add_transition_factor(\n",
    "#     values=TENSOR_LATENT_IOB,\n",
    "#     target=\"latent_iob\",\n",
    "#     parents=[\n",
    "#         \"latent_iob\",\n",
    "#         \"basal_insulin\",\n",
    "#     ],\n",
    "# )\n",
    "pump.B[\"latent_iob\"].data = jnp.asarray(TENSOR_LATENT_IOB)\n",
    "\n",
    "\n",
    "TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS = create_generalized_modulated_tensor(\n",
    "    shape=(\n",
    "        len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "        len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "        len(STATE_VALUES_PATIENT_AGE),\n",
    "        # len(STATE_VALUES_MEAL),\n",
    "        # len(STATE_VALUES_IOB),\n",
    "        len(STATE_VALUES_MOMENTUM),\n",
    "        # len(ACTION_VALUES_BASAL_INSULIN),\n",
    "        len(ACTION_VALUES_CONSTANT),\n",
    "    ),\n",
    "    modulator_dims=[3],\n",
    "    base_std=2,\n",
    "    noise_level=0.5,\n",
    "    # std_mod_effects=[2],\n",
    "    # max_mode_shift_effects=[1],\n",
    ")\n",
    "\n",
    "# pump.add_transition_factor(\n",
    "#     values=TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS,\n",
    "#     target=\"uncontrolled_glucose_dynamics\",\n",
    "#     parents=[\n",
    "#         \"uncontrolled_glucose_dynamics\",\n",
    "#         \"latent_patient_age\",\n",
    "#         # \"meal\",\n",
    "#         # \"latent_iob\",\n",
    "#         \"momentum\",\n",
    "#         # \"basal_insulin\",\n",
    "#         \"constant\",\n",
    "#     ],\n",
    "# )\n",
    "pump.B[\"uncontrolled_glucose_dynamics\"].data = jnp.asarray(TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS)\n",
    "\n",
    "TENSOR_MEAL = create_normalized_random_tensor(\n",
    "    shape=(\n",
    "        len(STATE_VALUES_MEAL),\n",
    "        len(STATE_VALUES_MEAL),\n",
    "        len(ACTION_VALUES_CONSTANT),\n",
    "    ),\n",
    ")\n",
    "# pump.add_transition_factor(\n",
    "#     values=TENSOR_MEAL,\n",
    "#     target=\"meal\",\n",
    "#     parents=[\n",
    "#         \"meal\",\n",
    "#         \"constant\",\n",
    "#     ],\n",
    "# )\n",
    "pump.B[\"meal\"].data = jnp.asarray(TENSOR_MEAL)\n",
    "\n",
    "\n",
    "# Patient age is a constant across an episode,\n",
    "# so we can represent it as an identity matrix.\n",
    "# We then add a third dimension for the action variable.\n",
    "TENSOR_PATIENT_AGE = create_normalized_identity_transition_tensor(\n",
    "    shape=(\n",
    "        len(STATE_VALUES_PATIENT_AGE),\n",
    "        len(STATE_VALUES_PATIENT_AGE),\n",
    "        len(ACTION_VALUES_CONSTANT),\n",
    "    )\n",
    ")\n",
    "# pump.add_transition_factor(\n",
    "#     values=TENSOR_PATIENT_AGE,\n",
    "#     target=\"latent_patient_age\",\n",
    "#     parents=[\n",
    "#         \"latent_patient_age\",\n",
    "#         \"constant\",\n",
    "#     ],\n",
    "# )\n",
    "pump.B[\"latent_patient_age\"].data = jnp.asarray(TENSOR_PATIENT_AGE)\n",
    "\n",
    "TENSOR_MOMENTUM = create_generalized_modulated_tensor(\n",
    "    shape=(\n",
    "        len(STATE_VALUES_MOMENTUM),\n",
    "        len(STATE_VALUES_MOMENTUM),\n",
    "        len(STATE_VALUES_MEAL),\n",
    "        len(STATE_VALUES_IOB),\n",
    "        # len(ACTION_VALUES_CONSTANT),\n",
    "        len(ACTION_VALUES_BASAL_INSULIN),\n",
    "    ),\n",
    "    modulator_dims=[2, 3, 4],\n",
    "    base_std=1,\n",
    "    std_mod_effects=[0.5, -.5, -1],\n",
    "    max_mode_shift_effects=[0.5, -0.1, -0.3],\n",
    "\n",
    ")\n",
    "# pump.add_transition_factor(\n",
    "#     values=TENSOR_MOMENTUM,\n",
    "#     target=\"momentum\",\n",
    "#     parents=[\n",
    "#         \"momentum\",\n",
    "#         \"meal\",\n",
    "#         \"latent_iob\",\n",
    "#         # \"constant\",\n",
    "#         \"basal_insulin\",\n",
    "#     ],\n",
    "# )\n",
    "pump.B[\"momentum\"].data = jnp.asarray(TENSOR_MOMENTUM)\n",
    "\n",
    "######## LIKELIHOOD FACTORS ########\n",
    "# A likelihood factor represents how likely an observation is,\n",
    "# given the hidden states that influence it. This allows the POMDP\n",
    "# to reason backwards to infer the most likely hidden states.\n",
    "TENSOR_IOB = create_high_temperature_identity_tensor(\n",
    "    shape=(\n",
    "        len(OBSERVATION_VALUES_IOB),\n",
    "        len(STATE_VALUES_IOB),\n",
    "    )\n",
    ")\n",
    "\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_IOB,\n",
    "#     target=\"iob\",\n",
    "#     parents=[\n",
    "#         \"latent_iob\",\n",
    "#     ],\n",
    "# )\n",
    "pump.A[\"iob\"].data = jnp.asarray(TENSOR_IOB)\n",
    "\n",
    "# TENSOR_TOP_POLICY_COUNT_RECENT_STEPS = np.array(\n",
    "#     [\n",
    "#         [0, 0, 0, .3, .5],\n",
    "#         [0, 0, .1, .8, .5],\n",
    "#         [0, .1, .8, 0, 0],\n",
    "#         [.5, .8, .1, 0, 0],\n",
    "#         [.5, .1, 0, 0, 0],\n",
    "#     ]\n",
    "# )\n",
    "# TENSOR_TOP_POLICY_COUNT_RECENT_STEPS = np.divide(TENSOR_TOP_POLICY_COUNT_RECENT_STEPS, np.sum(TENSOR_TOP_POLICY_COUNT_RECENT_STEPS, axis=0, keepdims=True))\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_TOP_POLICY_COUNT_RECENT_STEPS,\n",
    "#     target=\"top_policy_count_recent_steps\",\n",
    "#     parents=[\n",
    "#         \"momentum\",\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "\n",
    "TENSOR_CGM = create_high_temperature_identity_tensor(\n",
    "    shape=(\n",
    "        len(OBSERVATION_VALUES_CGM),\n",
    "        # len(STATE_VALUES_GLUCOSE_UTILIZATION),\n",
    "        len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "        # len(STATE_VALUES_MEAL),\n",
    "        # len(STATE_VALUES_PATIENT_AGE),\n",
    "    )\n",
    ")\n",
    "# TENSOR_CGM = np.eye(len(OBSERVATION_VALUES_CGM))\n",
    "# TENSOR_CGM = create_normalized_random_tensor(\n",
    "#     shape=(\n",
    "#         len(OBSERVATION_VALUES_CGM),\n",
    "#         len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "#         len(STATE_VALUES_MEAL),\n",
    "#         len(STATE_VALUES_PATIENT_AGE),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_CGM,\n",
    "#     target=\"CGM\",\n",
    "#     parents=[\n",
    "#         \"uncontrolled_glucose_dynamics\",\n",
    "#         # \"meal\",\n",
    "#     ],\n",
    "# )\n",
    "pump.A[\"CGM\"].data = jnp.asarray(TENSOR_CGM)\n",
    "\n",
    "# past CGM from a previous timestep\n",
    "# TENSOR_CGM_T_MINUS_1 = create_normalized_random_tensor(\n",
    "#     shape=(\n",
    "#         len(OBSERVATION_VALUES_CGM_T_MINUS_1),\n",
    "#         # len(STATE_VALUES_GLUCOSE_UTILIZATION),\n",
    "#         # len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "#         len(STATE_VALUES_MEAL),\n",
    "#         # len(STATE_VALUES_PATIENT_AGE),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# guesswork\n",
    "TENSOR_CGM_T_MINUS_1 = jnp.array(\n",
    "    [\n",
    "        [.3, 0],\n",
    "        [.3, 0],\n",
    "        [.3, .1],\n",
    "        [.1, .2],\n",
    "        [0, .7],\n",
    "    ]\n",
    ")\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_CGM_T_MINUS_1,\n",
    "#     target=\"CGM_T-1\",\n",
    "#     parents=[\n",
    "#         # \"uncontrolled_glucose_dynamics\",\n",
    "#         \"meal\",\n",
    "#     ],\n",
    "# )\n",
    "pump.A[\"CGM_T-1\"].data = TENSOR_CGM_T_MINUS_1\n",
    "\n",
    "# guesswork\n",
    "TENSOR_CGM_T_MINUS_2 = jnp.array(\n",
    "    [\n",
    "        [.3, 0],\n",
    "        [.3, 0],\n",
    "        [.4, .02],\n",
    "        [0, .08],\n",
    "        [0, .9],\n",
    "    ]\n",
    ")\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_CGM_T_MINUS_2,\n",
    "#     target=\"CGM_T-2\",\n",
    "#     parents=[\n",
    "#         # \"uncontrolled_glucose_dynamics\",\n",
    "#         \"meal\",\n",
    "#     ],\n",
    "# )\n",
    "pump.A[\"CGM_T-2\"].data = jnp.asarray(TENSOR_CGM_T_MINUS_2)\n",
    "\n",
    "# # print(f\"OBSERVATION_VALUES_BASAL_INSULIN_T_MINUS_1: {len(OBSERVATION_VALUES_BASAL_INSULIN_T_MINUS_1)}\")\n",
    "# # print(f\"STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS: {len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS)}\")\n",
    "# TENSOR_BASAL_INSULIN_T_MINUS_1 = create_normalized_random_tensor(\n",
    "#     shape=(\n",
    "#         len(OBSERVATION_VALUES_BASAL_INSULIN_T_MINUS_1),\n",
    "#         len(STATE_VALUES_UNCONTROLLED_GLUCOSE_DYNAMICS),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_BASAL_INSULIN_T_MINUS_1,\n",
    "#     target=\"basal_insulin_T-1\",\n",
    "#     parents=[\n",
    "#         \"uncontrolled_glucose_dynamics\",\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# a likelihood factor for the patient's age, which is an indicator \n",
    "# of the endogenous insulin production.\n",
    "TENSOR_PATIENT_AGE = jnp.eye(len(OBSERVATION_VALUES_PATIENT_AGE))\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_PATIENT_AGE,\n",
    "#     target=\"patient_age\",\n",
    "#     parents=[\n",
    "#         \"latent_patient_age\",\n",
    "#     ],\n",
    "# )\n",
    "pump.A[\"patient_age\"].data = jnp.asarray(TENSOR_PATIENT_AGE)\n",
    "\n",
    "# TENSOR_HOUR = create_normalized_random_tensor(\n",
    "#     shape=(\n",
    "#         len(OBSERVATION_VALUES_HOUR),\n",
    "#         len(STATE_VALUES_MEAL),\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# from the simulator\n",
    "TENSOR_HOUR = jnp.array(\n",
    "    [\n",
    "        [1/8, 0.0976],\n",
    "        [1/8, 0.1550],\n",
    "        [1/8, 0.0818],\n",
    "        [1/8, 0.1823],\n",
    "        [1/8, 0.2100],\n",
    "        [1/8, 0.0981],\n",
    "        [1/8, 0.0707],\n",
    "        [1/8, 0.1046],\n",
    "    ]\n",
    ")\n",
    "TENSOR_HOUR = TENSOR_HOUR / jnp.sum(TENSOR_HOUR, axis=0, keepdims=True)\n",
    "# pump.add_likelihood_factor(\n",
    "#     values=TENSOR_HOUR,\n",
    "#     target=\"hour\",\n",
    "#     parents=[\n",
    "#         \"meal\",\n",
    "#     ],\n",
    "# )\n",
    "# pump.A[\"hour\"].data = TENSOR_HOUR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## PREFERENCE FACTORS ########\n",
    "# The agent prefers to keep CGM within a certain range.\n",
    "# We use a gaussian distribution to reward the model for \n",
    "# being in the center of the range.\n",
    "preferences = dist_gaussian(\n",
    "    N=len(OBSERVATION_VALUES_CGM),\n",
    "    scaled_mode=0.31,\n",
    "    right_skew=.1,\n",
    "    std=1.6,\n",
    ")\n",
    "# Subtracting a small value creates a penalty for being outside the range.\n",
    "TENSOR_CGM_PREFERENCES = jnp.array([x -.01 for x in preferences])\n",
    "\n",
    "\n",
    "# Values at the extremes of the range should be penalized more heavily.\n",
    "# This produces a reward gradient for the model to follow back\n",
    "# to where it should be.\n",
    "max_idx = jnp.argmax(TENSOR_CGM_PREFERENCES)\n",
    "\n",
    "TENSOR_CGM_PREFERENCES = np.zeros(len(OBSERVATION_VALUES_CGM))\n",
    "DEAD_PENALTY = -5\n",
    "GOLDILOCKS_REWARD = 5\n",
    "PEAK_REWARD_VALUE = 110\n",
    "\n",
    "def exponential_penalty(x, x_start, x_end, y_start, y_end):\n",
    "\n",
    "    if np.isclose(x_start, x_end):\n",
    "        return np.full(np.shape(x), y_start, dtype=np.float64)\n",
    "\n",
    "    dx = x_end - x_start\n",
    "    dy = y_end - y_start\n",
    "    if np.isclose(dy, 0.0):\n",
    "        return np.full(np.shape(x), y_start, dtype=np.float64)\n",
    "    if np.isclose(abs(dy), 2.0):\n",
    "        result = y_start + (x - x_start) * (dy / dx)\n",
    "        if isinstance(result, np.ndarray):\n",
    "            return result.astype(np.float64)\n",
    "        return np.float64(result)\n",
    "    val_for_log = abs(dy) / 2.0\n",
    "    b = np.log(val_for_log) / dx\n",
    "    denominator_a = val_for_log - 1.0\n",
    "    a = dy / denominator_a\n",
    "    c = y_start - a\n",
    "    return a * np.exp(b*(x-x_start)) + c\n",
    "\n",
    "for i, glucose in enumerate(OBSERVATION_VALUES_CGM):\n",
    "    glucose = float(glucose)\n",
    "    if glucose == 50:\n",
    "        TENSOR_CGM_PREFERENCES[i] = DEAD_PENALTY\n",
    "    elif glucose <= 70:\n",
    "        # Exponential interpolation between Dead penalty at 50 and 0 at 70\n",
    "        TENSOR_CGM_PREFERENCES[i] = exponential_penalty(glucose, 50, 70, DEAD_PENALTY, 0)\n",
    "    elif glucose < PEAK_REWARD_VALUE:\n",
    "        # Linear interpolation between 0 at 70 and GOLDILOCKS_REWARD at PEAK_REWARD_VALUE\n",
    "        TENSOR_CGM_PREFERENCES[i] = (glucose - 70) * (GOLDILOCKS_REWARD/(PEAK_REWARD_VALUE-70))\n",
    "    elif glucose == PEAK_REWARD_VALUE:\n",
    "        TENSOR_CGM_PREFERENCES[i] = GOLDILOCKS_REWARD\n",
    "    elif glucose <= 180:\n",
    "        # Linear interpolation between GOLDILOCKS_REWARD at PEAK_REWARD_VALUE and 0 at 180\n",
    "        TENSOR_CGM_PREFERENCES[i] = GOLDILOCKS_REWARD - (glucose - PEAK_REWARD_VALUE) * (GOLDILOCKS_REWARD/(180-PEAK_REWARD_VALUE))\n",
    "    elif glucose < 600:\n",
    "        # Exponential interpolation between 0 at 180 and DEAD_PENALTY at 600\n",
    "        TENSOR_CGM_PREFERENCES[i] = exponential_penalty(glucose, 180, 600, 0, DEAD_PENALTY)\n",
    "    elif glucose == 600:\n",
    "        TENSOR_CGM_PREFERENCES[i] = DEAD_PENALTY\n",
    "    else:\n",
    "        TENSOR_CGM_PREFERENCES[i] = DEAD_PENALTY\n",
    "\n",
    "# print(\"Preference values for CGM:\")\n",
    "# print(list(zip(OBSERVATION_VALUES_CGM, [round(x, 2) for x in TENSOR_CGM_PREFERENCES])))\n",
    "plot_reward_function(list(zip(OBSERVATION_VALUES_CGM, [round(x, 2) for x in TENSOR_CGM_PREFERENCES])))\n",
    "\n",
    "# pump.add_preference_factor(\n",
    "#     values=np.array(TENSOR_CGM_PREFERENCES),\n",
    "#     target=\"CGM\",\n",
    "# )\n",
    "pump.C[\"CGM\"].data = jnp.array(TENSOR_CGM_PREFERENCES)\n",
    "\n",
    "\n",
    "# # don't use too much insulin\n",
    "# TENSOR_IOB_PREFERENCES = [0 for _ in range(len(OBSERVATION_VALUES_IOB))]\n",
    "# TENSOR_IOB_PREFERENCES[-1] = -1\n",
    "# pump.add_preference_factor(\n",
    "#     values=np.array(TENSOR_IOB_PREFERENCES),\n",
    "#     target=\"iob\",\n",
    "# )\n",
    "\n",
    "# COOPER: Not sure how to format a prior in the new PyMDP\n",
    "# ######## PRIORS ########\n",
    "# pump.add_prior_factor(\n",
    "#     # values=np.array([.6, .1, .05, .1, .15]),\n",
    "#     values=np.array([.3, .05, .65]), # haven't eaten, meal absorption high, meal absorption low\n",
    "#     target=\"meal\",\n",
    "# )\n",
    "\n",
    "######## VALIDATION ########\n",
    "# pump.validate()\n",
    "\n",
    "# ######## PLOT TENSOR SLICE ########\n",
    "# plot_tensor_slice(TENSOR_UNCONTROLLED_GLUCOSE_DYNAMICS, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_size = 0\n",
    "# for factor in pump.vfg.factors:\n",
    "#     print(f\"\\n{factor.variables}\")\n",
    "#     print(factor.values.shape)\n",
    "#     print(factor.values.size)\n",
    "#     total_size += factor.values.size\n",
    "\n",
    "# for variable, data in pump.vfg.variables.items():\n",
    "#     print(f\"\\n{variable}\")\n",
    "#     print(data.elements)\n",
    "#     print(len(data.elements))\n",
    "# print(f\"\\ntotal_size: {total_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the agent to Genius\n",
    "\n",
    "Here we connect to our running instance of Genius and upload our model. If you have a cloud instance, you'll need to change the hostname and port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax.tree_util as jtu\n",
    "# agent_http_protocol, agent_hostname, agent_port = \"http\", \"localhost\", 3000\n",
    "\n",
    "# agent = GeniusAgent(\n",
    "#     agent_http_protocol=agent_http_protocol,\n",
    "#     agent_hostname=agent_hostname,\n",
    "#     agent_port=agent_port,\n",
    "# )\n",
    "# agent.load_genius_model(model=pump)\n",
    "\n",
    "\n",
    "from pymdp.agent import Agent\n",
    "\n",
    "\n",
    "B_action_dependencies = [\n",
    "    [list(model_description[\"controls\"].keys()).index(i) for i in s[\"controlled_by\"]] \n",
    "    for s in model_description[\"states\"].values()\n",
    "]\n",
    "num_controls = [len(c[\"elements\"]) for c in model_description[\"controls\"].values()]\n",
    "\n",
    "A = [pump.A[i].data for i in model_description[\"observations\"].keys()]\n",
    "B = [pump.B[i].data for i in model_description[\"states\"].keys()]\n",
    "\n",
    "# pA = jtu.tree_map(lambda a: jnp.ones_like(a), A)\n",
    "# pB = jtu.tree_map(lambda b: jnp.ones_like(b), B)\n",
    "pA = [jnp.array(a, dtype=jnp.float32) for a in A]\n",
    "pB = [jnp.array(b, dtype=jnp.float32) for b in B]\n",
    "\n",
    "POLICY_LEN = 3\n",
    "\n",
    "agent = Agent(\n",
    "    A=pump.A,\n",
    "    # B=jnp.broadcast_to(jnp.asarray(pump.B), (1,) + pump.B.shape),\n",
    "    # B=jnp.broadcast_to(pump.B, (1,) + pump.B.shape),\n",
    "    # B=jnp.asarray(pump.B),\n",
    "    B=pump.B,\n",
    "    C=pump.C,\n",
    "    D=pump.D,\n",
    "    pA=pA,\n",
    "    pB=pB,\n",
    "    batch_size=1,\n",
    "    B_action_dependencies=B_action_dependencies,\n",
    "    num_controls=num_controls,\n",
    "    policy_len=POLICY_LEN,\n",
    "    # learn_A=True,\n",
    "    learn_B=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Continual Learning\n",
    "\n",
    "It's time to put our model to the test. We'll run through an episode with each patient, and see how well our model improves over time.\n",
    "\n",
    "Everything needed to actually run the agent is inside the \"ACTION LOOP\" section. The rest of the code is just for plotting and tracking statistics. We'll use two metrics of success:\n",
    "- Survival: How many episodes can the agent get through without `risk` reaching 100?\n",
    "- Time in Range (TIR): What percentage of the time does the agent spend in the CGM range of 100-180 mg/dL?\n",
    "\n",
    "Both of these metrics should improve over time as the agent learns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(agent))\n",
    "print(agent.num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax.tree_util as jtu\n",
    "from jax import random as jr\n",
    "from pymdp.envs.rollout import infer_and_plan\n",
    "from jax import jit, vmap\n",
    "\n",
    "\n",
    "# SOJOURN_TIME = 5\n",
    "\n",
    "MAX_EPISODE_LENGTH = 500 # about 8 hours\n",
    "# MAX_EPISODE_LENGTH = 500 * SOJOURN_TIME # about 8 hours\n",
    "\n",
    "PAST_OBSERVATION_LAG_MULTIPLE = 1\n",
    "\n",
    "# learning hyperparameters\n",
    "LEARNING_STOP_BUFFER_EPISODES = 40\n",
    "LEARN_INTERVAL_EPISODES = 10\n",
    "LEARN_INTERVAL_STEPS = 500 * SOJOURN_TIME\n",
    "\n",
    "# Toggles\n",
    "LEARN_LIKELIHOODS = False\n",
    "LEARN_TRANSITIONS = True\n",
    "USING_ALTERNATION = False\n",
    "LEARNING_STOPPED = False\n",
    "LEARNING_STOPPED_EPISODE = None\n",
    "SAVE_ANIMATION = False\n",
    "\n",
    "# Track cumulative metrics across episodes\n",
    "cumulative_optimality = []\n",
    "cumulative_survival = []\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "try:\n",
    "    os.mkdir(f\"out/{TIMESTAMP}\") # output folder for plots from this run\n",
    "    os.mkdir(f\"out/{TIMESTAMP}/vfg-episodes\")\n",
    "    os.mkdir(f\"out/{TIMESTAMP}/control_results\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# pump.save(f\"out/{TIMESTAMP}/vfg-0.json\") # save the initial VFG\n",
    "\n",
    "total_steps_across_episodes = 0\n",
    "total_episodes_completed = 0\n",
    "total_survival_steps = 0\n",
    "total_optimal_steps = 0\n",
    "\n",
    "random_gyms = [(gym_name, gym) for gym_name, gym in gyms.items()] * 25 * SOJOURN_TIME\n",
    "random.shuffle(random_gyms)\n",
    "print(f\"Total # patients: {len(random_gyms)}\")\n",
    "for gym_name, gym in random_gyms:\n",
    "\n",
    "    # Alternate likelihoods and transitions every so often\n",
    "    if USING_ALTERNATION and ((total_episodes_completed+1) % LEARN_INTERVAL_EPISODES == 0):\n",
    "        print(f\"Episode {total_episodes_completed+1}\")\n",
    "        if LEARN_LIKELIHOODS == True:\n",
    "            LEARN_LIKELIHOODS = False\n",
    "        else:\n",
    "            LEARN_LIKELIHOODS = True\n",
    "        \n",
    "        if LEARN_TRANSITIONS == True:\n",
    "            LEARN_TRANSITIONS = False\n",
    "        else:\n",
    "            LEARN_TRANSITIONS = True\n",
    "\n",
    "    \n",
    "\n",
    "    obs, state_info = gym.reset()\n",
    "    action_history = [0] * SOJOURN_TIME #starting with lowest basal insulin value\n",
    "    print(f\"discretizing obs: {discretize_observation({\"CGM\": obs[\"CGM\"]}, {\"CGM\": OBSERVATION_VALUES_CGM})[\"CGM\"],}\")\n",
    "    obs = {\n",
    "        # \"CGM\": discretize_observation({\"CGM\": obs[\"CGM\"]}, {\"CGM\": OBSERVATION_VALUES_CGM})[\"CGM\"],\n",
    "        \"CGM\": obs[\"CGM\"],\n",
    "        \"patient_age\": obs[\"patient_age\"],\n",
    "        \"CGM_T-1\": str(len(OBSERVATION_VALUES_CGM_T_MINUS_1)//2), #starting with the same CGM value\n",
    "        \"CGM_T-2\": str(len(OBSERVATION_VALUES_CGM_T_MINUS_2)//2),\n",
    "        \"iob\": \"0\",\n",
    "        # \"hour\": \"60\",\n",
    "        # \"top_policy_count_recent_steps\": \"0\",\n",
    "    } \n",
    "    print(f\"initial obs: {obs}\")\n",
    "    obs = discretize_observation(obs, {\n",
    "        \"CGM\": OBSERVATION_VALUES_CGM,\n",
    "        \"patient_age\": OBSERVATION_VALUES_PATIENT_AGE,\n",
    "        \"CGM_T-1\": OBSERVATION_VALUES_CGM_T_MINUS_1,\n",
    "        \"CGM_T-2\": OBSERVATION_VALUES_CGM_T_MINUS_2,\n",
    "        \"iob\": OBSERVATION_VALUES_IOB,\n",
    "    })\n",
    "    print(f\"discretized obs: {obs}\")\n",
    "    \n",
    "    cgm_history = [obs[\"CGM\"]]\n",
    "    # experiment: get slope as CGM_T-1\n",
    "    slope = len(OBSERVATION_VALUES_CGM_T_MINUS_1) // 2\n",
    "    obs[\"CGM_T-1\"] = str(slope)\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    # plt.ioff() # Turn off interactive mode - No longer needed for saving animation\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 9), sharex=True)\n",
    "    fig.suptitle(f\"Patient: {gym_name} (Training Episode {total_episodes_completed + 1})\")\n",
    "\n",
    "    # Setup subplots\n",
    "    ax[0].set_ylabel(\"Value\")\n",
    "    ax[0].set_title(\"Risk and CGM (Discretized) over Time\")\n",
    "    ax[0].grid(True)\n",
    "    line_risk, = ax[0].plot([], [], label='Risk of Hypo- or Hyperglycemia (%)')\n",
    "    line_cgm_raw, = ax[0].plot([], [], label='CGM (mg glucose / dL blood)')\n",
    "    line_cgm, = ax[0].plot([], [], label='CGM (Discretized)')\n",
    "    line_iob, = ax[0].plot([], [], label='Insulin on Board x 2 (Discretized)', color='purple')\n",
    "    # Placeholder for meal lines legend item\n",
    "    meal_legend_line, = ax[0].plot([], [], color='r', linestyle='--', label='Meal (unobserved)', lw=0.8)\n",
    "    ax[0].legend(loc='upper left')\n",
    "    # Store the artist for meal vlines using a mutable container\n",
    "    plot_state = {'meal_artist': None}\n",
    "\n",
    "    ax[1].set_ylabel(\"Basal Insulin (units/min)\")\n",
    "    ax[1].set_title(\"Action (Basal Insulin) over Time\")\n",
    "    ax[1].grid(True)\n",
    "    line_action, = ax[1].plot([], [], marker='.')\n",
    "    # Set fixed y-axis for insulin action\n",
    "    max_insulin_value = max(float(x) for x in RAW_ACTION_VALUES_BASAL_INSULIN) if RAW_ACTION_VALUES_BASAL_INSULIN else 1 # Handle empty list case\n",
    "    ax[1].set_ylim(-0.05 * max_insulin_value, max_insulin_value * 1.05) # Add 5% padding\n",
    "\n",
    "    ax[2].set_ylabel(\"TIR Score (%)\")\n",
    "    ax[2].set_title(\"% Time without Hypo- or Hyperglycemia and Current TIR\")\n",
    "    ax[2].set_ylim(0, 105)\n",
    "    ax[2].grid(True)\n",
    "    line_optimality, = ax[2].plot([], [], label='Current TIR (%)', color='g')\n",
    "    score_text = ax[2].text(0.02, 0.95, \"\", transform=ax[2].transAxes, verticalalignment='top')\n",
    "    ax[2].set_xlabel(\"Step (minutes)\")\n",
    "    # Add legend to the third subplot\n",
    "    ax[2].legend(loc='lower left')\n",
    "\n",
    "\n",
    "    # Data storage for plotting\n",
    "    steps_list = []\n",
    "    cgm_values_discrete = []\n",
    "    cgm_values_raw = []\n",
    "    risk_values = []\n",
    "    action_values = []\n",
    "    meal_steps = []\n",
    "    meal_amounts = []\n",
    "    optimal_steps_count_episode = 0\n",
    "    optimality_perc_list = []\n",
    "    iob_values = []\n",
    "\n",
    "    top_policy_usage_list = []\n",
    "    \n",
    "\n",
    "    # Display initial plot structure - Removed for animation saving\n",
    "    # display_handle = display.display(fig, display_id=True)\n",
    "    # plt.ion() # Turn interactive mode back on - Removed for animation saving\n",
    "\n",
    "\n",
    "    # start with None action, and expand agent.D to add time dimension\n",
    "    # action = -jnp.ones((agent.batch_size, len(agent.num_controls)), dtype=jnp.int32)\n",
    "    \n",
    "    action_idx = 0\n",
    "    action_prev = None\n",
    "    action = None\n",
    "\n",
    "    qs = jtu.tree_map(lambda x: jnp.expand_dims(x, -2), agent.D)\n",
    "\n",
    "    # reset environment and get initial observation\n",
    "    rng_key = jr.PRNGKey(0)\n",
    "    keys = jr.split(rng_key, agent.batch_size + 1)\n",
    "    rng_key = keys[0]\n",
    "\n",
    "    \n",
    "    # get the prior\n",
    "    qs = jtu.tree_map(lambda x: jnp.expand_dims(x, 1), agent.D) # qs needs a time dimension too\n",
    "\n",
    "\n",
    "\n",
    "    learn_history = []\n",
    "    while state_info[\"risk\"] < 100 and step < MAX_EPISODE_LENGTH:\n",
    "\n",
    "        # --------------------------- ACTION LOOP ---------------------------\n",
    "        # print(f\"obs: {obs}\")\n",
    "        # if obs.keys() != pump.vfg.variables.keys():\n",
    "        #     # print the keys that are in obs but not in pump.vfg.variables\n",
    "        #     print(f\"keys in obs but not in pump.vfg.variables: {set(obs.keys()) - set(pump.vfg.variables.keys())}\")\n",
    "        #     print(f\"keys in pump.vfg.variables but not in obs: {set(pump.vfg.variables.keys()) - set(obs.keys())}\")\n",
    "        if step % SOJOURN_TIME == 0:\n",
    "            \n",
    "            # agent.infer_states(obs, qs_init)\n",
    "            # qs = jtu.tree_map(lambda x: jnp.expand_dims(x, axis=0), agent.D)\n",
    "            # q_pi, G = agent.infer_policies(qs)\n",
    "            # action = agent.sample_action(q_pi)\n",
    "            # control_result = agent.decode_multi_actions(action)\n",
    "\n",
    "\n",
    "\n",
    "            obs = one_hot_encode_observation(obs, [len(OBSERVATION_VALUES_CGM), len(OBSERVATION_VALUES_PATIENT_AGE), len(OBSERVATION_VALUES_CGM_T_MINUS_1), len(OBSERVATION_VALUES_CGM_T_MINUS_2), len(OBSERVATION_VALUES_IOB)])\n",
    "            obs = jtu.tree_map(lambda x: x[None, ...], obs)\n",
    "            # print(f\"obs shapes: {[x.shape for x in obs]}\")\n",
    "            # print(f\"one hot encoded obs: {obs}\")\n",
    "        \n",
    "\n",
    "            action_prev = action\n",
    "            action = agent.encode_multi_actions(jnp.array([[action_idx,0]]))\n",
    "            qs_prev = qs\n",
    "            # print(f\"type of agent: {type(agent)}\")\n",
    "            # print(f\"type of qs: {type(qs)}\")\n",
    "            # print(f\"type of obs: {type(obs)}\")\n",
    "            # print(f\"type of action: {type(action)}\")\n",
    "            # print(f\"type of rng_key: {type(rng_key)}\")\n",
    "            agent, action, qs, info = jit(infer_and_plan)(agent, qs, obs, action, rng_key)\n",
    "            keys = jr.split(rng_key, agent.batch_size + 1)\n",
    "            rng_key = keys[0]\n",
    "\n",
    "            # print(f\"qs differs from qs_prev: {any([not jnp.allclose(x, y, atol=1e-4) for x, y in zip(qs, qs_prev)])}\")\n",
    "\n",
    "            # # for learning A and/or B\n",
    "            # if action_prev is not None and action is not None:\n",
    "            #     if LEARN_LIKELIHOODS or LEARN_TRANSITIONS:\n",
    "            #         if LEARN_TRANSITIONS:\n",
    "            #             # stacking beliefs for B learning\n",
    "            #             beliefs_B = jtu.tree_map(\n",
    "            #                 lambda x, y: jnp.concatenate([x, y], axis=1), qs_prev, qs\n",
    "            #             )\n",
    "            #             # reshaping action to match the stacked beliefs\n",
    "            #             action_B = jnp.expand_dims(action_prev, 1)  # adding time dimension\n",
    "            #         else:\n",
    "            #             beliefs_B = None\n",
    "            #             action_B = action_prev\n",
    "\n",
    "            #         prev_agent = agent\n",
    "\n",
    "            #         agent = agent.infer_parameters(\n",
    "            #             qs,\n",
    "            #             obs,\n",
    "            #             action_B if agent.learn_B else action_prev,\n",
    "            #             beliefs_B=beliefs_B,\n",
    "            #         )\n",
    "\n",
    "            #         print(f\"agent B differs from prev_agent: {any([not jnp.allclose(x, y, atol=1e-4) for x, y in zip(agent.B, prev_agent.B)])}\")\n",
    "            \n",
    "            # print(f\"action: {action}\")\n",
    "            # print(f\"info: {info}\")\n",
    "            # print(f\"action maps: {agent.action_maps}\")\n",
    "            control_result = agent.decode_multi_actions(action)\n",
    "            action_idx = control_result[0][0]\n",
    "            # control_result = agent.encode_multi_actions(control_result)\n",
    "\n",
    "            # # save the agent.model to a file\n",
    "            # agent.model.save(f\"out/{TIMESTAMP}/vfg-episodes/vfg-b-{total_episodes_completed}.json\")\n",
    "\n",
    "            # # # save the control result to JSON file\n",
    "            # with open(f\"out/{TIMESTAMP}/control_results/control_result_{total_episodes_completed + 1}_{gym_name}.json\", \"w\") as f:\n",
    "            #     json.dump(control_result, f)\n",
    "\n",
    "            # track how often the top policy is used\n",
    "            # if control_result[\"action_data\"][\"basal_insulin\"][\"selected_action\"] == len(ACTION_VALUES_BASAL_INSULIN) - 1:\n",
    "            if action_idx == len(agent.num_controls) - 1:\n",
    "                top_policy_usage_list.append(1)\n",
    "            else:\n",
    "                top_policy_usage_list.append(0)\n",
    "        else:\n",
    "            top_policy_usage_list.append(0)\n",
    "        # print(f\"variables with chosen actions: {control_result['action_data'].keys()}\")\n",
    "        if step > 0:\n",
    "            cgm_history.append(response[\"state\"][\"CGM\"]) #append previous CGM\n",
    "        if SOJOURN_TIME > 1:\n",
    "            # loop through the policy \"arpeggio\" selected\n",
    "            i = step % SOJOURN_TIME\n",
    "            i2 = i % len(BASAL_INSULIN_ARPEGGIOS_3[0])\n",
    "            policy = control_result[\"action_data\"][\"basal_insulin\"][\"selected_action\"]\n",
    "            action = str(BASAL_INSULIN_ARPEGGIOS_3[policy][i2])\n",
    "        else:\n",
    "            # action = control_map(\n",
    "            #     controls=ACTION_VALUES_BASAL_INSULIN,\n",
    "            #     chosen_action=control_result[\"action_data\"][\"basal_insulin\"][\"selected_action\"],\n",
    "            # )\n",
    "            action = ACTION_VALUES_BASAL_INSULIN[action_idx]\n",
    "        response = get_response_from_glucose_gym(action, gym)\n",
    "        learn_history.append(obs)\n",
    "        \n",
    "        # Capture original CGM before discretization for optimality check\n",
    "        original_cgm = response['state']['CGM']\n",
    "\n",
    "        # Now discretize the state\n",
    "        response[\"state\"] = discretize_observation(response[\"state\"], {\"CGM\": OBSERVATION_VALUES_CGM})\n",
    "        # print(f\"using CGM_T-1 with lag multiple: {PAST_OBSERVATION_LAG_MULTIPLE}\")\n",
    "        current_iob = calculate_and_bin_iob(\n",
    "            past_dosages_per_minute=[float(RAW_ACTION_VALUES_BASAL_INSULIN[x]) for x in action_history],\n",
    "            num_bins=len(OBSERVATION_VALUES_IOB),\n",
    "            max_expected_iob=5.5, # magic number\n",
    "            # max_possible_dose=float(RAW_ACTION_VALUES_BASAL_INSULIN[-1]),\n",
    "            aggregation_window_minutes=IOB_AGGREGATION_WINDOW_MINUTES,\n",
    "            effect_half_life_minutes=75,\n",
    "        )\n",
    "        # count the number of times the top policy has been selected in the past 5 steps\n",
    "        top_policy_raw_count_recent_steps = sum(top_policy_usage_list[-OBSERVATION_VALUES_TOP_POLICY_COUNT_RECENT_STEPS_LEVELS * SOJOURN_TIME:])\n",
    "        if top_policy_raw_count_recent_steps > 0:\n",
    "            top_policy_count_recent_steps = str(min(top_policy_raw_count_recent_steps, OBSERVATION_VALUES_TOP_POLICY_COUNT_RECENT_STEPS_LEVELS-1))\n",
    "        else:\n",
    "            top_policy_count_recent_steps = \"0\"\n",
    "\n",
    "        act_1 = action_history[-PAST_OBSERVATION_LAG_MULTIPLE * SOJOURN_TIME]\n",
    "        age = response[\"state\"][\"patient_age\"]\n",
    "        obs = {\n",
    "            \"CGM\": response[\"state\"][\"CGM\"],\n",
    "            \"patient_age\": 0 if age == \"adolescent\" else 1,\n",
    "            \"CGM_T-1\": str(get_slope_n_back(cgm_history[:i], 20, len(OBSERVATION_VALUES_CGM_T_MINUS_1), (-1, 2))),\n",
    "            \"CGM_T-2\": str(get_slope_n_back(cgm_history[:i], 20, len(OBSERVATION_VALUES_CGM_T_MINUS_2), (-5, 5), calculate_acceleration=True)),\n",
    "            \"iob\": str(current_iob),\n",
    "            # \"basal_insulin_T-1\": OBSERVATION_VALUES_BASAL_INSULIN_T_MINUS_1[act_1],\n",
    "            # \"hour\": discretize_observation({\"hour\": step}, {\"hour\": OBSERVATION_VALUES_HOUR})[\"hour\"],\n",
    "            # \"top_policy_count_recent_steps\": top_policy_count_recent_steps\n",
    "        }\n",
    "\n",
    "        # learn parameters\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "        if SOJOURN_TIME > 1:\n",
    "            action_history.append(RAW_ACTION_VALUES_BASAL_INSULIN.index(float(action)))\n",
    "        else:\n",
    "            # action_history.append(control_result[\"action_data\"][\"basal_insulin\"][\"selected_action\"]) # append the action afterwards\n",
    "            action_history.append(control_result[0][0]) # append the action afterwards\n",
    "        # obs = {\"CGM\": response[\"state\"][\"CGM\"], \"patient_age\": response[\"state\"][\"patient_age\"]}\n",
    "        # print(f\"{step}\\tgym: {gym_name}\\taction: {float(action):0.3f}\\tmeal: {response['state']['meal']:0.3f}\\tCGM_orig: {original_cgm:.1f}\\tCGM_disc: {response['state']['CGM']}\\trisk: {response['state']['risk']:.3f}\")\n",
    "\n",
    "        # --------------------------- / ACTION LOOP ---------------------------\n",
    "\n",
    "        steps_list.append(step)\n",
    "        current_cgm_discrete = response['state']['CGM']\n",
    "        cgm_values_discrete.append(float(current_cgm_discrete))\n",
    "        cgm_values_raw.append(float(original_cgm))\n",
    "        risk_values.append(response['state']['risk'])\n",
    "        action_values.append(float(action))\n",
    "        meal_amounts.append(response['state']['meal'])\n",
    "        if response['state']['meal'] > 0:\n",
    "            meal_steps.append(step)\n",
    "        iob_values.append(current_iob)\n",
    "\n",
    "        # Check for optimal range using the original CGM value\n",
    "        if 70 <= original_cgm <= 180:\n",
    "             optimal_steps_count_episode += 1\n",
    "        current_optimality_perc = (optimal_steps_count_episode / (step + 1)) * 100 if step >= 0 else 0\n",
    "        optimality_perc_list.append(current_optimality_perc)\n",
    "\n",
    "        state_info = response[\"state\"]\n",
    "        step += 1\n",
    "\n",
    "    final_step_count = step # Total frames for the animation\n",
    "\n",
    "    try:\n",
    "        # Create DataFrame to store all values\n",
    "        plot_data = pd.DataFrame({\n",
    "            'Step': steps_list,\n",
    "            'Episode': [total_episodes_completed + 1] * len(steps_list),\n",
    "            'CGM_Discrete': [int(x) for x in cgm_values_discrete],\n",
    "            'CGM_Raw': [float(x) for x in cgm_values_raw],\n",
    "            'Risk': risk_values,\n",
    "            'Action': action_values,\n",
    "            'meal_amounts': meal_amounts,\n",
    "            'TIR': optimality_perc_list,\n",
    "            'IOB': iob_values,\n",
    "            'learn_likelihoods': [LEARN_LIKELIHOODS] * len(steps_list),\n",
    "            'learn_transitions': [LEARN_TRANSITIONS] * len(steps_list),\n",
    "        })\n",
    "        plot_data.to_csv(f\"out/{TIMESTAMP}/episode_{total_episodes_completed + 1}_{gym_name}.csv\", index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving episode data: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- Animation Setup ---\n",
    "    # Make sure meal legend item is added only if meals occurred\n",
    "    if not meal_steps:\n",
    "        meal_legend_line.set_visible(False) # Hide the placeholder if no meals\n",
    "        ax[0].legend(loc='upper left') # Redraw legend without meal line if needed\n",
    "\n",
    "    def animate(frame):\n",
    "        \n",
    "        # Update line data up to current frame\n",
    "        current_steps = steps_list[:frame+1]\n",
    "        line_risk.set_data(current_steps, risk_values[:frame+1])\n",
    "        line_cgm_raw.set_data(current_steps, cgm_values_raw[:frame+1])\n",
    "        line_cgm.set_data(current_steps, cgm_values_discrete[:frame+1])\n",
    "        line_iob.set_data(current_steps, [x*2 for x in iob_values[:frame+1]])\n",
    "        line_action.set_data(current_steps, action_values[:frame+1])\n",
    "        line_optimality.set_data(current_steps, optimality_perc_list[:frame+1])\n",
    "\n",
    "        if plot_state['meal_artist']:\n",
    "            plot_state['meal_artist'].remove()\n",
    "            plot_state['meal_artist'] = None # Reset\n",
    "\n",
    "        current_meal_steps = [s for s in meal_steps if s <= frame]\n",
    "        if current_meal_steps:\n",
    "            ymin, ymax = ax[0].get_ylim() # Get current y-limits for vlines\n",
    "            # Ensure ymin/ymax are valid before drawing\n",
    "            if ymax > ymin:\n",
    "                 # Assign to the dictionary key\n",
    "                 plot_state['meal_artist'] = ax[0].vlines(current_meal_steps, ymin=ymin, ymax=ymax, color='r', linestyle='--', lw=0.8)\n",
    "\n",
    "        # Update score text to show current episode's TIR at this frame\n",
    "        current_tir = optimality_perc_list[frame] if frame < len(optimality_perc_list) else 0\n",
    "        score_str = f\"Current TIR: {current_tir:.1f}%\"\n",
    "        score_text.set_text(score_str)\n",
    "\n",
    "        # Rescale axes based on data up to current frame\n",
    "        ax[0].relim()\n",
    "        ax[0].autoscale_view() # Autoscale both axes for CGM/Risk\n",
    "        ax[1].relim()\n",
    "        ax[1].autoscale_view(scalex=True, scaley=False) # Autoscale x, keep y fixed for Action\n",
    "        ax[2].relim()\n",
    "        ax[2].autoscale_view(scalex=True, scaley=False) # Autoscale x, keep y fixed for TIR plot\n",
    "\n",
    "        # Return tuple of artists that were modified\n",
    "        # Return tuple of artists that were modified\n",
    "        artists = [line_risk, line_cgm_raw, line_cgm, line_iob, line_action, line_optimality, score_text]\n",
    "        # Use the dictionary to check if the artist exists\n",
    "        if plot_state['meal_artist']:\n",
    "            artists.append(plot_state['meal_artist'])\n",
    "        return tuple(artists)\n",
    "\n",
    "    # --- Create and Save Animation ---\n",
    "    if SAVE_ANIMATION:\n",
    "        print(f\"Creating animation for Episode {total_episodes_completed + 1} ({final_step_count} frames)...\")\n",
    "        # Set interval (milliseconds per frame) - e.g., 50ms for 20fps\n",
    "        anim = FuncAnimation(fig, animate, frames=final_step_count, interval=50, blit=False)\n",
    "\n",
    "        # Save the animation\n",
    "        animation_filename = f\"out/{TIMESTAMP}/episode_{total_episodes_completed + 1}_{gym_name}.gif\"\n",
    "        try:\n",
    "            # if SAVE_ANIMATION:\n",
    "            anim.save(animation_filename, writer='pillow')\n",
    "            print(f\"Saved animation to {animation_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving animation: {e}\")\n",
    "            print(\"Ensure 'pillow' is installed (`pip install pillow`).\")\n",
    "    else:\n",
    "        # If not saving animation, just draw the final plot state\n",
    "        if final_step_count > 0:\n",
    "            animate(final_step_count - 1)\n",
    "\n",
    "\n",
    "    # --- Save Static Plot (as before) ---\n",
    "    # Update title for final static plot\n",
    "    fig.suptitle(f\"Gym: {gym_name} (Episode {total_episodes_completed + 1}) \\n Ended at step {final_step_count}/{MAX_EPISODE_LENGTH} -- Learn Likelihoods: {LEARN_LIKELIHOODS}, Learn Transitions: {LEARN_TRANSITIONS}, Policy Length: {POLICY_LEN}\")\n",
    "    # Update score text for final static plot to show overall averages again\n",
    "    avg_optimality_perc = (total_optimal_steps / total_steps_across_episodes * 100) if total_steps_across_episodes > 0 else 0.0\n",
    "    avg_survival_perc = (total_survival_steps / total_episodes_completed / MAX_EPISODE_LENGTH * 100) if total_episodes_completed > 0 else 0.0\n",
    "    score_str_final = (\n",
    "        f\"Avg % of 8hr Session Survival (Prev Episodes): {avg_survival_perc:.1f}%\\n\"\n",
    "        f\"Avg % Time in Range / TIR (Prev Episodes): {avg_optimality_perc:.1f}%\"\n",
    "    )\n",
    "    score_text.set_text(score_str_final)\n",
    "    ax[2].legend(loc='lower left') # Ensure legend is correct\n",
    "\n",
    "    # Redraw final state for static plot (including final meal lines if any)\n",
    "    if meal_steps:\n",
    "        ymin, ymax = ax[0].get_ylim()\n",
    "        if ymax > ymin:\n",
    "            ax[0].vlines(meal_steps, ymin=ymin, ymax=ymax, color='r', linestyle='--', lw=0.8) # Draw all meal lines\n",
    "\n",
    "    plot_filename = f\"out/{TIMESTAMP}/episode_{total_episodes_completed + 1}_{gym_name}.png\"\n",
    "    fig.savefig(plot_filename)\n",
    "    print(f\"Saved final plot to {plot_filename}\")\n",
    "    plt.close(fig) # Close the figure after saving plot and animation\n",
    "\n",
    "    # --- Update Overall Stats and Cumulative Metrics ---\n",
    "    total_steps_across_episodes += final_step_count\n",
    "    total_episodes_completed += 1\n",
    "    total_survival_steps += final_step_count\n",
    "    total_optimal_steps += optimal_steps_count_episode\n",
    "\n",
    "    # Calculate and store cumulative metrics\n",
    "    current_optimality = total_optimal_steps / total_steps_across_episodes * 100\n",
    "    current_survival = (total_survival_steps / (total_episodes_completed * MAX_EPISODE_LENGTH)) * 100\n",
    "    cumulative_optimality.append(current_optimality)\n",
    "    cumulative_survival.append(current_survival)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate current episode metrics\n",
    "    current_episode_optimality = (optimal_steps_count_episode / final_step_count * 100) if final_step_count > 0 else 0\n",
    "    current_episode_survival = (final_step_count / MAX_EPISODE_LENGTH * 100)\n",
    "\n",
    "    # Plot cumulative metrics and current episode performance\n",
    "    fig_metrics, ax_metrics = plt.subplots(figsize=(10, 6))\n",
    "    episodes = range(1, total_episodes_completed + 1)\n",
    "    \n",
    "    # Plot cumulative lines\n",
    "    for x, y in zip(episodes, cumulative_optimality):\n",
    "        ax_metrics.annotate(f'{y:.1f}%', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    ax_metrics.plot(episodes, cumulative_optimality, label='Cumulative TIR %', marker='o')\n",
    "    \n",
    "    for x, y in zip(episodes, cumulative_survival):\n",
    "        ax_metrics.annotate(f'{y:.1f}%', (x, y), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
    "    ax_metrics.plot(episodes, cumulative_survival, label='Cumulative Survival %', marker='s')\n",
    "\n",
    "\n",
    "    \n",
    "    # Check if learning should stop based on performance plateau\n",
    "    if total_episodes_completed > LEARNING_STOP_BUFFER_EPISODES and not LEARNING_STOPPED:\n",
    "        # Get metrics for buffer window\n",
    "        recent_survival = cumulative_survival[-LEARNING_STOP_BUFFER_EPISODES:]\n",
    "        recent_optimality = cumulative_optimality[-LEARNING_STOP_BUFFER_EPISODES:]\n",
    "        \n",
    "        # Check if max values haven't increased\n",
    "        if max(recent_survival) <= max(cumulative_survival[:-LEARNING_STOP_BUFFER_EPISODES]) and \\\n",
    "           max(recent_optimality) <= max(cumulative_optimality[:-LEARNING_STOP_BUFFER_EPISODES]):\n",
    "            USING_ALTERNATION = False\n",
    "            LEARN_LIKELIHOODS = False \n",
    "            LEARN_TRANSITIONS = False\n",
    "            LEARNING_STOPPED_EPISODE = total_episodes_completed\n",
    "            # agent.model.save(f\"out/{TIMESTAMP}/vfg-episodes/vfg-b-{total_episodes_completed}.json\")\n",
    "            print(f\"Learning stopped at episode {total_episodes_completed} due to performance plateau\")\n",
    "\n",
    "    # Plot current episode performance\n",
    "    ax_metrics.annotate(f'{current_episode_optimality:.1f}%', \n",
    "                       (total_episodes_completed, current_episode_optimality),\n",
    "                       textcoords=\"offset points\", xytext=(10,0), ha='left')\n",
    "    ax_metrics.plot(total_episodes_completed, current_episode_optimality, 'g*', \n",
    "                   label='Current Episode TIR %', markersize=10)\n",
    "    \n",
    "    ax_metrics.annotate(f'{current_episode_survival:.1f}%',\n",
    "                       (total_episodes_completed, current_episode_survival),\n",
    "                       textcoords=\"offset points\", xytext=(10,0), ha='left')\n",
    "    ax_metrics.plot(total_episodes_completed, current_episode_survival, 'r*',\n",
    "                   label='Current Episode Survival %', markersize=10)\n",
    "    \n",
    "    ax_metrics.set_xlabel('Episode')\n",
    "    ax_metrics.set_ylabel('Percentage')\n",
    "    ax_metrics.set_title('Cumulative Performance Metrics')\n",
    "    ax_metrics.grid(True)\n",
    "    ax_metrics.legend()\n",
    "    metrics_filename = f\"out/{TIMESTAMP}/cumulative_metrics_episode_{total_episodes_completed}.png\"\n",
    "    fig_metrics.savefig(metrics_filename)\n",
    "    plt.close(fig_metrics)\n",
    "\n",
    "\n",
    "    # save the VFG learned from this episode\n",
    "    # if ((total_episodes_completed+1) % LEARN_INTERVAL_EPISODES == 0) and not LEARNING_STOPPED:\n",
    "    #     agent.model.save(f\"out/{TIMESTAMP}/vfg-episodes/vfg-b-{total_episodes_completed}.json\")\n",
    "\n",
    "    # # save the final VFG\n",
    "    # agent.get_model_from_server()\n",
    "    # agent.model.save(f\"out/{TIMESTAMP}/vfg-1.json\")\n",
    "\n",
    "    # import pandas as pd\n",
    "    # # Save detailed metrics for each timestep/episode\n",
    "    # metrics_df = pd.DataFrame({\n",
    "    #     'episode': episode_numbers,\n",
    "    #     'timestep': timesteps,\n",
    "    #     'cgm_value': cgm_values,\n",
    "    #     'insulin_action': insulin_actions,\n",
    "    #     'reward': rewards,\n",
    "    #     'is_optimal': optimal_steps,\n",
    "    #     'survival_length': episode_lengths,\n",
    "    #     'episode_optimality': episode_optimality_scores,\n",
    "    #     'episode_survival': episode_survival_scores\n",
    "    # })\n",
    "\n",
    "    if LEARNING_STOPPED_EPISODE is not None:\n",
    "        if total_episodes_completed - LEARNING_STOPPED_EPISODE > 30:\n",
    "            print(f\"Done with validation set\")\n",
    "            break\n",
    "\n",
    "# Save to CSV\n",
    "metrics_filename = f\"out/{TIMESTAMP}/detailed_metrics.csv\"\n",
    "# metrics_df.to_csv(metrics_filename, index=False)\n",
    "\n",
    "print(\"\\n--- Simulation Summary ---\")\n",
    "print(f\"Total Episodes Completed: {total_episodes_completed}\")\n",
    "print(f\"Total Steps Across All Episodes: {total_steps_across_episodes}\")\n",
    "if total_episodes_completed > 0:\n",
    "    avg_steps = total_survival_steps / total_episodes_completed\n",
    "    avg_survival_score = avg_steps / MAX_EPISODE_LENGTH * 100\n",
    "    print(f\"Average Steps Survived per Episode: {avg_steps:.2f}\")\n",
    "    print(f\"Overall Average Survival Score: {avg_survival_score:.2f}%\")\n",
    "if total_steps_across_episodes > 0:\n",
    "    overall_optimality_score = total_optimal_steps / total_steps_across_episodes * 100\n",
    "    print(f\"Overall Average TIR Score (CGM 70-180): {overall_optimality_score:.2f}%\")\n",
    "else:\n",
    "    print(\"No steps were taken across episodes.\")\n",
    "if total_episodes_completed == 0:\n",
    "     print(\"No episodes were completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
