{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdp.jax import control\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import nn, vmap, random, lax\n",
    "\n",
    "from typing import List, Optional\n",
    "from jaxtyping import Array\n",
    "from jax import random as jr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up generative model (random one with trivial observation model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a generative model\n",
    "num_states = [5, 3]\n",
    "num_controls = [2, 2]\n",
    "\n",
    "# make some arbitrary policies (policy depth 3, 2 control factors)\n",
    "policy_1 = jnp.array([[0, 1],\n",
    "                         [1, 1],\n",
    "                         [0, 0]])\n",
    "policy_2 = jnp.array([[1, 0],\n",
    "                        [0, 0],\n",
    "                        [1, 1]])\n",
    "policy_matrix = jnp.stack([policy_1, policy_2]) \n",
    "\n",
    "# observation modalities (isomorphic/identical to hidden states, just need to include for the need to include likleihood model)\n",
    "num_obs = [5, 3]\n",
    "num_factors = len(num_states)\n",
    "num_modalities = len(num_obs)\n",
    "\n",
    "# sample parameters of the model (A, B, C)\n",
    "key = jr.PRNGKey(1)\n",
    "factor_keys = jr.split(key, num_factors)\n",
    "\n",
    "d = [0.1* jr.uniform(factor_key, (ns,)) for factor_key, ns in zip(factor_keys, num_states)]\n",
    "qs_init = [jr.dirichlet(factor_key, d_f) for factor_key, d_f  in zip(factor_keys, d)]\n",
    "A = [jnp.eye(no) for no in num_obs]\n",
    "\n",
    "factor_keys = jr.split(factor_keys[-1], num_factors)\n",
    "b = [jr.uniform(factor_keys[f], shape=(num_controls[f], num_states[f], num_states[f])) for f in range(num_factors)]\n",
    "b_sparse = [jnp.where(b_f < 0.75, 1e-5, b_f) for b_f in b]\n",
    "B = [jnp.swapaxes(jr.dirichlet(factor_keys[f], b_sparse[f]), 2, 0) for f in range(num_factors)]\n",
    "\n",
    "modality_keys = jr.split(factor_keys[-1], num_modalities)\n",
    "C = [nn.one_hot(jr.randint(modality_keys[m], shape=(1,), minval=0, maxval=num_obs[m]), num_obs[m]) for m in range(num_modalities)]\n",
    "\n",
    "# trivial dependencies -- factor 1 drives modality 1, etc.\n",
    "A_dependencies = [[0], [1]]\n",
    "B_dependencies = [[0], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sparse constraints vectors `H` and inductive matrix `I`, using inductive parameters like depth and threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random constraints (H vector)\n",
    "factor_keys = jr.split(key, num_factors)\n",
    "H = [jr.uniform(factor_key, (ns,)) for factor_key, ns in zip(factor_keys, num_states)]\n",
    "H = [jnp.where(h < 0.75, 0., 1.) for h in H]\n",
    "\n",
    "# depth and threshold for inductive planning algorithm. I made policy-depth equal to inductive planning depth, out of ignorance -- need to ask Tim or Tommaso about this\n",
    "inductive_depth, inductive_threshold = 3, 0.5\n",
    "I = control.generate_I_matrix(H, B, inductive_threshold, inductive_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate posterior probability of policies and negative EFE using new version of `update_posterior_policies`\n",
    "#### This function no longer computes info gain (for both states and parameters) since deterministic model is assumed, and includes new inductive matrix `I` and `inductive_epsilon` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate Q(pi) and negative EFE using the inductive planning algorithm\n",
    "\n",
    "E = jnp.ones(policy_matrix.shape[0])\n",
    "pA = jtu.tree_map(lambda a: jnp.ones_like(a), A)\n",
    "pB = jtu.tree_map(lambda b: jnp.ones_like(b), B)\n",
    "\n",
    "q_pi, neg_efe = control.update_posterior_policies_inductive(policy_matrix, qs_init, A, B, C, E, pA, pB, A_dependencies, B_dependencies, I, gamma=16.0, use_utility=True, use_inductive=True, inductive_epsilon=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
