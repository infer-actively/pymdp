{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import nn, vmap, random, lax\n",
    "from typing import List, Optional\n",
    "from jaxtyping import Array\n",
    "from jax import random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pymdp.envs import GridWorldEnv\n",
    "from pymdp.jax import control as j_control\n",
    "from pymdp.jax.agent import Agent as AIFAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid world generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = 7, 7\n",
    "num_states = [num_rows*num_columns] # number of states equals the number of grid locations\n",
    "num_obs = [num_rows*num_columns]    # number of observations equals the number of grid locations (fully observable)\n",
    "\n",
    "# number of agents\n",
    "n_batches = 5\n",
    "\n",
    "# construct A arrays\n",
    "A = [jnp.broadcast_to(jnp.eye(num_states[0]), (n_batches,) + (num_obs[0], num_states[0]))] # fully observable (identity observation matrix\n",
    "\n",
    "# construct B arrays\n",
    "grid_world = GridWorldEnv(shape=[num_rows, num_columns])\n",
    "B = [jnp.broadcast_to(jnp.array(grid_world.get_transition_dist()), (n_batches,) + (num_states[0], num_states[0], grid_world.n_control))]  # easy way to get the generative model parameters is to extract them from one of pre-made GridWorldEnv classes\n",
    "num_controls = [grid_world.n_control] # number of control states equals the number of actions\n",
    " \n",
    "# create mapping from gridworld coordinates to linearly-index states\n",
    "grid = np.arange(grid_world.n_states).reshape(grid_world.shape)\n",
    "it = np.nditer(grid, flags=[\"multi_index\"])\n",
    "coord_to_idx_map = {}\n",
    "while not it.finished:\n",
    "    coord_to_idx_map[it.multi_index] = it.iterindex\n",
    "    it.iternext()\n",
    "\n",
    "# construct C arrays\n",
    "desired_position = (6,6) # lower corner\n",
    "desired_state_id = coord_to_idx_map[desired_position]\n",
    "desired_obs_id = jnp.argmax(A[0][:, desired_state_id]) # throw this in there, in case there is some indeterminism between states and observations\n",
    "C = [jnp.broadcast_to(nn.one_hot(desired_obs_id, num_obs[0]), (n_batches, num_obs[0]))]\n",
    "\n",
    "# construct D arrays\n",
    "starting_position = (3, 3) # middle\n",
    "# starting_position = (0, 0) # upper left corner\n",
    "starting_state_id = coord_to_idx_map[starting_position]\n",
    "starting_obs_id = jnp.argmax(A[0][:, starting_state_id]) # throw this in there, in case there is some indeterminism between states and observations\n",
    "D = [jnp.broadcast_to(nn.one_hot(starting_state_id, num_states[0]), (n_batches, num_states[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon, inductive_threshold = 1, 0.1\n",
    "inductive_depth = 7\n",
    "policy_matrix = j_control.construct_policies(num_states, num_controls, policy_len=planning_horizon)\n",
    "\n",
    "# inductive planning goal states\n",
    "H = [jnp.broadcast_to(nn.one_hot(desired_state_id, num_states[0]), (n_batches, num_states[0]))] # list of factor-specific goal vectors (shape of each is (n_batches, num_states[f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize an `Agent()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent\n",
    "agent = AIFAgent(A, B, C, D, E=None, pA=None, pB=None, policies=policy_matrix, policy_len=planning_horizon, \n",
    "                inductive_depth=inductive_depth, inductive_threshold=inductive_threshold,\n",
    "                H=H, use_utility=True, use_states_info_gain=False, use_param_info_gain=False, use_inductive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run active inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid position for agent 2 at time 0: (3, 3)\n",
      "Grid position for agent 2 at time 1: (3, 4)\n",
      "Grid position for agent 2 at time 2: (3, 5)\n",
      "Grid position for agent 2 at time 3: (3, 6)\n",
      "Grid position for agent 2 at time 4: (4, 6)\n",
      "Grid position for agent 2 at time 5: (5, 6)\n",
      "Grid position for agent 2 at time 6: (6, 6)\n"
     ]
    }
   ],
   "source": [
    "# T = 14 # needed if you start further away from the goal (e.g. in upper left corner)\n",
    "T = 7 # can get away with fewer timesteps if you start closer to the goal (e.g. in the middle)\n",
    "\n",
    "qs_init = [jnp.broadcast_to(nn.one_hot(starting_state_id, num_states[0]), (n_batches, num_states[0]))] # same as D\n",
    "obs_idx = [jnp.broadcast_to(starting_obs_id, (n_batches,))] # list of len (num_modalities), each list element of shape (n_batches,)\n",
    "obs_idx  = jtu.tree_map(lambda x: jnp.expand_dims(x, -1), obs_idx) #  list of len (num_modalities), elements each of shape (n_batches,1), this adds a trivial \"time dimension\"\n",
    "\n",
    "state = jnp.broadcast_to(starting_state_id, (n_batches,))\n",
    "infer_args = (agent.D, None,)\n",
    "batch_keys = jr.split(jr.PRNGKey(0), n_batches)\n",
    "batch_to_track = 1\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    print('Grid position for agent {} at time {}: {}'.format(batch_to_track+1, t, np.unravel_index(state[batch_to_track], grid_world.shape)))\n",
    "\n",
    "    if t == 0:\n",
    "        actions = None\n",
    "    else:\n",
    "        actions = actions_t\n",
    "    beliefs = agent.infer_states(obs_idx, actions, *infer_args)\n",
    "    q_pi, _ = agent.infer_policies(beliefs)\n",
    "    actions_t = agent.sample_action(q_pi, rng_key=batch_keys)\n",
    "    infer_args = agent.update_empirical_prior(actions_t, beliefs)\n",
    "\n",
    "    # get next state and observation from the grid world (need to vmap everything over batches)\n",
    "    state = vmap(lambda b, s, a: jnp.argmax(b[:, s, a]), in_axes=(0,0,0))(B[0], state, actions_t)\n",
    "    next_obs = vmap(lambda a, s: jnp.argmax(a[:, s]), in_axes=(0,0))(A[0], state)\n",
    "    obs_idx = [next_obs]\n",
    "    obs_idx  = jtu.tree_map(lambda x: jnp.expand_dims(x, -1), obs_idx) # add a trivial time dimension to the observation to enable indexing during agent.infer_states\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
