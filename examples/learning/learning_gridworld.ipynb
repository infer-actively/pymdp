{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import nn, vmap, lax, jit\n",
    "from jax import random as jr\n",
    "import numpy as np\n",
    "\n",
    "from pymdp.legacy.envs import GridWorldEnv\n",
    "from pymdp.envs import Env\n",
    "from pymdp.agent import Agent as AIFAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid world generative model\n",
    "\n",
    "Here we will explore learning of the generative model inside a simple grid 5x5 world environment, where at each state agent can move into 4 possible directions. The agent can explore the environment for 100 time steps, after which it is reaturned to the original position. We will start first with an example where likelihood is fixed, and state transitions are unkown. Next we will explore the example where likelihood is unknown but the state transitions are known, and finally a we will look at learning under joint uncertainty over likelihood and transitions (we would expect this case not to work in general with flat priors on both components). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = 7, 7\n",
    "num_states = [num_rows * num_columns] # number of states equals the number of grid locations\n",
    "num_obs = [num_rows * num_columns]    # number of observations equals the number of grid locations (fully observable)\n",
    "\n",
    "# number of agents\n",
    "n_batches = 20\n",
    "\n",
    "# construct A arrays\n",
    "A = [jnp.broadcast_to(jnp.eye(num_states[0]), (n_batches,) + (num_obs[0], num_states[0]))] # fully observable (identity observation matrix\n",
    "\n",
    "# construct B arrays\n",
    "grid_world = GridWorldEnv(shape=[num_rows, num_columns])\n",
    "# easy way to get the generative model parameters is to extract them from one of pre-made GridWorldEnv classes\n",
    "B = [jnp.broadcast_to(jnp.array(grid_world.get_transition_dist()), (n_batches,) + (num_states[0], num_states[0], grid_world.n_control))]  \n",
    "num_controls = [grid_world.n_control] # number of control states equals the number of actions\n",
    " \n",
    "# create mapping from gridworld coordinates to linearly-index states\n",
    "states = np.arange(grid_world.n_states)\n",
    "coords = np.unravel_index(states, (num_rows, num_columns) )\n",
    "coord_to_idx_map = {(i, j): s for i, j, s in zip(coords[0], coords[1], states)}\n",
    "\n",
    "# # construct C arrays\n",
    "# desired_position = (0, 0) # lower corner\n",
    "# desired_state_id = coord_to_idx_map[desired_position]\n",
    "# desired_obs_id = jnp.argmax(A[0][:, desired_state_id]) # throw this in there, in case there is some indeterminism between states and observations\n",
    "# C = [jnp.broadcast_to(nn.one_hot(desired_obs_id, num_obs[0]), (n_batches, num_obs[0]))]\n",
    "\n",
    "# construct D arrays\n",
    "starting_position = (3, 3) # upper left corner\n",
    "starting_state_id = coord_to_idx_map[starting_position]\n",
    "starting_obs_id = jnp.argmax(A[0][:, starting_state_id]) # throw this in there, in case there is some indeterminism between states and observations\n",
    "D = [jnp.broadcast_to(nn.one_hot(starting_state_id, num_states[0]), (n_batches, num_states[0]))]\n",
    "\n",
    "params = {\n",
    "    'A': A,\n",
    "    'B': B,\n",
    "    'D': D\n",
    "}\n",
    "\n",
    "dependencies = {\n",
    "    'A': [[0]],\n",
    "    'B': [[0]]\n",
    "}\n",
    "\n",
    "grid_world = Env(params, dependencies=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KL divergence between Dirichlet distributions\n",
    "from jax.scipy.special import gammaln, digamma\n",
    "\n",
    "@jit\n",
    "def kl_div_dirichlet(alpha1, alpha2):\n",
    "    alpha0 = alpha1.sum(1, keepdims=True)\n",
    "    kl = gammaln(alpha0.squeeze(1)) - gammaln(alpha2.sum(1))\n",
    "    kl += jnp.sum(gammaln(alpha2) - gammaln(alpha1) + (alpha1 - alpha2) * (digamma(alpha1) - digamma(alpha0)), 1)\n",
    "\n",
    "    return kl\n",
    "\n",
    "# Define the rollout function\n",
    "def rollout(rng_key, agent, env, num_timesteps):\n",
    "    \"\"\"\n",
    "    Rollout an agent in an environment for a number of timesteps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent: ``Agent``\n",
    "        Agent to interact with the environment\n",
    "    env: ``Env`\n",
    "        Environment to interact with\n",
    "    num_timesteps: ``int``\n",
    "        Number of timesteps to rollout for\n",
    "    rng_key: ``PRNGKey``\n",
    "        Random key to use for sampling actions\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    last: ``dict``\n",
    "        Carry dictionary from the last timestep\n",
    "    info: ``dict``\n",
    "        Dictionary containing information about the rollout, i.e. executed actions, observations, beliefs, etc.\n",
    "    env: ``Env``\n",
    "        Environment state after the rollout\n",
    "    \"\"\"\n",
    "    # get the batch_size of the agent\n",
    "    batch_size = agent.batch_size\n",
    "\n",
    "    def step_fn(carry, x):\n",
    "        action_t = carry[\"action_t\"]\n",
    "        observation_t = jtu.tree_map(lambda x: jnp.expand_dims(x, -1), carry[\"observation_t\"])\n",
    "        empirical_prior = carry[\"empirical_prior\"]\n",
    "        env = carry[\"env\"]\n",
    "        states = env.states[0]\n",
    "        rng_key = carry[\"rng_key\"]\n",
    "\n",
    "        qs = agent.infer_states(\n",
    "            observation_t,\n",
    "            empirical_prior\n",
    "        )\n",
    "        qpi, nefe = agent.infer_policies(qs)\n",
    "\n",
    "        keys = jr.split(rng_key, batch_size + 1)\n",
    "        rng_key = keys[0]\n",
    "        new_action = agent.sample_action(qpi, rng_key=keys[1:])\n",
    "\n",
    "        keys = jr.split(rng_key, batch_size + 1)\n",
    "        rng_key = keys[0]\n",
    "        observation_t, env = env.step(keys[1:], actions=new_action)\n",
    "\n",
    "        empirical_prior, qs = agent.update_empirical_prior(new_action, qs)\n",
    "\n",
    "        carry = {\n",
    "            \"action_t\": new_action,\n",
    "            \"observation_t\": observation_t,\n",
    "            \"empirical_prior\": empirical_prior,\n",
    "            \"env\": env,\n",
    "            \"rng_key\": rng_key,\n",
    "        }\n",
    "        info = {\n",
    "            \"qpi\": qpi,\n",
    "            \"qs\": jtu.tree_map(lambda x: x[:, 0, ...], qs),\n",
    "            \"states\": states,\n",
    "            \"observations\": observation_t,\n",
    "            \"actions\": action_t,\n",
    "        }\n",
    "\n",
    "        return carry, info\n",
    "\n",
    "    # generate initial observation\n",
    "    keys = jr.split(rng_key, batch_size + 1)\n",
    "    states_0 = env.states[0]\n",
    "    observation_0, env = env.step(keys[1:])\n",
    "\n",
    "    initial_carry = {\n",
    "        \"action_t\": None,\n",
    "        \"observation_t\": observation_0,\n",
    "        \"empirical_prior\": agent.D,\n",
    "        \"env\": env,\n",
    "        \"rng_key\": keys[0],\n",
    "    }\n",
    "\n",
    "    carry1, info1 = step_fn(initial_carry, None)\n",
    "\n",
    "    # Scan over time dimension (axis 1)\n",
    "    last, info = lax.scan(step_fn, carry1, jnp.arange(num_timesteps))\n",
    "\n",
    "    init_obs = jtu.tree_map(lambda x, y: jnp.stack([x, y], -1), observation_0, carry1['observation_t'])\n",
    "\n",
    "    info = jtu.tree_map(lambda x: jnp.swapaxes(x, 0, 1), info)\n",
    "    info['qs'] = jtu.tree_map( lambda x, y: jnp.concatenate([jnp.expand_dims(x, 1), y], 1), info1['qs'], info['qs'])\n",
    "    info['observations'] = jtu.tree_map(lambda x, y: jnp.concatenate([x, y[:, :-1]], 1), init_obs, info['observations'])\n",
    "    info['states'] = jtu.tree_map(lambda x, y: jnp.concatenate([jnp.expand_dims(x, 1), y], 1), states_0, info['states'])\n",
    "\n",
    "    return last, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize different agents using `Agent()` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent with A matrix being fixed to the A of the generative process\n",
    "C = [jnp.zeros((n_batches, num_obs[0]))]\n",
    "pB = [jnp.ones_like(B[0]) / num_states[0]]\n",
    "_B = jtu.tree_map(lambda x: x / x.sum(1, keepdims=True), pB)\n",
    "_D = [jnp.ones((n_batches, num_states[0]))]\n",
    "\n",
    "agents = []\n",
    "for i in range(5):\n",
    "    agents.append( \n",
    "        AIFAgent(\n",
    "            A,\n",
    "            _B,\n",
    "            C,\n",
    "            _D,\n",
    "            E=None,\n",
    "            pA=None,\n",
    "            pB=pB,\n",
    "            policy_len=3,\n",
    "            use_utility=False,\n",
    "            use_states_info_gain=True,\n",
    "            use_param_info_gain=True,\n",
    "            gamma=jnp.ones(1),\n",
    "            alpha=jnp.ones(1) * i * .2,\n",
    "            onehot_obs=False,\n",
    "            action_selection=\"stochastic\",\n",
    "            inference_algo=\"ovf\",\n",
    "            num_iter=1,\n",
    "            learn_A=False,\n",
    "            learn_B=True,\n",
    "            learn_D=True,\n",
    "            batch_size=n_batches,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run active inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pB0 = 1e4 * B[0] + 1e-4\n",
    "num_timesteps = 50\n",
    "num_blocks = 40\n",
    "key = jr.PRNGKey(0)\n",
    "divs = {i : [] for i in range(len(agents))}\n",
    "for block in range(num_blocks):\n",
    "    for i, agent in enumerate(agents):\n",
    "        key, _key = jr.split(key)\n",
    "        _, grid_world = grid_world.reset(_key)\n",
    "\n",
    "        key, _key = jr.split(key)\n",
    "        last, info = jit(rollout, static_argnums=[3,] )(_key, agent, grid_world, num_timesteps)\n",
    "        grid_world = last['env']\n",
    "\n",
    "        beliefs = info['qs']\n",
    "        actions = info['actions']\n",
    "        outcomes = info['observations']\n",
    "\n",
    "        agents[i] = agent.infer_parameters(beliefs, outcomes, actions)\n",
    "        divs[i].append(kl_div_dirichlet(agents[i].pB[0], pB0).sum(-1).mean(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5), sharex=True, sharey=True)\n",
    "for i in range(len(agents)):\n",
    "    p = axes.plot(jnp.stack(divs[i]).mean(-1), lw=3, label=agents[i].alpha.mean())\n",
    "    axes.plot(jnp.stack(divs[i]), color=p[0].get_color(), alpha=.2)\n",
    "\n",
    "axes.legend(title='alpha')\n",
    "axes.set_ylabel('KL divergence')\n",
    "axes.set_xlabel('epoch')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(num_controls[0]):\n",
    "    for j, agent in enumerate(agents[:2]):\n",
    "        sns.heatmap(agent.B[0][0, ..., i], ax=axes[j, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "    sns.heatmap(B[0][0, ..., i], ax=axes[2, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent with B matrix being fixed to the B of the generative process\n",
    "C = [jnp.zeros((n_batches, num_obs[0]))]\n",
    "pA = [jnp.ones_like(A[0]) / num_obs[0]]\n",
    "_A = jtu.tree_map(lambda x: x / x.sum(1, keepdims=True), pA)\n",
    "\n",
    "agents = []\n",
    "for i in range(5):\n",
    "    agents.append( \n",
    "        AIFAgent(\n",
    "            _A,\n",
    "            B,\n",
    "            C,\n",
    "            D,\n",
    "            E=None,\n",
    "            pA=pA,\n",
    "            pB=None,\n",
    "            policy_len=3,\n",
    "            use_utility=False,\n",
    "            use_states_info_gain=True,\n",
    "            use_param_info_gain=True,\n",
    "            gamma=jnp.ones(1),\n",
    "            alpha=jnp.ones(1) * i * .2,\n",
    "            onehot_obs=False,\n",
    "            action_selection=\"stochastic\",\n",
    "            inference_algo=\"ovf\",\n",
    "            num_iter=1,\n",
    "            learn_A=True,\n",
    "            learn_B=False,\n",
    "            learn_D=False,\n",
    "            batch_size=n_batches,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdp.jax.inference import smoothing_ovf\n",
    "\n",
    "key, _key = jr.split(key)\n",
    "grid_world = grid_world.reset(_key)\n",
    "\n",
    "key, _key = jr.split(key)\n",
    "last, info = jit(rollout, static_argnums=[3,] )(_key, agents[0], grid_world, num_timesteps)\n",
    "\n",
    "beliefs = info['qs']\n",
    "actions = info['actions']\n",
    "smoothed_marginals_and_joints = vmap(smoothing_ovf)(beliefs, agents[0].B, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA0 = 1e4 * A[0] + 1e-4\n",
    "num_timesteps = 50\n",
    "num_blocks = 20\n",
    "key = jr.PRNGKey(0)\n",
    "divs = {i: [] for i in range(len(agents))}\n",
    "for block in range(num_blocks):\n",
    "    for i, agent in enumerate(agents):\n",
    "        key, _key = jr.split(key)\n",
    "        grid_world = grid_world.reset(_key)\n",
    "\n",
    "        key, _key = jr.split(key)\n",
    "        last, info = jit(rollout, static_argnums=[3,] )(_key, agent, grid_world, num_timesteps)\n",
    "        grid_world = last['env']\n",
    "\n",
    "        beliefs = info['qs']\n",
    "        actions = info['actions']\n",
    "        outcomes = info['observations']\n",
    "\n",
    "        agents[i] = agent.infer_parameters(beliefs, outcomes, actions)\n",
    "        divs[i].append(kl_div_dirichlet(agents[i].pA[0], pA0).mean(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5), sharex=True, sharey=True)\n",
    "for i in range(len(agents)):\n",
    "    p = axes.plot(jnp.stack(divs[i]).mean(-1), lw=3, label=agents[i].alpha.mean())\n",
    "    axes.plot(jnp.stack(divs[i]), color=p[0].get_color(), alpha=.2)\n",
    "\n",
    "axes.legend(title='alpha')\n",
    "axes.set_ylabel('KL divergence')\n",
    "axes.set_xlabel('epoch')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(5):\n",
    "    for j, agent in enumerate(agents[:2]):\n",
    "        sns.heatmap(agent.A[0][i], ax=axes[j, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "    sns.heatmap(A[0][i], ax=axes[2, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "\n",
    "    axes[0, i].set_title(f'batch={i+1}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent with B matrix being fixed to the B of the generative process but flat beliefs over initial states\n",
    "C = [jnp.zeros((n_batches, num_obs[0]))]\n",
    "pA = [jnp.ones_like(A[0]) / num_obs[0]]\n",
    "_A = jtu.tree_map(lambda x: x / x.sum(1, keepdims=True), pA)\n",
    "\n",
    "agents = []\n",
    "for i in range(5):\n",
    "    agents.append( \n",
    "        AIFAgent(\n",
    "            _A,\n",
    "            B,\n",
    "            C,\n",
    "            _D,\n",
    "            E=None,\n",
    "            pA=pA,\n",
    "            pB=None,\n",
    "            policy_len=3,\n",
    "            use_utility=False,\n",
    "            use_states_info_gain=True,\n",
    "            use_param_info_gain=True,\n",
    "            gamma=jnp.ones(1),\n",
    "            alpha=jnp.ones(1) * i * .2,\n",
    "            onehot_obs=False,\n",
    "            action_selection=\"stochastic\",\n",
    "            inference_algo=\"ovf\",\n",
    "            num_iter=1,\n",
    "            learn_A=True,\n",
    "            learn_B=False,\n",
    "            learn_D=False,\n",
    "            batch_size=n_batches,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA0 = 1e4 * A[0] + 1e-4\n",
    "num_timesteps = 50\n",
    "num_blocks = 20\n",
    "key = jr.PRNGKey(0)\n",
    "divs = {i: [] for i in range(len(agents))}\n",
    "for block in range(num_blocks):\n",
    "    for i, agent in enumerate(agents):\n",
    "        key, _key = jr.split(key)\n",
    "        grid_world = grid_world.reset(_key)\n",
    "\n",
    "        key, _key = jr.split(key)\n",
    "        last, info = jit(rollout, static_argnums=[3,] )(_key, agent, grid_world, num_timesteps)\n",
    "        grid_world = last['env']\n",
    "\n",
    "        beliefs = info['qs']\n",
    "        actions = info['actions']\n",
    "        outcomes = info['observations']\n",
    "\n",
    "        agents[i] = agent.infer_parameters(beliefs, outcomes, actions)\n",
    "        divs[i].append(kl_div_dirichlet(agents[i].pA[0], pA0).mean(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5), sharex=True, sharey=True)\n",
    "for i in range(len(agents)):\n",
    "    p = axes.plot(jnp.stack(divs[i]).mean(-1), lw=3, label=agents[i].alpha.mean())\n",
    "    axes.plot(jnp.stack(divs[i]), color=p[0].get_color(), alpha=.2)\n",
    "\n",
    "axes.legend(title='alpha')\n",
    "axes.set_ylabel('KL divergence')\n",
    "axes.set_xlabel('epoch')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(5):\n",
    "    for j, agent in enumerate(agents[:2]):\n",
    "        sns.heatmap(agent.A[0][i], ax=axes[j, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "    sns.heatmap(A[0][i], ax=axes[2, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "\n",
    "    axes[0, i].set_title(f'batch={i+1}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent with B matrix being fixed to the B of the generative process but flat beliefs over initial states\n",
    "C = [jnp.zeros((n_batches, num_obs[0]))]\n",
    "pA = [jnp.ones_like(A[0]) / num_obs[0]]\n",
    "_A = jtu.tree_map(lambda x: x / x.sum(1, keepdims=True), pA)\n",
    "pB = [jnp.ones_like(B[0]) / num_states[0]]\n",
    "_B = jtu.tree_map(lambda x: x / x.sum(1, keepdims=True), pB)\n",
    "\n",
    "agents = []\n",
    "for i in range(5):\n",
    "    agents.append( \n",
    "        AIFAgent(\n",
    "            _A,\n",
    "            _B,\n",
    "            C,\n",
    "            D,\n",
    "            E=None,\n",
    "            pA=pA,\n",
    "            pB=pB,\n",
    "            policy_len=3,\n",
    "            use_utility=False,\n",
    "            use_states_info_gain=True,\n",
    "            use_param_info_gain=True,\n",
    "            gamma=jnp.ones(1),\n",
    "            alpha=jnp.ones(1) * i * .2,\n",
    "            onehot_obs=False,\n",
    "            action_selection=\"stochastic\",\n",
    "            inference_algo=\"ovf\",\n",
    "            num_iter=1,\n",
    "            learn_A=True,\n",
    "            learn_B=False,\n",
    "            learn_D=False,\n",
    "            batch_size=n_batches,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA0 = 1e4 * A[0] + 1e-4\n",
    "pB0 = 1e4 * B[0] + 1e-4\n",
    "num_timesteps = 50\n",
    "num_blocks = 100\n",
    "key = jr.PRNGKey(0)\n",
    "divs1 = {i: [] for i in range(len(agents))}\n",
    "divs2 = {i: [] for i in range(len(agents))}\n",
    "for block in range(num_blocks):\n",
    "    for i, agent in enumerate(agents):\n",
    "        key, _key = jr.split(key)\n",
    "        grid_world = grid_world.reset(_key)\n",
    "\n",
    "        key, _key = jr.split(key)\n",
    "        last, info = jit(rollout, static_argnums=[3,] )(_key, agent, grid_world, num_timesteps)\n",
    "        grid_world = last['env']\n",
    "\n",
    "        beliefs = info['qs']\n",
    "        actions = info['actions']\n",
    "        outcomes = info['observations']\n",
    "\n",
    "        agents[i] = agent.infer_parameters(beliefs, outcomes, actions)\n",
    "        divs1[i].append(kl_div_dirichlet(agents[i].pA[0], pA0).mean(-1))\n",
    "        divs2[i].append(kl_div_dirichlet(agents[i].pB[0], pB0).sum(-1).mean(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=False)\n",
    "for i in range(len(agents)):\n",
    "    p = axes[0].plot(jnp.stack(divs1[i]).mean(-1), lw=3, label=agents[i].alpha.mean())\n",
    "    axes[0].plot(jnp.stack(divs1[i]), color=p[0].get_color(), alpha=.2)\n",
    "\n",
    "    p = axes[1].plot(jnp.stack(divs2[i]).mean(-1), lw=3, label=agents[i].alpha.mean())\n",
    "    axes[1].plot(jnp.stack(divs2[i]), color=p[0].get_color(), alpha=.2)\n",
    "\n",
    "axes[0].legend(title='alpha')\n",
    "axes[0].set_ylabel('KL divergence')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[0].set_title('A matrix')\n",
    "axes[1].set_title('B matrix')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(5):\n",
    "    for j, agent in enumerate(agents[:2]):\n",
    "        sns.heatmap(agent.A[0][i], ax=axes[j, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "    sns.heatmap(A[0][i], ax=axes[2, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "\n",
    "    axes[0, i].set_title(f'batch={i+1}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(num_controls[0]):\n",
    "    for j, agent in enumerate(agents[:2]):\n",
    "        sns.heatmap(agent.B[0][0, ..., i], ax=axes[j, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "    sns.heatmap(B[0][0, ..., i], ax=axes[2, i], cmap='viridis', vmax=1., vmin=0.)\n",
    "    axes[0, i].set_title(f'action {i+1}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent with B matrix being fixed to the B of the generative process but flat beliefs over initial states\n",
    "C = [jnp.zeros((n_batches, num_obs[0]))]\n",
    "pA = [jnp.ones_like(A[0]) / num_obs[0]]\n",
    "_A = jtu.tree_map(lambda x: x / x.sum(1, keepdims=True), pA)\n",
    "tmpB = jnp.clip(B[0].sum(-1), max=1)\n",
    "pB = [jnp.expand_dims(tmpB, -1) + jnp.ones_like(B[0]) / num_states[0]]\n",
    "_B = jtu.tree_map(lambda x: x / x.sum(1, keepdims=True), pB)\n",
    "\n",
    "agents = []\n",
    "for i in range(5):\n",
    "    agents.append( \n",
    "        AIFAgent(\n",
    "            _A,\n",
    "            _B,\n",
    "            C,\n",
    "            _D,\n",
    "            E=None,\n",
    "            pA=pA,\n",
    "            pB=pB,\n",
    "            policy_len=3,\n",
    "            use_utility=False,\n",
    "            use_states_info_gain=True,\n",
    "            use_param_info_gain=True,\n",
    "            gamma=jnp.ones(1),\n",
    "            alpha=jnp.ones(1) * i * .2,\n",
    "            onehot_obs=False,\n",
    "            action_selection=\"stochastic\",\n",
    "            inference_algo=\"ovf\",\n",
    "            num_iter=1,\n",
    "            learn_A=True,\n",
    "            learn_B=False,\n",
    "            learn_D=False,\n",
    "            batch_size=n_batches,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA0 = 1e4 * A[0] + 1e-4\n",
    "pB0 = 1e4 * B[0] + 1e-4\n",
    "num_timesteps = 50\n",
    "num_blocks = 100\n",
    "key = jr.PRNGKey(0)\n",
    "divs1 = {i: [] for i in range(len(agents))}\n",
    "divs2 = {i: [] for i in range(len(agents))}\n",
    "for block in range(num_blocks):\n",
    "    for i, agent in enumerate(agents):\n",
    "        key, _key = jr.split(key)\n",
    "        grid_world = grid_world.reset(_key)\n",
    "\n",
    "        key, _key = jr.split(key)\n",
    "        last, info = jit(rollout, static_argnums=[3,] )(_key, agent, grid_world, num_timesteps)\n",
    "        grid_world = last['env']\n",
    "\n",
    "        beliefs = info['qs']\n",
    "        actions = info['actions']\n",
    "        outcomes = info['observations']\n",
    "\n",
    "        agents[i] = agent.infer_parameters(beliefs, outcomes, actions)\n",
    "        divs1[i].append(kl_div_dirichlet(agents[i].pA[0], pA0).mean(-1))\n",
    "        divs2[i].append(kl_div_dirichlet(agents[i].pB[0], pB0).sum(-1).mean(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=False)\n",
    "for i in range(len(agents)):\n",
    "    p = axes[0].plot(jnp.stack(divs1[i]).mean(-1), lw=3, label=agents[i].alpha.mean())\n",
    "    axes[0].plot(jnp.stack(divs1[i]), color=p[0].get_color(), alpha=.2)\n",
    "\n",
    "    p = axes[1].plot(jnp.stack(divs2[i]).mean(-1), lw=3, label=agents[i].alpha.mean())\n",
    "    axes[1].plot(jnp.stack(divs2[i]), color=p[0].get_color(), alpha=.2)\n",
    "\n",
    "axes[0].legend(title='alpha')\n",
    "axes[0].set_ylabel('KL divergence')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[0].set_title('A matrix')\n",
    "axes[1].set_title('B matrix')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymdp_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
